{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cecbd73",
   "metadata": {
    "papermill": {
     "duration": 0.022123,
     "end_time": "2025-12-13T17:37:34.479022",
     "exception": false,
     "start_time": "2025-12-13T17:37:34.456899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# AdaptableSnail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6a4e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:37:34.518721Z",
     "iopub.status.busy": "2025-12-13T17:37:34.518450Z",
     "iopub.status.idle": "2025-12-13T17:39:15.998157Z",
     "shell.execute_reply": "2025-12-13T17:39:15.997306Z"
    },
    "papermill": {
     "duration": 101.520538,
     "end_time": "2025-12-13T17:39:16.018564",
     "exception": false,
     "start_time": "2025-12-13T17:37:34.498026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== START INFERENCE ===\n",
      "Predicting Video 438887472...\n",
      "\n",
      "Done! Saved submission to /kaggle/working/submission1.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "np.seterr(invalid=\"ignore\", divide=\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    \"\"\"\n",
    "    Chứa cấu hình tham số (Hyperparameters).\n",
    "    \"\"\"\n",
    "    fps: float = 30.0\n",
    "    pix_per_cm: float = 1.0\n",
    "    smooth_sigma: float = 1.0\n",
    "    use_pairwise: bool = True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. AGENT CONTEXT\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class AgentContext:\n",
    "    \"\"\"\n",
    "    Container chứa dữ liệu đã tiền xử lý của một con chuột.\n",
    "    Giúp tránh việc tính toán lại vận tốc/gia tốc nhiều lần.\n",
    "    \"\"\"\n",
    "    idx: pd.Index          # Index frame\n",
    "    pos: np.ndarray        # [F, 2] cm\n",
    "    vel: np.ndarray        # [F, 2] cm/s\n",
    "    speed: np.ndarray      # [F, 1] cm/s\n",
    "    acc: np.ndarray        # [F, 2] cm/s^2\n",
    "    \n",
    "    cx: pd.Series          # Series tọa độ X (để dùng rolling)\n",
    "    cy: pd.Series          # Series tọa độ Y\n",
    "    speed_series: pd.Series # Series tốc độ\n",
    "    \n",
    "    raw_df: Optional[pd.DataFrame] = None # Dữ liệu gốc các bộ phận \n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. FEATURE EXTRACTOR\n",
    "# =============================================================================\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Class trích xuất đặc trưng hành vi từ dữ liệu tracking.\n",
    "    \"\"\"\n",
    "    def __init__(self, fps: float, pix_per_cm: float, smooth_sigma: float = 1.0, use_pairwise: bool = True):\n",
    "        # Map tham số từ init vào Config\n",
    "        self.cfg = FeatureConfig(\n",
    "            fps=float(fps), \n",
    "            pix_per_cm=float(pix_per_cm), \n",
    "            smooth_sigma=smooth_sigma,\n",
    "            use_pairwise=use_pairwise\n",
    "        )\n",
    "        \n",
    "        # Đăng ký các hàm feature sẽ chạy\n",
    "        self.feature_registry = {\n",
    "            \"kinematics\": self._feat_basic_kinematics,\n",
    "            \"multiscale\": self._feat_multiscale,\n",
    "            \"long_range\": self._feat_long_range,\n",
    "            \"cumulative\": self._feat_cumulative,\n",
    "            \"curvature\": self._feat_curvature,\n",
    "            \"speed_asym\": self._feat_speed_asym,\n",
    "            \"gauss_shift\": self._feat_gauss_shift,\n",
    "            \"grooming\": self._feat_avoidance_trajectory,\n",
    "            \"pose\": self._feat_pose_shape,\n",
    "            \"a\": self._feat_submission_temporal,\n",
    "            \"pairwise\": self._feat_pairwise\n",
    "        }\n",
    "\n",
    "    # --- Helpers ---\n",
    "    def _scale(self, n_frames_30fps: int) -> int:\n",
    "        \"\"\"Quy đổi số frame từ chuẩn 30fps sang fps thực tế của video.\"\"\"\n",
    "        return max(1, int(round(n_frames_30fps * self.cfg.fps / 30.0)))\n",
    "\n",
    "    def _to_cm(self, arr):\n",
    "        \"\"\"Chuyển pixel -> cm.\"\"\"\n",
    "        return arr / self.cfg.pix_per_cm\n",
    "\n",
    "    def _smooth(self, x):\n",
    "        \"\"\"Làm mượt dữ liệu bằng Gaussian filter.\"\"\"\n",
    "        if self.cfg.smooth_sigma is None or x.shape[0] < 3: return x\n",
    "        if np.all(np.isnan(x)): return x\n",
    "        return gaussian_filter1d(x, sigma=self.cfg.smooth_sigma, axis=0, mode=\"nearest\")\n",
    "\n",
    "    def _forward_fill_nan(self, pos):\n",
    "        \"\"\"\n",
    "        Điền dữ liệu thiếu (NaN) bằng giá trị hợp lệ trước đó (Forward Fill).\n",
    "        \"\"\"\n",
    "        if np.all(np.isnan(pos)):\n",
    "            return np.zeros_like(pos)\n",
    "\n",
    "        pos_ffill = pos.copy()\n",
    "        mask = np.any(~np.isnan(pos_ffill), axis=1)\n",
    "        if not mask.any():\n",
    "            return np.zeros_like(pos_ffill)\n",
    "\n",
    "        valid_idx = np.where(mask)[0]\n",
    "        first, last = valid_idx[0], valid_idx[-1]\n",
    "        pos_ffill[:first] = pos_ffill[first]\n",
    "        pos_ffill[last + 1:] = pos_ffill[last]\n",
    "        df_temp = pd.DataFrame(pos_ffill)\n",
    "        df_temp = df_temp.ffill()\n",
    "        return df_temp.to_numpy()\n",
    "    \n",
    "    def _speed_series(self, cx: pd.Series, cy: pd.Series) -> pd.Series:\n",
    "        dx = cx.diff()\n",
    "        dy = cy.diff()\n",
    "        v = np.hypot(dx, dy).fillna(0.0) * self.cfg.fps\n",
    "        return v.astype(\"float32\")\n",
    "    \n",
    "    def _roll_future_mean(self, s: pd.Series, w: int, min_p: int = 1) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).mean().iloc[::-1]\n",
    "\n",
    "    def _roll_future_var(self, s: pd.Series, w: int, min_p: int = 2) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).var().iloc[::-1]\n",
    "\n",
    "    # --- Core Logic ---\n",
    "    def _compute_kinematics(self, pos_px: np.ndarray):\n",
    "        \"\"\"\n",
    "        Tính toán vật lý cơ bản: Pos(cm), Vel, Speed, Acc.\n",
    "        Input: Array [Frames, 2] (pixel).\n",
    "        Output: Tuple (pos_cm, vel, speed, acc).\n",
    "        \"\"\"\n",
    "        pos_ffill = self._forward_fill_nan(pos_px)\n",
    "        pos_cm = self._to_cm(pos_ffill.astype(np.float32))\n",
    "        pos_cm = self._smooth(pos_cm)                                               # [F, 2]\n",
    "\n",
    "        dt = 1.0 / self.cfg.fps\n",
    "        vel = np.zeros_like(pos_cm, dtype=np.float32)\n",
    "        vel[1:] = (pos_cm[1:] - pos_cm[:-1]) / dt                                   # [F, 2: (vx, vy)]\n",
    "        speed = np.linalg.norm(vel, axis=1, keepdims=True).astype(np.float32)       # [F, 1]\n",
    "\n",
    "        acc = np.zeros_like(pos_cm, dtype=np.float32)                          \n",
    "        acc[1:] = (vel[1:] - vel[:-1]) / dt                                         # [F, 2:(ax, ay)]\n",
    "        return pos_cm.astype(np.float32), vel, speed, acc\n",
    "\n",
    "    def _build_context(self, frames, pos_px, mouse_df=None) -> AgentContext:\n",
    "        \"\"\"\n",
    "        Tạo AgentContext chứa đầy đủ thông tin vật lý của 1 con chuột.\n",
    "        \"\"\"\n",
    "        p, v, s, a = self._compute_kinematics(pos_px)\n",
    "        idx = pd.Index(frames, name=\"frame\")\n",
    "        \n",
    "        return AgentContext(\n",
    "            idx=idx, pos=p, vel=v, speed=s, acc=a, \n",
    "            cx=pd.Series(p[:, 0], index=idx), \n",
    "            cy=pd.Series(p[:, 1], index=idx), \n",
    "            speed_series=pd.Series(s[:, 0], index=idx), \n",
    "            raw_df=mouse_df\n",
    "        )\n",
    "\n",
    "    # --- Feature Modules ---\n",
    "    def _feat_basic_kinematics(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Lấy các giá trị thô: tọa độ x, y, vận tốc vx, vy, tốc độ, gia tốc ax, ay.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"a_x\": ctx.pos[:, 0], \"a_y\": ctx.pos[:, 1],\n",
    "            \"a_vx\": ctx.vel[:, 0], \"a_vy\": ctx.vel[:, 1],\n",
    "            \"a_speed\": ctx.speed[:, 0],\n",
    "            \"a_ax\": ctx.acc[:, 0], \"a_ay\": ctx.acc[:, 1]\n",
    "        }\n",
    "\n",
    "    def _feat_multiscale(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tính tốc độ trung bình (Mean) và độ lệch chuẩn (Std) ở đa mức thời gian.\n",
    "        Feature 'sp_ratio' đo độ bùng nổ (Burstiness).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        speed = ctx.speed_series\n",
    "        frame_scales = [10, 40, 160]\n",
    "        for scale in frame_scales:\n",
    "            ws = self._scale(scale)\n",
    "            if len(speed) >= ws:\n",
    "                roller = speed.rolling(ws, min_periods=max(1, ws//4), center=True)\n",
    "                feats[f\"sp_m{scale}\"] = roller.mean().astype(\"float32\")\n",
    "                feats[f\"sp_s{scale}\"] = roller.std().astype(\"float32\")\n",
    "        feats[f\"sp_ratio\"] = feats[\"sp_m10\"] / (feats[\"sp_m160\"] + 1e-6)\n",
    "        return feats \n",
    "        \n",
    "    def _feat_long_range(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng ngữ cảnh dài hạn:\n",
    "        - x_ml, y_ml: Vị trí trung bình trong quá khứ.\n",
    "        - sp_pct: Xếp hạng (percentile) của tốc độ hiện tại so với quá khứ.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        speed = ctx.speed_series\n",
    "\n",
    "        for window in [120, 240]:\n",
    "            ws = self._scale(window)\n",
    "            if len(ctx.cx) >= ws:\n",
    "                feats[f\"x_ml{window}\"] = ctx.cx.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "                feats[f\"y_ml{window}\"] = ctx.cy.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "\n",
    "        for span in [60, 120]:\n",
    "            s = self._scale(span)\n",
    "            feats[f\"x_e{span}\"] = ctx.cx.ewm(span=s, min_periods=1).mean()\n",
    "            feats[f\"y_e{span}\"] = ctx.cy.ewm(span=s, min_periods=1).mean()\n",
    "\n",
    "        for window in [60, 120]:\n",
    "            ws = self._scale(window)\n",
    "            if len(speed) >= ws:\n",
    "                feats[f\"sp_pct{window}\"] = speed.rolling(\n",
    "                    ws, min_periods=max(5, ws // 6), center=True\n",
    "                ).rank(pct=True)\n",
    "        return feats\n",
    "    \n",
    "\n",
    "    def _feat_curvature(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        feats = {}\n",
    "\n",
    "        vel_x, vel_y = ctx.vel[:, 0], ctx.vel[:, 1]\n",
    "        acc_x, acc_y = ctx.acc[:, 0], ctx.acc[:, 1]\n",
    "        cross_prod = vel_x * acc_y - vel_y * acc_x\n",
    "        vel_mag = np.sqrt(vel_x**2 + vel_y**2)\n",
    "        moving_mask = vel_mag > 2.0\n",
    "        vel_mag_safe = np.maximum(vel_mag, 0.1 / self.cfg.fps)\n",
    "        raw_curv = cross_prod / (vel_mag_safe**3)\n",
    "        raw_curv = np.where(moving_mask, raw_curv, 0.0)\n",
    "        min_turn_radius_cm = 0.5\n",
    "        max_k = 1.0 / min_turn_radius_cm\n",
    "        raw_curv = np.clip(raw_curv, -max_k, max_k)\n",
    "        abs_curv = np.abs(raw_curv)\n",
    "        abs_curv_series = pd.Series(abs_curv, index=ctx.idx)\n",
    "\n",
    "        for w in [30, 60]:\n",
    "            ws = self._scale(w)\n",
    "            min_p = max(ws // 3, 1)\n",
    "            feats[f\"curv_mean_{w}\"] = abs_curv_series.rolling(ws, min_periods=min_p).mean()\n",
    "\n",
    "        angle = np.arctan2(vel_y, vel_x)\n",
    "        angle_series = pd.Series(angle, index=ctx.idx)\n",
    "        angle_change = np.abs(angle_series.diff().fillna(0.0))\n",
    "        angle_change = np.where(angle_change > np.pi, 2 * np.pi - angle_change, angle_change)\n",
    "        angle_change_series = pd.Series(angle_change, index=ctx.idx)\n",
    "        angle_change_series = pd.Series(np.where(moving_mask, angle_change_series, 0.0), index=ctx.idx)\n",
    "\n",
    "        ws = self._scale(30)\n",
    "        feats[\"turn_rate_30\"] = angle_change_series.rolling(ws, min_periods=max(ws // 3, 1)).sum()\n",
    "\n",
    "        return feats\n",
    "    \n",
    "    def _feat_cumulative(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tổng quãng đường di chuyển trong một khoảng thời gian dài xung quanh frame hiện tại.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        L = max(1, self._scale(180))\n",
    "        step = np.hypot(ctx.cx.diff(), ctx.cy.diff()).fillna(0.0)\n",
    "        path = step.rolling(2 * L + 1, min_periods=max(5, L // 6), center=True).sum()\n",
    "        feats[\"path_cum180\"] =  path.fillna(0.0).astype(\"float32\")\n",
    "        return feats\n",
    "\n",
    "    def _feat_speed_asym(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Bất đối xứng tốc độ (Tương lai - Quá khứ).\n",
    "        \"\"\"\n",
    "        w = max(3, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        v_past = v.rolling(w, min_periods=1).mean()\n",
    "        v_fut = self._roll_future_mean(v, w, min_p=1)\n",
    "        return {\"spd_asym_1s\": (v_fut - v_past).fillna(0.0)}\n",
    "    \n",
    "    def _feat_gauss_shift(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Độ lệch Gaussian (KL Divergence) giữa quá khứ và tương lai.\n",
    "        Đo lường sự thay đổi trạng thái thống kê.\n",
    "        \"\"\"\n",
    "        w = max(5, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        mu_p = v.rolling(w, min_periods=1).mean()\n",
    "        va_p = v.rolling(w, min_periods=1).var().clip(lower=1e-6)\n",
    "        mu_f = self._roll_future_mean(v, w, min_p=1)\n",
    "        va_f = self._roll_future_var(v, w, min_p=1).clip(lower=1e-6)\n",
    "\n",
    "        kl_pf = 0.5 * (\n",
    "            (va_p / va_f) + ((mu_f - mu_p) ** 2) / va_f - 1.0 + np.log(va_f / va_p)\n",
    "        )\n",
    "        kl_fp = 0.5 * (\n",
    "            (va_f / va_p) + ((mu_p - mu_f) ** 2) / va_p - 1.0 + np.log(va_p / va_f)\n",
    "        )\n",
    "        return {\n",
    "            \"spd_symkl_1s\": (kl_pf + kl_fp).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        }\n",
    "\n",
    "    def _feat_avoidance_trajectory(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Tính toán quỹ đạo né tránh:\n",
    "        1. Relative Heading: Góc di chuyển so với hướng tới đối thủ.\n",
    "        2. Future Distance Gain: Dự báo xem hành động này có giúp chuột ra xa đối thủ trong tương lai không.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        if target_ctx is None: \n",
    "            return feats\n",
    "\n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        angle_to_target = np.arctan2(rel_vec[:, 1], rel_vec[:, 0])\n",
    "        my_heading = np.arctan2(ctx.vel[:, 1], ctx.vel[:, 0])\n",
    "        \n",
    "        diff = np.abs(angle_to_target - my_heading)\n",
    "        diff = np.minimum(diff, 2*np.pi - diff) # Chuẩn hóa về [0, pi]\n",
    "        feats[\"heading_rel_cos\"] = pd.Series(np.cos(diff), index=idx, dtype=\"float32\")\n",
    "        \n",
    "        feats[\"heading_rel_abs\"] = pd.Series(diff, index=idx, dtype=\"float32\")\n",
    "        dist_now = np.linalg.norm(rel_vec, axis=1)\n",
    "        s_dist = pd.Series(dist_now, index=idx)\n",
    "        \n",
    "        scales = [15, 30] # 0.5s và 1s\n",
    "        for w in scales:\n",
    "            ws = self._scale(w)\n",
    "            dist_future = s_dist.shift(-ws)\n",
    "            gain = dist_future - s_dist\n",
    "            \n",
    "            feats[f\"dist_gain_{w}f\"] = gain.fillna(0.0).astype(\"float32\")\n",
    "\n",
    "        return feats\n",
    "    \n",
    "    def _extract_part(self, ctx: AgentContext, part: str) -> Optional[np.ndarray]:\n",
    "        if ctx.raw_df is None: return None\n",
    "        if part not in ctx.raw_df.columns.get_level_values(0): return None\n",
    "        try:\n",
    "            sub_df = ctx.raw_df.xs(part, axis=1, level=0)[[\"x\", \"y\"]].reindex(ctx.idx)\n",
    "        except KeyError: return None\n",
    "        raw = sub_df.to_numpy()\n",
    "        raw = self._forward_fill_nan(raw)\n",
    "        cm = self._to_cm(raw.astype(np.float32))\n",
    "        return self._smooth(cm)\n",
    "    \n",
    "    def _extract_parts_dict(self, ctx: AgentContext, parts: List[str] = None) -> Dict[str, Optional[np.ndarray]]:\n",
    "        out = {}\n",
    "        for p in parts:\n",
    "            out[p] = self._extract_part(ctx, p)\n",
    "        return out\n",
    "        \n",
    "    def _feat_pose_shape(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Placeholder cho các đặc trưng hình dáng (Elongation, Body Angle...).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "\n",
    "        def zero(): return pd.Series(0.0, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        def dist(k1, k2):\n",
    "            p1, p2 = parts.get(k1), parts.get(k2)\n",
    "            if p1 is None or p2 is None: return zero()\n",
    "            d = np.linalg.norm(p1 - p2, axis=1)\n",
    "            return pd.Series(d, index=ctx.idx, dtype=\"float32\")\n",
    "        \n",
    "        def body_angle():\n",
    "            if parts.get(\"nose\") is None: return zero()\n",
    "            if parts.get(\"body_center\") is None: return zero()\n",
    "            if parts.get(\"tail_base\") is None: return zero()\n",
    "\n",
    "            v1 = parts.get(\"nose\") - parts.get(\"body_center\")\n",
    "            v2 = parts.get(\"tail_base\") - parts.get(\"body_center\")\n",
    "            dot_product = np.sum(v1 * v2, axis=1)\n",
    "            mag = np.linalg.norm(v1, axis=1) * np.linalg.norm(v2, axis=1)\n",
    "            cos_angle = np.clip(dot_product / (mag + 1e-6), -1.0, 1.0).astype(\"float32\")\n",
    "            return cos_angle\n",
    "        \n",
    "        def elongation():\n",
    "            if parts.get(\"nose\")          is None: return zero()\n",
    "            if parts.get(\"tail_base\")     is None: return zero()\n",
    "            if parts.get(\"lateral_left\")  is None: return zero()\n",
    "            if parts.get(\"lateral_right\") is None: return zero()\n",
    "\n",
    "            d1 = dist(\"nose\", \"tail_base\")\n",
    "            d2 = dist(\"lateral_left\", \"lateral_right\")\n",
    "            elongation = d1 / (d2 + 1e-6).astype(\"float32\")\n",
    "            return elongation\n",
    "        \n",
    "        def vel(part: str, n_frames_30fps: int) -> Dict:\n",
    "            part_pos = self._extract_part(ctx, part)\n",
    "            if part_pos is None: return zero()\n",
    "            \n",
    "            s_x = pd.Series(part_pos[:, 0], index=ctx.idx)\n",
    "            s_y = pd.Series(part_pos[:, 1], index=ctx.idx)\n",
    "            raw_speed = self._speed_series(s_x, s_y)\n",
    "\n",
    "            ws = self._scale(n_frames_30fps)\n",
    "            val = raw_speed.rolling(ws, min_periods=1, center=True).mean()\n",
    "            return val.astype(\"float32\")\n",
    "\n",
    "\n",
    "        target_parts = [\"nose\", \"neck\", \"body_center\", \"tail_base\", \n",
    "                        \"ear_left\", \"ear_right\", \n",
    "                        \"lateral_left\", \"lateral_right\", \"tail_midpoint\", \"tail_tip\"]\n",
    "        \n",
    "        parts = self._extract_parts_dict(ctx, target_parts)\n",
    "\n",
    "        feats[\"a_body_width\"]                = dist(\"lateral_left\", \"lateral_right\")\n",
    "        feats[\"aa_nose_bodycenter_dist\"]     = dist(\"nose\", \"body_center\")\n",
    "        feats[\"aa_nose_tailbase_dist\"]       = dist(\"nose\", \"tail_base\")\n",
    "        feats[\"aa_bodycenter_tailbase_dist\"] = dist(\"body_center\", \"tail_base\")\n",
    "        \n",
    "        feats[\"aa_bodycenter_ear_l_dist\"]    = dist(\"body_center\", \"ear_left\")\n",
    "        feats[\"aa_bodycenter_ear_r_dist\"]    = dist(\"body_center\", \"ear_right\")\n",
    "        feats[\"aa_bodycenter_lateral_l_dist\"]= dist(\"body_center\", \"lateral_left\")\n",
    "        feats[\"aa_bodycenter_lateral_r_dist\"]= dist(\"body_center\", \"lateral_right\")\n",
    "        \n",
    "        feats[\"a_body_angle\"]                = body_angle()\n",
    "        feats[\"a_elongation\"]                = elongation()\n",
    "        feats[\"a_tail_base_vel_500ms\"]       = vel(\"tail_base\", 15)\n",
    "        feats[\"a_tail_base_vel_1000ms\"]      = vel(\"tail_base\", 30)\n",
    "        feats[\"a_tail_base_vel_2000ms\"]      = vel(\"tail_base\", 60)\n",
    "        feats[\"a_tail_base_vel_3000ms\"]      = vel(\"tail_base\", 90)\n",
    "        feats[\"a_nose_vel_500ms\"]            = vel(\"nose\", 15)\n",
    "        feats[\"a_nose_vel_1000ms\"]           = vel(\"nose\", 30)\n",
    "        feats[\"a_nose_vel_2000ms\"]           = vel(\"nose\", 60)\n",
    "        feats[\"a_nose_vel_3000ms\"]           = vel(\"nose\", 90)\n",
    "        feats[\"a_ear_right_vel_500ms\"]       = vel(\"ear_right\", 15)\n",
    "        feats[\"a_ear_right_vel_1000ms\"]      = vel(\"ear_right\", 30)\n",
    "        feats[\"a_ear_right_vel_2000ms\"]      = vel(\"ear_right\", 60)\n",
    "        feats[\"a_ear_right_vel_3000ms\"]      = vel(\"ear_right\", 90)\n",
    "        # len_1 = dist(\"tail_base\", \"tail_midpoint\")\n",
    "        # len_2 = dist(\"tail_midpoint\", \"tail_tip\")\n",
    "        # len_full = dist(\"tail_base\", \"tail_tip\")\n",
    "        # feats[\"tail_curl\"] = ((len_1 + len_2) / (len_full + 1e-6)).astype(\"float32\")\n",
    "        return feats\n",
    "\n",
    "    def _feat_submission_temporal(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng 'Ký ức sợ hãi' (Fear Memory) để bắt Submit tĩnh.\n",
    "        Giúp phân biệt Submit (sau khi bị đánh) vs Rest (bình yên).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        if target_ctx is None: return feats\n",
    "        \n",
    "        idx = ctx.idx\n",
    "        vec_to_target = target_ctx.pos - ctx.pos\n",
    "        dist = np.linalg.norm(vec_to_target, axis=1)\n",
    "        dist_safe = pd.Series(dist, index=idx).replace(0, 1e-6)\n",
    "        t_vel = target_ctx.vel\n",
    "        dot_threat = np.sum(t_vel * (-vec_to_target), axis=1)\n",
    "        \n",
    "        threat_raw = (dot_threat / dist_safe).clip(lower=0) \n",
    "        threat_raw = threat_raw * (dist_safe < 15.0).astype(float)\n",
    "        threat_series = pd.Series(threat_raw, index=idx, dtype=\"float32\")\n",
    "        ws_memory = self._scale(90)\n",
    "        \n",
    "        feats[\"fear_memory_3s\"] = threat_series.rolling(ws_memory, min_periods=1).max().astype(\"float32\")\n",
    "        my_speed = ctx.speed_series\n",
    "        is_still = (my_speed < 1.0).astype(float)\n",
    "        parts = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\"])\n",
    "        if parts[\"nose\"] is not None:\n",
    "            spine_len = np.linalg.norm(parts[\"nose\"] - parts[\"tail_base\"], axis=1)\n",
    "            is_compact = (spine_len < 8.0).astype(float) # Ví dụ chuột dài < 8cm là co\n",
    "            is_compact = pd.Series(is_compact, index=idx)\n",
    "        else:\n",
    "            is_compact = pd.Series(0.0, index=idx)\n",
    "        feats[\"static_submit_prob\"] = (\n",
    "            is_still * is_compact * feats[\"fear_memory_3s\"]\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        return feats\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def _feat_pairwise(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng tương tác cặp đôi (Pairwise): Khoảng cách, Tốc độ tiếp cận.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None: \n",
    "            return feats\n",
    "\n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "\n",
    "        # --- 1. KHOẢNG CÁCH CƠ BẢN (DISTANCES) ---\n",
    "        # Vector nối Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        feats[\"rel_dist\"] = pd.Series(dist, index=idx, dtype=\"float32\")\n",
    "\n",
    "        # --- 2. KHOẢNG CÁCH CHI TIẾT (NOSE-TO-PART) ---\n",
    "        # Lấy các bộ phận quan trọng\n",
    "        my_parts = self._extract_parts_dict(ctx, [\"nose\", \"neck\"])\n",
    "        target_parts = self._extract_parts_dict(target_ctx, \n",
    "            [\"nose\", \"tail_base\", \"body_center\", \"ear_left\", \"ear_right\", \n",
    "             \"lateral_left\", \"lateral_right\", \"tail_midpoint\"])\n",
    "\n",
    "        def dist_ab(pt_a, pt_b):\n",
    "            if pt_a is None or pt_b is None: return zero()\n",
    "            d = np.linalg.norm(pt_a - pt_b, axis=1)\n",
    "            return pd.Series(d, index=idx, dtype=\"float32\")\n",
    "\n",
    "        an, tn = my_parts[\"nose\"], target_parts[\"nose\"]\n",
    "        feats[\"dist_nose_nose\"] = dist_ab(an, tn)\n",
    "        feats[\"dist_nose_tail\"] = dist_ab(an, target_parts[\"tail_base\"])\n",
    "        feats[\"dist_nose_body\"] = dist_ab(an, target_parts[\"body_center\"])\n",
    "        feats[\"dist_nose_el\"]   = dist_ab(an, target_parts[\"ear_left\"])\n",
    "        feats[\"dist_nose_er\"]   = dist_ab(an, target_parts[\"ear_right\"])\n",
    "        feats[\"dist_nose_tll\"]  = dist_ab(an, target_parts[\"lateral_left\"])\n",
    "        feats[\"dist_nose_tlr\"]  = dist_ab(an, target_parts[\"lateral_right\"])\n",
    "        feats[\"dist_nose_tmp\"]  = dist_ab(an, target_parts[\"tail_midpoint\"])\n",
    "\n",
    "        # --- 3. ĐỊNH HƯỚNG & GÓC NHÌN (ORIENTATION & GAZE) ---\n",
    "        # Helper lấy vector cơ thể (Mũi - Đuôi/Thân)\n",
    "        def get_body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"nose\")\n",
    "            # Ưu tiên đuôi, nếu ko có thì dùng thân\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if tail is None: tail = parts_dict.get(\"body_center\") # Fallback\n",
    "            \n",
    "            if head is not None and tail is not None:\n",
    "                return head - tail\n",
    "            return None\n",
    "\n",
    "        a_vec = get_body_vec(my_parts)\n",
    "        t_vec = get_body_vec(target_parts)\n",
    "\n",
    "        # A. Body Cosine: Hai con cùng chiều hay ngược chiều?\n",
    "        if a_vec is not None and t_vec is not None:\n",
    "            dot = np.sum(a_vec * t_vec, axis=1)\n",
    "            mags = np.linalg.norm(a_vec, axis=1) * np.linalg.norm(t_vec, axis=1)\n",
    "            feats[\"body_cosine\"] = pd.Series(\n",
    "                np.clip(dot / (mags + 1e-6), -1.0, 1.0), index=idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"body_cosine\"] = zero()\n",
    "\n",
    "        # B. Gaze Cosine: Tôi có đang nhìn về phía Target không?\n",
    "        # Vector ánh nhìn = Target_Pos - My_Pos = rel_vec\n",
    "        if a_vec is not None:\n",
    "            dot_gaze = np.sum(a_vec * rel_vec, axis=1)\n",
    "            mag_a = np.linalg.norm(a_vec, axis=1)\n",
    "            # dist đã tính ở bước 1\n",
    "            feats[\"gaze_cosine\"] = pd.Series(\n",
    "                np.clip(dot_gaze / (mag_a * dist + 1e-6), -1.0, 1.0),\n",
    "                index=idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"gaze_cosine\"] = zero()\n",
    "\n",
    "        # --- 4. PHÂN RÃ VẬN TỐC (VELOCITY DECOMPOSITION) - CHÌA KHÓA CHO AVOID/ESCAPE ---\n",
    "        # Vector đơn vị hướng về địch (u)\n",
    "        dist_safe = dist.copy()\n",
    "        dist_safe[dist_safe == 0] = 1e-6\n",
    "        u_vec = rel_vec / dist_safe[:, None]\n",
    "\n",
    "        # a_vel và t_vel lấy từ Context\n",
    "        a_vel, t_vel = ctx.vel, target_ctx.vel\n",
    "\n",
    "        # A. Approach Speed (Vận tốc dọc trục nối 2 con)\n",
    "        # Dương: Lao vào nhau | Âm: Chạy ra xa nhau\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1) # Target hướng ngược lại\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)\n",
    "\n",
    "        # B. Lateral Speed (Vận tốc ngang - Vuông góc trục nối)\n",
    "        # Vector chiếu: v_proj = (v . u) * u\n",
    "        a_proj = a_along[:, None] * u_vec\n",
    "        a_lat_vec = a_vel - a_proj\n",
    "        a_lat_speed = np.linalg.norm(a_lat_vec, axis=1)\n",
    "\n",
    "        feats[\"approach_speed_agent\"]  = pd.Series(a_along, index=idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_target\"] = pd.Series(t_along, index=idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_rel\"]    = pd.Series(rel_along, index=idx, dtype=\"float32\")\n",
    "        feats[\"lateral_speed_agent\"]   = pd.Series(a_lat_speed, index=idx, dtype=\"float32\")\n",
    "        return feats\n",
    "\n",
    "\n",
    "    # --- Methods tương thích ---\n",
    "    \n",
    "    def build_pose_tensor(self, tracking: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Chuyển dữ liệu tracking (DataFrame) sang Tensor [Frames, Mice, 2] và Dict chi tiết.\n",
    "        \"\"\"\n",
    "        tracking = tracking.sort_values(\"video_frame\")\n",
    "        frames = np.sort(tracking[\"video_frame\"].unique())\n",
    "        \n",
    "        pvid = tracking.pivot(\n",
    "            index=\"video_frame\", \n",
    "            columns=[\"mouse_id\", \"bodypart\"], \n",
    "            values=[\"x\", \"y\"]\n",
    "        )\n",
    "        pvid = pvid.reorder_levels([1, 2, 0], axis=1).sort_index(axis=1).astype(\"float32\")\n",
    "        mouse_ids = list(pvid.columns.get_level_values(0).unique())\n",
    "        pos = np.full((len(frames), len(mouse_ids), 2), np.nan, dtype=np.float32)\n",
    "        per_mouse_df = {}\n",
    "        \n",
    "        for i, mid in enumerate(mouse_ids):\n",
    "            single = pvid[mid]\n",
    "            per_mouse_df[mid] = single\n",
    "            \n",
    "            if \"body_center\" in single.columns.get_level_values(0):\n",
    "                cx = single[\"body_center\"][\"x\"]\n",
    "                cy = single[\"body_center\"][\"y\"]\n",
    "            else:\n",
    "                cx = single.xs(\"x\", level=1, axis=1).mean(axis=1)\n",
    "                cy = single.xs(\"y\", level=1, axis=1).mean(axis=1)\n",
    "            \n",
    "            pos[:, i, 0] = cx.reindex(frames).values\n",
    "            pos[:, i, 1] = cy.reindex(frames).values\n",
    "            \n",
    "        return frames, mouse_ids, pos, per_mouse_df\n",
    "\n",
    "    def extract_agent_target(\n",
    "        self, \n",
    "        frames: np.ndarray, \n",
    "        mouse_ids: List[Any], \n",
    "        pos: np.ndarray, \n",
    "        agent_id: Any, \n",
    "        target_id: Any, \n",
    "        per_mouse_df: Dict = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Trích xuất đặc trưng cho cặp (Agent, Target).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            aid_idx = mouse_ids.index(agent_id)\n",
    "        except ValueError:\n",
    "            return pd.DataFrame() \n",
    "\n",
    "        # 1. Build Agent Context\n",
    "        ctx_agent = self._build_context(\n",
    "            frames, \n",
    "            pos[:, aid_idx, :], \n",
    "            per_mouse_df.get(agent_id) if per_mouse_df else None\n",
    "        )\n",
    "\n",
    "        # 2. Build Target Context\n",
    "        ctx_target = None\n",
    "        if self.cfg.use_pairwise and target_id is not None and target_id in mouse_ids:\n",
    "             tid_idx = mouse_ids.index(target_id)\n",
    "             ctx_target = self._build_context(\n",
    "                 frames, \n",
    "                 pos[:, tid_idx, :], \n",
    "                 per_mouse_df.get(target_id) if per_mouse_df else None\n",
    "             )\n",
    "\n",
    "        # 3. Run all features\n",
    "        all_data = {}\n",
    "        for func_name, func in self.feature_registry.items():\n",
    "            out_dict = func(ctx_agent, target_ctx=ctx_target)\n",
    "            all_data.update(out_dict)\n",
    "\n",
    "        df_out = pd.DataFrame(all_data, index=ctx_agent.idx)\n",
    "        df_out = df_out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        \n",
    "        return df_out.reindex(sorted(df_out.columns), axis=1)\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# === IMPORT MODEL & OPTUNA ===\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "# Cấu hình\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "np.seterr(invalid=\"ignore\", divide=\"ignore\")\n",
    "\n",
    "# Metric\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/usr/lib/mabe-f-beta\")\n",
    "try:\n",
    "    from metric import score\n",
    "except ImportError:\n",
    "    def score(*args, **kwargs): return 0.0\n",
    "\n",
    "# =========================================================\n",
    "# 1. CẤU HÌNH & SEED\n",
    "# =========================================================\n",
    "SEED = 42\n",
    "def seed_everything(seed=42):\n",
    "    np.random.seed(seed)\n",
    "seed_everything(SEED)\n",
    "\n",
    "INPUT_DIR = Path(\"/kaggle/input/MABe-mouse-behavior-detection\")\n",
    "TRAIN_TRACKING_DIR = INPUT_DIR / \"train_tracking\"\n",
    "TRAIN_ANNOTATION_DIR = INPUT_DIR / \"train_annotation\"\n",
    "TEST_TRACKING_DIR = INPUT_DIR / \"test_tracking\"\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "RESULTS_DIR = Path(r\"/kaggle/input/results-ensemble-optuna\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "SELF_BEHAVIORS = [\"biteobject\", \"climb\", \"dig\", \"exploreobject\", \"freeze\", \"genitalgroom\", \"huddle\", \"rear\", \"rest\", \"run\", \"selfgroom\"]\n",
    "PAIR_BEHAVIORS = [\"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\", \"chase\", \"chaseattack\", \"defend\", \"disengage\", \"dominance\", \"dominancegroom\", \"dominancemount\", \"ejaculate\", \"escape\", \"flinch\", \"follow\", \"intromit\", \"mount\", \"reciprocalsniff\", \"shepherd\", \"sniff\", \"sniffbody\", \"sniffface\", \"sniffgenital\", \"submit\", \"tussle\"]\n",
    "BAD_VIDEOS = []\n",
    "\n",
    "# =========================================================\n",
    "# 2. DATA LOADING & PREPARATION (NO CACHE)\n",
    "# =========================================================\n",
    "\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    return pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "\n",
    "def get_video_params(video_id: Any, meta: pd.DataFrame) -> Tuple[float, float]:\n",
    "    row = meta.loc[meta[\"video_id\"] == video_id]\n",
    "    if row.empty: return 30.0, 1.0\n",
    "    row = row.iloc[0]\n",
    "    return float(row[\"frames_per_second\"]), float(row[\"pix_per_cm_approx\"])\n",
    "\n",
    "def load_tracking(lab_id: str, video_id: Any, is_test=False) -> pd.DataFrame:\n",
    "    d = TEST_TRACKING_DIR if is_test else TRAIN_TRACKING_DIR\n",
    "    path = d / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists(): raise FileNotFoundError(path)\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def load_annotation(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    path = TRAIN_ANNOTATION_DIR / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists(): return pd.DataFrame(columns=[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "    return pd.read_parquet(path)[[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]]\n",
    "\n",
    "# Hàm lấy feature KHÔNG CACHE để tránh tràn RAM\n",
    "def get_frame_features_no_cache(lab_id, video_id, agent_id, target_id, meta, is_test=False):\n",
    "    if is_test:\n",
    "        row = meta[meta[\"video_id\"] == video_id].iloc[0]\n",
    "        fps, pix = float(row[\"frames_per_second\"]), float(row[\"pix_per_cm_approx\"])\n",
    "        pix = pix if np.isfinite(pix) and pix > 0 else 1.0\n",
    "    else:\n",
    "        fps, pix = get_video_params(video_id, meta)\n",
    "\n",
    "    tracking = load_tracking(lab_id, video_id, is_test)\n",
    "    \n",
    "    # === GỌI CLASS FeatureExtractor (Đã có ở cell trước) ===\n",
    "    fe = FeatureExtractor(fps=fps, pix_per_cm=pix, smooth_sigma=1.0, use_pairwise=True)\n",
    "    \n",
    "    frames, mouse_ids, pos, per_mouse_df = fe.build_pose_tensor(tracking)\n",
    "    \n",
    "    features_df = fe.extract_agent_target(\n",
    "        frames=frames, mouse_ids=mouse_ids, pos=pos,\n",
    "        agent_id=agent_id, target_id=target_id, per_mouse_df=per_mouse_df\n",
    "    )\n",
    "    features_df.index = frames\n",
    "    return frames, features_df\n",
    "\n",
    "def build_frame_dataset_for_lab_behavior(lab_id, behavior, train_meta, mode=\"self\"):\n",
    "    videos = train_meta[train_meta[\"lab_id\"] == lab_id][\"video_id\"].unique().tolist()\n",
    "    index_list, feature_list, label_list = [], [], []\n",
    "\n",
    "    for video_id in videos:\n",
    "        ann = load_annotation(lab_id, video_id)\n",
    "        if ann.empty: continue\n",
    "        \n",
    "        ann_bhv = ann[ann[\"action\"] == behavior]\n",
    "        if ann_bhv.empty: continue\n",
    "\n",
    "        pairs = ann_bhv[[\"agent_id\", \"target_id\"]].drop_duplicates().values.tolist()\n",
    "        for (agent_id, target_id) in pairs:\n",
    "            target_id_use = agent_id if mode == \"self\" else target_id\n",
    "            \n",
    "            # Lấy features (tính trực tiếp)\n",
    "            frames, feat_df = get_frame_features_no_cache(lab_id, video_id, agent_id, target_id_use, train_meta)\n",
    "\n",
    "            ann_pair = ann_bhv[(ann_bhv[\"agent_id\"] == agent_id) & (ann_bhv[\"target_id\"] == target_id)]\n",
    "            if ann_pair.empty and mode == \"self\": ann_pair = ann_bhv[ann_bhv[\"agent_id\"] == agent_id]\n",
    "\n",
    "            pos_frames = set()\n",
    "            for _, r in ann_pair.iterrows(): pos_frames.update(range(int(r[\"start_frame\"]), int(r[\"stop_frame\"])))\n",
    "            \n",
    "            if not pos_frames: continue\n",
    "            label = np.isin(frames, list(pos_frames)).astype(\"int8\")\n",
    "            if label.sum() == 0: continue\n",
    "\n",
    "            # Lưu vào list và reset index ngay để giảm memory overhead\n",
    "            index_list.append(pd.DataFrame({\"video_id\": video_id, \"agent_id\": agent_id, \"target_id\": target_id, \"video_frame\": frames}))\n",
    "            feature_list.append(feat_df.reset_index(drop=True))\n",
    "            label_list.append(label)\n",
    "            \n",
    "            # Dọn dẹp ngay\n",
    "            del frames, feat_df, label\n",
    "\n",
    "    if not index_list: return pd.DataFrame(), pd.DataFrame(), np.zeros(0, dtype=\"int8\")\n",
    "    \n",
    "    return pd.concat(index_list, ignore_index=True), pd.concat(feature_list, ignore_index=True), np.concatenate(label_list).astype(\"int8\")\n",
    "\n",
    "# =========================================================\n",
    "# 3. TRAINING & ENSEMBLE HELPERS\n",
    "# =========================================================\n",
    "\n",
    "def train_catboost_fold(X_tr, y_tr, X_va, y_va, sw=1.0):\n",
    "    p = {\n",
    "        'iterations': 1000, 'learning_rate': 0.05, 'depth': 6, 'scale_pos_weight': sw,\n",
    "        'task_type': 'GPU', 'devices': '0', 'verbose': 0, 'allow_writing_files': False,\n",
    "        'l2_leaf_reg': 5, 'bootstrap_type': 'Bernoulli', 'subsample': 0.8, 'random_seed': SEED\n",
    "    }\n",
    "    m = cb.CatBoostClassifier(**p)\n",
    "    m.fit(cb.Pool(X_tr, y_tr), eval_set=cb.Pool(X_va, y_va), early_stopping_rounds=20, use_best_model=True)\n",
    "    return m\n",
    "\n",
    "def train_lightgbm_fold(X_tr, y_tr, X_va, y_va, sw=1.0):\n",
    "    p = {\n",
    "        'objective': 'binary', 'metric': 'binary_logloss', 'learning_rate': 0.05,\n",
    "        'max_depth': 6, 'num_leaves': 31, 'scale_pos_weight': sw, 'device': 'gpu',\n",
    "        'verbosity': -1, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8,\n",
    "        'subsample_freq': 1, 'seed': SEED\n",
    "    }\n",
    "    m = lgb.train(p, lgb.Dataset(X_tr, y_tr), 1000, valid_sets=[lgb.Dataset(X_va, y_va)], callbacks=[lgb.early_stopping(20, verbose=False)])\n",
    "    return m\n",
    "\n",
    "def optimize_ensemble_weights(oof_dict, y_true):\n",
    "    models = list(oof_dict.keys())\n",
    "    def obj(trial):\n",
    "        w = [trial.suggest_float(m, 0.0, 1.0) for m in models]\n",
    "        s = sum(w) + 1e-6; w = [x/s for x in w]\n",
    "        p = np.zeros_like(y_true, dtype=float)\n",
    "        for i, m in enumerate(models): p += oof_dict[m] * w[i]\n",
    "        th = trial.suggest_float(\"th\", 0.1, 0.9)\n",
    "        return f1_score(y_true, (p >= th).astype(int), zero_division=0)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "    study.optimize(obj, n_trials=50)\n",
    "    best = study.best_params\n",
    "    th = best.pop(\"th\")\n",
    "    rw = [best[m] for m in models]; s = sum(rw)+1e-6\n",
    "    return {m: w/s for m, w in zip(models, rw)}, th\n",
    "\n",
    "def train_validate_ensemble(lab_id, behavior, indices, features, labels):\n",
    "    res_dir = RESULTS_DIR / lab_id / behavior\n",
    "    res_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if len(labels) == 0 or labels.sum() == 0: return 0.0\n",
    "\n",
    "    X = features.values.astype(\"float32\")\n",
    "    y = labels.astype(\"int8\")\n",
    "    groups = indices[\"video_id\"].values\n",
    "    \n",
    "    oof_preds = {m: np.zeros(len(y), dtype=\"float32\") for m in [\"xgb\", \"cat\", \"lgb\"]}\n",
    "    folds = np.ones(len(y), dtype=\"int8\") * -1\n",
    "\n",
    "    cv = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "    for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups=groups)):\n",
    "        print(f\"   Fold {fold}...\", end=\" \")\n",
    "        fd_dir = res_dir / f\"fold_{fold}\"; fd_dir.mkdir(parents=True, exist_ok=True)\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]; X_va, y_va = X[va_idx], y[va_idx]\n",
    "        pos = y_tr.sum(); neg = len(y_tr) - pos\n",
    "        sw = float(neg/pos) if pos > 0 else 1.0\n",
    "\n",
    "        # 1. XGBoost\n",
    "        dtr = xgb.QuantileDMatrix(X_tr, label=y_tr, feature_names=features.columns.tolist(), max_bin=64)\n",
    "        dva = xgb.DMatrix(X_va, label=y_va, feature_names=features.columns.tolist())\n",
    "        xp = {\n",
    "            \"objective\":\"binary:logistic\", \"eval_metric\":\"logloss\", \"device\":\"cuda\", \n",
    "            \"tree_method\":\"hist\", \"learning_rate\":0.05, \"max_depth\":6, \"scale_pos_weight\":sw,\n",
    "            \"min_child_weight\":5, \"subsample\":0.8, \"colsample_bytree\":0.8, \"max_bin\":64, \"seed\": SEED\n",
    "        }\n",
    "        \n",
    "        # === ĐÃ THÊM 'evals=' VÀO DÒNG DƯỚI ===\n",
    "        mx = xgb.train(\n",
    "            params=xp, \n",
    "            dtrain=dtr, \n",
    "            num_boost_round=1000, \n",
    "            evals=[(dva, \"valid\")],\n",
    "            callbacks=[xgb.callback.EarlyStopping(rounds=20, save_best=True)], \n",
    "            verbose_eval=False\n",
    "        )\n",
    "        mx.save_model(fd_dir / \"model_xgb.json\")\n",
    "        oof_preds[\"xgb\"][va_idx] = mx.predict(dva)\n",
    "\n",
    "        # 2. CatBoost\n",
    "        mc = train_catboost_fold(X_tr, y_tr, X_va, y_va, sw)\n",
    "        mc.save_model(str(fd_dir / \"model_cat.cbm\"))\n",
    "        oof_preds[\"cat\"][va_idx] = mc.predict_proba(X_va)[:,1]\n",
    "\n",
    "        # 3. LightGBM\n",
    "        ml = train_lightgbm_fold(X_tr, y_tr, X_va, y_va, sw)\n",
    "        ml.save_model(fd_dir / \"model_lgb.txt\")\n",
    "        oof_preds[\"lgb\"][va_idx] = ml.predict(X_va)\n",
    "        folds[va_idx] = fold\n",
    "        \n",
    "        print(\"Done.\")\n",
    "        del X_tr, y_tr, X_va, y_va, dtr, dva, mx, mc, ml\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"   Optimizing Weights...\", end=\" \")\n",
    "    weights, th = optimize_ensemble_weights(oof_preds, y)\n",
    "    with open(res_dir / \"ensemble_params.json\", \"w\") as f: json.dump({\"weights\": weights, \"threshold\": th}, f)\n",
    "    \n",
    "    final_pred = sum(oof_preds[m] * weights[m] for m in weights)\n",
    "    final_lbl = (final_pred >= th).astype(\"int8\")\n",
    "    \n",
    "    # Save OOF\n",
    "    df = indices.copy(); df[\"fold\"] = folds; df[\"pred\"] = final_pred; df[\"lbl\"] = final_lbl\n",
    "    df.to_parquet(res_dir / \"oof.parquet\", index=False)\n",
    "    \n",
    "    f1 = f1_score(y, final_lbl, zero_division=0)\n",
    "    print(f\"Best F1: {f1:.4f} (Th={th:.2f}, W={weights})\")\n",
    "    (res_dir / \"f1.txt\").write_text(f\"{f1:.6f}\")\n",
    "    return float(f1)\n",
    "\n",
    "# =========================================================\n",
    "# 4. INFERENCE\n",
    "# =========================================================\n",
    "\n",
    "def load_ensemble_models(lab_id, behavior):\n",
    "    base = RESULTS_DIR / lab_id / behavior\n",
    "    if not base.exists(): return []\n",
    "    models = []\n",
    "    for fd in sorted(base.glob(\"fold_*\")):\n",
    "        if not (fd / \"model_xgb.json\").exists(): continue\n",
    "        \n",
    "        xgb_b = xgb.Booster(); xgb_b.load_model(str(fd / \"model_xgb.json\"))\n",
    "        cat_m = cb.CatBoostClassifier(); \n",
    "        try: cat_m.load_model(str(fd / \"model_cat.cbm\"))\n",
    "        except: cat_m = None\n",
    "        try: lgb_m = lgb.Booster(model_file=str(fd / \"model_lgb.txt\"))\n",
    "        except: lgb_m = None\n",
    "        models.append({\"xgb\": xgb_b, \"cat\": cat_m, \"lgb\": lgb_m})\n",
    "    return models\n",
    "\n",
    "def predict_behaviors_for_pair(lab_id, video_id, aid, tid, behaviors, test_meta):\n",
    "    if lab_id != \"AdaptableSnail\": return None\n",
    "    frames, feat_df = get_frame_features_no_cache(lab_id, video_id, aid, tid, test_meta, is_test=True)\n",
    "    if feat_df.empty: return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "    \n",
    "    scores = {}\n",
    "    for bhv in behaviors:\n",
    "        base = RESULTS_DIR / lab_id / bhv\n",
    "        if not (base / \"ensemble_params.json\").exists(): continue\n",
    "        with open(base / \"ensemble_params.json\") as f: p = json.load(f)\n",
    "        ws, th = p[\"weights\"], p[\"threshold\"]\n",
    "        \n",
    "        folds = load_ensemble_models(lab_id, bhv)\n",
    "        if not folds: continue\n",
    "        \n",
    "        cols = folds[0][\"xgb\"].feature_names\n",
    "        X = pd.DataFrame(0.0, index=feat_df.index, columns=cols, dtype=np.float32)\n",
    "        c = list(set(cols) & set(feat_df.columns))\n",
    "        if c: X[c] = feat_df[c]\n",
    "        dtest = xgb.DMatrix(X, feature_names=cols)\n",
    "        \n",
    "        agg = np.zeros(len(feat_df), dtype=np.float32)\n",
    "        for m in folds:\n",
    "            px = m[\"xgb\"].predict(dtest)\n",
    "            pc = m[\"cat\"].predict_proba(X)[:,1] if m[\"cat\"] else np.zeros_like(px)\n",
    "            pl = m[\"lgb\"].predict(X) if m[\"lgb\"] else np.zeros_like(px)\n",
    "            \n",
    "            avg = px*ws.get(\"xgb\", 0.33) + pc*ws.get(\"cat\", 0.33) + pl*ws.get(\"lgb\", 0.33)\n",
    "            agg += avg * (avg >= th).astype(\"int8\")\n",
    "        \n",
    "        if folds: scores[bhv] = agg / len(folds)\n",
    "        \n",
    "        del X, dtest\n",
    "        gc.collect()\n",
    "\n",
    "    if not scores: return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "    \n",
    "    bl = list(scores.keys()); mat = np.vstack([scores[b] for b in bl]).T\n",
    "    lbls = np.where(mat.max(1)==0, \"none\", np.array(bl)[mat.argmax(1)])\n",
    "    \n",
    "    segs = []; prev = \"none\"; start = None; pf = None\n",
    "    for f, l in zip(frames, lbls):\n",
    "        if l != prev:\n",
    "            if prev != \"none\": segs.append({\"video_id\": int(video_id), \"action\": prev, \"start_frame\": int(start), \"stop_frame\": int(pf)+1})\n",
    "            prev = l; start = f\n",
    "        pf = f\n",
    "    if prev != \"none\": segs.append({\"video_id\": int(video_id), \"action\": prev, \"start_frame\": int(start), \"stop_frame\": int(pf)+1})\n",
    "    \n",
    "    return pd.DataFrame(segs)\n",
    "\n",
    "# =========================================================\n",
    "# 5. MAIN\n",
    "# =========================================================\n",
    "target_lab = \"AdaptableSnail\"\n",
    "print(\"\\n=== START INFERENCE ===\")\n",
    "test_meta = pd.read_csv(INPUT_DIR / \"test.csv\")\n",
    "test_meta = test_meta[test_meta[\"lab_id\"] == target_lab].reset_index(drop=True)\n",
    "\n",
    "trained = sorted([p.name for p in (RESULTS_DIR/target_lab).iterdir() if p.is_dir()])\n",
    "sb, pb = [b for b in trained if b in SELF_BEHAVIORS], [b for b in trained if b in PAIR_BEHAVIORS]\n",
    "\n",
    "all_segs = []\n",
    "def fid(i): return str(i) if str(i).startswith(\"mouse\") else f\"mouse{i}\"\n",
    "\n",
    "for vid in sorted(test_meta[\"video_id\"].unique()):\n",
    "    print(f\"Predicting Video {vid}...\")\n",
    "    tr = load_tracking(target_lab, vid, is_test=True)\n",
    "    mids = sorted(tr[\"mouse_id\"].unique())\n",
    "    \n",
    "    if sb:\n",
    "        for m in mids:\n",
    "            df = predict_behaviors_for_pair(target_lab, vid, m, m, sb, test_meta)\n",
    "            if df is not None and not df.empty:\n",
    "                df[\"agent_id\"] = fid(m); df[\"target_id\"] = \"self\"\n",
    "                all_segs.append(df)\n",
    "    if pb and len(mids) > 1:\n",
    "        for a, t in itertools.permutations(mids, 2):\n",
    "            df = predict_behaviors_for_pair(target_lab, vid, a, t, pb, test_meta)\n",
    "            if df is not None and not df.empty:\n",
    "                df[\"agent_id\"] = fid(a); df[\"target_id\"] = fid(t)\n",
    "                all_segs.append(df)\n",
    "    del tr\n",
    "    gc.collect()\n",
    "\n",
    "cols = [\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "\n",
    "if all_segs:\n",
    "    sub1 = pd.concat(all_segs, ignore_index=True)\n",
    "    sub1 = sub1[cols].sort_values([\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\"]).reset_index(drop=True)    \n",
    "    sub1.insert(0, \"row_id\", np.arange(len(sub1), dtype=np.int64))\n",
    "else:\n",
    "    sub1 = pd.DataFrame(columns=[\"row_id\"] + cols)\n",
    "\n",
    "sub1.to_csv(WORKING_DIR / \"submission1.csv\", index=False)\n",
    "print(f\"\\nDone! Saved submission to {WORKING_DIR / 'submission1.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18cc3f",
   "metadata": {
    "papermill": {
     "duration": 0.017555,
     "end_time": "2025-12-13T17:39:16.053763",
     "exception": false,
     "start_time": "2025-12-13T17:39:16.036208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BoisterParrot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a04867",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:16.090880Z",
     "iopub.status.busy": "2025-12-13T17:39:16.090604Z",
     "iopub.status.idle": "2025-12-13T17:39:16.207908Z",
     "shell.execute_reply": "2025-12-13T17:39:16.207109Z"
    },
    "papermill": {
     "duration": 0.13736,
     "end_time": "2025-12-13T17:39:16.209438",
     "exception": false,
     "start_time": "2025-12-13T17:39:16.072078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "\n",
    "# 1) Xóa mọi thứ trong /kaggle/working trừ .csv\n",
    "for path in WORKING_DIR.iterdir():\n",
    "    # giữ lại file .csv\n",
    "    if path.is_file() and path.suffix == \".csv\":\n",
    "        continue\n",
    "\n",
    "    if path.is_file():\n",
    "        try:\n",
    "            path.unlink()\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove file {path}: {e}\")\n",
    "    elif path.is_dir():\n",
    "        try:\n",
    "            shutil.rmtree(path, ignore_errors=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove dir {path}: {e}\")\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b965bcc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:16.250210Z",
     "iopub.status.busy": "2025-12-13T17:39:16.249946Z",
     "iopub.status.idle": "2025-12-13T17:39:16.378694Z",
     "shell.execute_reply": "2025-12-13T17:39:16.377696Z"
    },
    "papermill": {
     "duration": 0.150893,
     "end_time": "2025-12-13T17:39:16.380068",
     "exception": false,
     "start_time": "2025-12-13T17:39:16.229175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== START INFERENCE ===\n",
      "\n",
      "Done! Saved submission to /kaggle/working/submission2.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "np.seterr(invalid=\"ignore\", divide=\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    \"\"\"\n",
    "    Chứa cấu hình tham số (Hyperparameters).\n",
    "    \"\"\"\n",
    "    fps: float = 30.0\n",
    "    pix_per_cm: float = 1.0\n",
    "    smooth_sigma: float = 1.0\n",
    "    use_pairwise: bool = True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. AGENT CONTEXT\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class AgentContext:\n",
    "    \"\"\"\n",
    "    Container chứa dữ liệu đã tiền xử lý của một con chuột.\n",
    "    Giúp tránh việc tính toán lại vận tốc/gia tốc nhiều lần.\n",
    "    \"\"\"\n",
    "    idx: pd.Index          # Index frame\n",
    "    pos: np.ndarray        # [F, 2] cm\n",
    "    vel: np.ndarray        # [F, 2] cm/s\n",
    "    speed: np.ndarray      # [F, 1] cm/s\n",
    "    acc: np.ndarray        # [F, 2] cm/s^2\n",
    "    \n",
    "    cx: pd.Series          # Series tọa độ X (để dùng rolling)\n",
    "    cy: pd.Series          # Series tọa độ Y\n",
    "    speed_series: pd.Series # Series tốc độ\n",
    "    \n",
    "    raw_df: Optional[pd.DataFrame] = None # Dữ liệu gốc các bộ phận \n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. FEATURE EXTRACTOR\n",
    "# =============================================================================\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Class trích xuất đặc trưng hành vi từ dữ liệu tracking.\n",
    "    \"\"\"\n",
    "    def __init__(self, fps: float, pix_per_cm: float, smooth_sigma: float = 1.0, use_pairwise: bool = True):\n",
    "        # Map tham số từ init vào Config\n",
    "        self.cfg = FeatureConfig(\n",
    "            fps=float(fps), \n",
    "            pix_per_cm=float(pix_per_cm), \n",
    "            smooth_sigma=smooth_sigma,\n",
    "            use_pairwise=use_pairwise\n",
    "        )\n",
    "        \n",
    "        # Đăng ký các hàm feature sẽ chạy\n",
    "        self.feature_registry = {\n",
    "            \"kinematics\": self._feat_basic_kinematics,\n",
    "            \"multiscale\": self._feat_multiscale,\n",
    "            \"long_range\": self._feat_long_range,\n",
    "            \"cumulative\": self._feat_cumulative,\n",
    "            \"curvature\": self._feat_curvature,\n",
    "            \"speed_asym\": self._feat_speed_asym,\n",
    "            \"gauss_shift\": self._feat_gauss_shift,\n",
    "            \"avoid\": self._feat_avoidance_trajectory,\n",
    "            \"pose\": self._feat_pose_shape,\n",
    "            \"a\": self._feat_follow_pattern,\n",
    "            \"b\": self._feat_shortburst_social,\n",
    "            \"pairwise\": self._feat_pairwise\n",
    "        }\n",
    "\n",
    "    # --- Helpers ---\n",
    "    def _scale(self, n_frames_30fps: int) -> int:\n",
    "        \"\"\"Quy đổi số frame từ chuẩn 30fps sang fps thực tế của video.\"\"\"\n",
    "        return max(1, int(round(n_frames_30fps * self.cfg.fps / 30.0)))\n",
    "\n",
    "    def _to_cm(self, arr):\n",
    "        \"\"\"Chuyển pixel -> cm.\"\"\"\n",
    "        return arr / self.cfg.pix_per_cm\n",
    "\n",
    "    def _smooth(self, x):\n",
    "        \"\"\"Làm mượt dữ liệu bằng Gaussian filter.\"\"\"\n",
    "        if self.cfg.smooth_sigma is None or x.shape[0] < 3: return x\n",
    "        if np.all(np.isnan(x)): return x\n",
    "        return gaussian_filter1d(x, sigma=self.cfg.smooth_sigma, axis=0, mode=\"nearest\")\n",
    "\n",
    "    def _forward_fill_nan(self, pos):\n",
    "        \"\"\"\n",
    "        Điền dữ liệu thiếu (NaN) bằng giá trị hợp lệ trước đó (Forward Fill).\n",
    "        \"\"\"\n",
    "        if np.all(np.isnan(pos)):\n",
    "            return np.zeros_like(pos)\n",
    "\n",
    "        pos_ffill = pos.copy()\n",
    "        mask = np.any(~np.isnan(pos_ffill), axis=1)\n",
    "        if not mask.any():\n",
    "            return np.zeros_like(pos_ffill)\n",
    "\n",
    "        valid_idx = np.where(mask)[0]\n",
    "        first, last = valid_idx[0], valid_idx[-1]\n",
    "        pos_ffill[:first] = pos_ffill[first]\n",
    "        pos_ffill[last + 1:] = pos_ffill[last]\n",
    "        df_temp = pd.DataFrame(pos_ffill)\n",
    "        df_temp = df_temp.ffill()\n",
    "        return df_temp.to_numpy()\n",
    "    \n",
    "    def _speed_series(self, cx: pd.Series, cy: pd.Series) -> pd.Series:\n",
    "        dx = cx.diff()\n",
    "        dy = cy.diff()\n",
    "        v = np.hypot(dx, dy).fillna(0.0) * self.cfg.fps\n",
    "        return v.astype(\"float32\")\n",
    "    \n",
    "    def _roll_future_mean(self, s: pd.Series, w: int, min_p: int = 1) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).mean().iloc[::-1]\n",
    "\n",
    "    def _roll_future_var(self, s: pd.Series, w: int, min_p: int = 2) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).var().iloc[::-1]\n",
    "\n",
    "    # --- Core Logic ---\n",
    "    def _compute_kinematics(self, pos_px: np.ndarray):\n",
    "        \"\"\"\n",
    "        Tính toán vật lý cơ bản: Pos(cm), Vel, Speed, Acc.\n",
    "        Input: Array [Frames, 2] (pixel).\n",
    "        Output: Tuple (pos_cm, vel, speed, acc).\n",
    "        \"\"\"\n",
    "        pos_ffill = self._forward_fill_nan(pos_px)\n",
    "        pos_cm = self._to_cm(pos_ffill.astype(np.float32))\n",
    "        pos_cm = self._smooth(pos_cm)                                               # [F, 2]\n",
    "\n",
    "        dt = 1.0 / self.cfg.fps\n",
    "        vel = np.zeros_like(pos_cm, dtype=np.float32)\n",
    "        vel[1:] = (pos_cm[1:] - pos_cm[:-1]) / dt                                   # [F, 2: (vx, vy)]\n",
    "        speed = np.linalg.norm(vel, axis=1, keepdims=True).astype(np.float32)       # [F, 1]\n",
    "\n",
    "        acc = np.zeros_like(pos_cm, dtype=np.float32)                          \n",
    "        acc[1:] = (vel[1:] - vel[:-1]) / dt                                         # [F, 2:(ax, ay)]\n",
    "        return pos_cm.astype(np.float32), vel, speed, acc\n",
    "\n",
    "    def _build_context(self, frames, pos_px, mouse_df=None) -> AgentContext:\n",
    "        \"\"\"\n",
    "        Tạo AgentContext chứa đầy đủ thông tin vật lý của 1 con chuột.\n",
    "        \"\"\"\n",
    "        p, v, s, a = self._compute_kinematics(pos_px)\n",
    "        idx = pd.Index(frames, name=\"frame\")\n",
    "        \n",
    "        return AgentContext(\n",
    "            idx=idx, pos=p, vel=v, speed=s, acc=a, \n",
    "            cx=pd.Series(p[:, 0], index=idx), \n",
    "            cy=pd.Series(p[:, 1], index=idx), \n",
    "            speed_series=pd.Series(s[:, 0], index=idx), \n",
    "            raw_df=mouse_df\n",
    "        )\n",
    "\n",
    "    # --- Feature Modules ---\n",
    "    def _feat_basic_kinematics(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Lấy các giá trị thô: tọa độ x, y, vận tốc vx, vy, tốc độ, gia tốc ax, ay.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"a_x\": ctx.pos[:, 0], \"a_y\": ctx.pos[:, 1],\n",
    "            \"a_vx\": ctx.vel[:, 0], \"a_vy\": ctx.vel[:, 1],\n",
    "            \"a_speed\": ctx.speed[:, 0],\n",
    "            \"a_ax\": ctx.acc[:, 0], \"a_ay\": ctx.acc[:, 1]\n",
    "        }\n",
    "\n",
    "    def _feat_multiscale(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tính tốc độ trung bình (Mean) và độ lệch chuẩn (Std) ở đa mức thời gian.\n",
    "        Feature 'sp_ratio' đo độ bùng nổ (Burstiness).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        speed = ctx.speed_series\n",
    "        frame_scales = [10, 40, 160]\n",
    "        for scale in frame_scales:\n",
    "            ws = self._scale(scale)\n",
    "            if len(speed) >= ws:\n",
    "                roller = speed.rolling(ws, min_periods=max(1, ws//4), center=True)\n",
    "                feats[f\"sp_m{scale}\"] = roller.mean().astype(\"float32\")\n",
    "                feats[f\"sp_s{scale}\"] = roller.std().astype(\"float32\")\n",
    "        feats[f\"sp_ratio\"] = feats[\"sp_m10\"] / (feats[\"sp_m160\"] + 1e-6)\n",
    "        return feats \n",
    "\n",
    "    \n",
    "        \n",
    "    def _feat_long_range(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng ngữ cảnh dài hạn:\n",
    "        - x_ml, y_ml: Vị trí trung bình trong quá khứ.\n",
    "        - sp_pct: Xếp hạng (percentile) của tốc độ hiện tại so với quá khứ.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        speed = ctx.speed_series\n",
    "\n",
    "        for window in [120, 240]:\n",
    "            ws = self._scale(window)\n",
    "            if len(ctx.cx) >= ws:\n",
    "                feats[f\"x_ml{window}\"] = ctx.cx.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "                feats[f\"y_ml{window}\"] = ctx.cy.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "\n",
    "        for span in [60, 120]:\n",
    "            s = self._scale(span)\n",
    "            feats[f\"x_e{span}\"] = ctx.cx.ewm(span=s, min_periods=1).mean()\n",
    "            feats[f\"y_e{span}\"] = ctx.cy.ewm(span=s, min_periods=1).mean()\n",
    "\n",
    "        for window in [60, 120]:\n",
    "            ws = self._scale(window)\n",
    "            if len(speed) >= ws:\n",
    "                feats[f\"sp_pct{window}\"] = speed.rolling(\n",
    "                    ws, min_periods=max(5, ws // 6), center=True\n",
    "                ).rank(pct=True)\n",
    "        return feats\n",
    "    \n",
    "\n",
    "    def _feat_curvature(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        feats = {}\n",
    "\n",
    "        vel_x, vel_y = ctx.vel[:, 0], ctx.vel[:, 1]\n",
    "        acc_x, acc_y = ctx.acc[:, 0], ctx.acc[:, 1]\n",
    "        cross_prod = vel_x * acc_y - vel_y * acc_x\n",
    "        vel_mag = np.sqrt(vel_x**2 + vel_y**2)\n",
    "        moving_mask = vel_mag > 2.0\n",
    "        vel_mag_safe = np.maximum(vel_mag, 0.1 / self.cfg.fps)\n",
    "        raw_curv = cross_prod / (vel_mag_safe**3)\n",
    "        raw_curv = np.where(moving_mask, raw_curv, 0.0)\n",
    "        min_turn_radius_cm = 0.5\n",
    "        max_k = 1.0 / min_turn_radius_cm\n",
    "        raw_curv = np.clip(raw_curv, -max_k, max_k)\n",
    "        abs_curv = np.abs(raw_curv)\n",
    "        abs_curv_series = pd.Series(abs_curv, index=ctx.idx)\n",
    "\n",
    "        for w in [30, 60]:\n",
    "            ws = self._scale(w)\n",
    "            min_p = max(ws // 3, 1)\n",
    "            feats[f\"curv_mean_{w}\"] = abs_curv_series.rolling(ws, min_periods=min_p).mean()\n",
    "\n",
    "        angle = np.arctan2(vel_y, vel_x)\n",
    "        angle_series = pd.Series(angle, index=ctx.idx)\n",
    "        angle_change = np.abs(angle_series.diff().fillna(0.0))\n",
    "        angle_change = np.where(angle_change > np.pi, 2 * np.pi - angle_change, angle_change)\n",
    "        angle_change_series = pd.Series(angle_change, index=ctx.idx)\n",
    "        angle_change_series = pd.Series(np.where(moving_mask, angle_change_series, 0.0), index=ctx.idx)\n",
    "\n",
    "        ws = self._scale(30)\n",
    "        feats[\"turn_rate_30\"] = angle_change_series.rolling(ws, min_periods=max(ws // 3, 1)).sum()\n",
    "\n",
    "        return feats\n",
    "    \n",
    "    def _feat_cumulative(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tổng quãng đường di chuyển trong một khoảng thời gian dài xung quanh frame hiện tại.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        L = max(1, self._scale(180))\n",
    "        step = np.hypot(ctx.cx.diff(), ctx.cy.diff()).fillna(0.0)\n",
    "        path = step.rolling(2 * L + 1, min_periods=max(5, L // 6), center=True).sum()\n",
    "        feats[\"path_cum180\"] =  path.fillna(0.0).astype(\"float32\")\n",
    "        return feats\n",
    "\n",
    "    def _feat_speed_asym(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Bất đối xứng tốc độ (Tương lai - Quá khứ).\n",
    "        \"\"\"\n",
    "        w = max(3, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        v_past = v.rolling(w, min_periods=1).mean()\n",
    "        v_fut = self._roll_future_mean(v, w, min_p=1)\n",
    "        return {\"spd_asym_1s\": (v_fut - v_past).fillna(0.0)}\n",
    "    \n",
    "    def _feat_gauss_shift(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Độ lệch Gaussian (KL Divergence) giữa quá khứ và tương lai.\n",
    "        Đo lường sự thay đổi trạng thái thống kê.\n",
    "        \"\"\"\n",
    "        w = max(5, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        mu_p = v.rolling(w, min_periods=1).mean()\n",
    "        va_p = v.rolling(w, min_periods=1).var().clip(lower=1e-6)\n",
    "        mu_f = self._roll_future_mean(v, w, min_p=1)\n",
    "        va_f = self._roll_future_var(v, w, min_p=1).clip(lower=1e-6)\n",
    "\n",
    "        kl_pf = 0.5 * (\n",
    "            (va_p / va_f) + ((mu_f - mu_p) ** 2) / va_f - 1.0 + np.log(va_f / va_p)\n",
    "        )\n",
    "        kl_fp = 0.5 * (\n",
    "            (va_f / va_p) + ((mu_p - mu_f) ** 2) / va_p - 1.0 + np.log(va_p / va_f)\n",
    "        )\n",
    "        return {\n",
    "            \"spd_symkl_1s\": (kl_pf + kl_fp).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        }\n",
    "\n",
    "    def _feat_avoidance_trajectory(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Tính toán quỹ đạo né tránh:\n",
    "        1. Relative Heading: Góc di chuyển so với hướng tới đối thủ.\n",
    "        2. Future Distance Gain: Dự báo xem hành động này có giúp chuột ra xa đối thủ trong tương lai không.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        if target_ctx is None: \n",
    "            return feats\n",
    "\n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        # Góc hướng tới địch (Angle to Target)\n",
    "        angle_to_target = np.arctan2(rel_vec[:, 1], rel_vec[:, 0])\n",
    "        \n",
    "        # Góc di chuyển của Tôi (My Heading)\n",
    "        my_heading = np.arctan2(ctx.vel[:, 1], ctx.vel[:, 0])\n",
    "        \n",
    "        # Độ lệch góc (Absolute Difference)\n",
    "        # Cần xử lý wrap góc (ví dụ: lệch giữa 179 độ và -179 độ là 2 độ chứ ko phải 358)\n",
    "        diff = np.abs(angle_to_target - my_heading)\n",
    "        diff = np.minimum(diff, 2*np.pi - diff) # Chuẩn hóa về [0, pi]\n",
    "        \n",
    "        # Feature: Cosine của góc lệch\n",
    "        # 1.0 (0 độ) -> Lao vào\n",
    "        # 0.0 (90 độ) -> AVOID (Lách ngang)\n",
    "        # -1.0 (180 độ) -> Escape\n",
    "        feats[\"heading_rel_cos\"] = pd.Series(np.cos(diff), index=idx, dtype=\"float32\")\n",
    "        \n",
    "        # Feature: Góc lệch tuyệt đối (đổi ra độ cho dễ hình dung nếu cần, ở đây để rad)\n",
    "        feats[\"heading_rel_abs\"] = pd.Series(diff, index=idx, dtype=\"float32\")\n",
    "\n",
    "\n",
    "        # --- 2. FUTURE DISTANCE GAIN (Hiệu quả tránh né) ---\n",
    "        # \"Sau 15 frame (0.5s) hoặc 30 frame (1s), mình có xa nó ra không?\"\n",
    "        \n",
    "        dist_now = np.linalg.norm(rel_vec, axis=1)\n",
    "        s_dist = pd.Series(dist_now, index=idx)\n",
    "        \n",
    "        scales = [15, 30] # 0.5s và 1s\n",
    "        for w in scales:\n",
    "            ws = self._scale(w)\n",
    "            \n",
    "            # Lấy khoảng cách ở tương lai (shift ngược lên)\n",
    "            # s.shift(-ws) là giá trị của t + ws\n",
    "            dist_future = s_dist.shift(-ws)\n",
    "            gain = dist_future - s_dist\n",
    "            \n",
    "            feats[f\"dist_gain_{w}f\"] = gain.fillna(0.0).astype(\"float32\")\n",
    "\n",
    "        return feats\n",
    "    \n",
    "    def _extract_part(self, ctx: AgentContext, part: str) -> Optional[np.ndarray]:\n",
    "        if ctx.raw_df is None: return None\n",
    "        if part not in ctx.raw_df.columns.get_level_values(0): return None\n",
    "        try:\n",
    "            sub_df = ctx.raw_df.xs(part, axis=1, level=0)[[\"x\", \"y\"]].reindex(ctx.idx)\n",
    "        except KeyError: return None\n",
    "        raw = sub_df.to_numpy()\n",
    "        raw = self._forward_fill_nan(raw)\n",
    "        cm = self._to_cm(raw.astype(np.float32))\n",
    "        return self._smooth(cm)\n",
    "    \n",
    "    def _extract_parts_dict(self, ctx: AgentContext, parts: List[str] = None) -> Dict[str, Optional[np.ndarray]]:\n",
    "        out = {}\n",
    "        for p in parts:\n",
    "            out[p] = self._extract_part(ctx, p)\n",
    "        return out\n",
    "        \n",
    "    def _feat_pose_shape(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Placeholder cho các đặc trưng hình dáng (Elongation, Body Angle...).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "\n",
    "        def zero(): return pd.Series(0.0, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        def dist(k1, k2):\n",
    "            p1, p2 = parts.get(k1), parts.get(k2)\n",
    "            if p1 is None or p2 is None: return zero()\n",
    "            d = np.linalg.norm(p1 - p2, axis=1)\n",
    "            return pd.Series(d, index=ctx.idx, dtype=\"float32\")\n",
    "        \n",
    "        def body_angle():\n",
    "            if parts.get(\"nose\") is None: return zero()\n",
    "            if parts.get(\"body_center\") is None: return zero()\n",
    "            if parts.get(\"tail_base\") is None: return zero()\n",
    "\n",
    "            v1 = parts.get(\"nose\") - parts.get(\"body_center\")\n",
    "            v2 = parts.get(\"tail_base\") - parts.get(\"body_center\")\n",
    "            dot_product = np.sum(v1 * v2, axis=1)\n",
    "            mag = np.linalg.norm(v1, axis=1) * np.linalg.norm(v2, axis=1)\n",
    "            cos_angle = np.clip(dot_product / (mag + 1e-6), -1.0, 1.0).astype(\"float32\")\n",
    "            return cos_angle\n",
    "        \n",
    "        # def elongation():\n",
    "        #     if parts.get(\"nose\")          is None: return zero()\n",
    "        #     if parts.get(\"tail_base\")     is None: return zero()\n",
    "        #     if parts.get(\"lateral_left\")  is None: return zero()\n",
    "        #     if parts.get(\"lateral_right\") is None: return zero()\n",
    "\n",
    "        #     d1 = dist(\"nose\", \"tail_base\")\n",
    "        #     d2 = dist(\"lateral_left\", \"lateral_right\")\n",
    "        #     elongation = d1 / (d2 + 1e-6).astype(\"float32\")\n",
    "        #     return elongation\n",
    "\n",
    "        \n",
    "        \n",
    "        def vel(part: str, n_frames_30fps: int) -> Dict:\n",
    "            part_pos = self._extract_part(ctx, part)\n",
    "            if part_pos is None: return zero()\n",
    "            \n",
    "            s_x = pd.Series(part_pos[:, 0], index=ctx.idx)\n",
    "            s_y = pd.Series(part_pos[:, 1], index=ctx.idx)\n",
    "            raw_speed = self._speed_series(s_x, s_y)\n",
    "\n",
    "            ws = self._scale(n_frames_30fps)\n",
    "            val = raw_speed.rolling(ws, min_periods=1, center=True).mean()\n",
    "            return val.astype(\"float32\")\n",
    "\n",
    "\n",
    "        target_parts = [\"nose\", \"body_center\", \"tail_base\", \n",
    "                        \"ear_left\", \"ear_right\"]\n",
    "        \n",
    "        parts = self._extract_parts_dict(ctx, target_parts)\n",
    "\n",
    "        # feats[\"a_body_width\"]                = dist(\"lateral_left\", \"lateral_right\")\n",
    "        # feats[\"aa_nose_bodycenter_dist\"]     = dist(\"nose\", \"body_center\")\n",
    "        # feats[\"aa_nose_tailbase_dist\"]       = dist(\"nose\", \"tail_base\")\n",
    "        # feats[\"aa_bodycenter_tailbase_dist\"] = dist(\"body_center\", \"tail_base\")\n",
    "        \n",
    "        # feats[\"aa_bodycenter_ear_l_dist\"]    = dist(\"body_center\", \"ear_left\")\n",
    "        # feats[\"aa_bodycenter_ear_r_dist\"]    = dist(\"body_center\", \"ear_right\")\n",
    "        # feats[\"aa_bodycenter_lateral_l_dist\"]= dist(\"body_center\", \"lateral_left\")\n",
    "        # feats[\"aa_bodycenter_lateral_r_dist\"]= dist(\"body_center\", \"lateral_right\")\n",
    "        \n",
    "        feats[\"a_body_angle\"]                = body_angle()\n",
    "        # feats[\"a_elongation\"]                = elongation()\n",
    "        feats[\"a_tail_base_vel_500ms\"]       = vel(\"tail_base\", 15)\n",
    "        feats[\"a_tail_base_vel_1000ms\"]      = vel(\"tail_base\", 30)\n",
    "        feats[\"a_tail_base_vel_2000ms\"]      = vel(\"tail_base\", 60)\n",
    "        feats[\"a_tail_base_vel_3000ms\"]      = vel(\"tail_base\", 90)\n",
    "        feats[\"a_nose_vel_500ms\"]            = vel(\"nose\", 15)\n",
    "        feats[\"a_nose_vel_1000ms\"]           = vel(\"nose\", 30)\n",
    "        feats[\"a_nose_vel_2000ms\"]           = vel(\"nose\", 60)\n",
    "        feats[\"a_nose_vel_3000ms\"]           = vel(\"nose\", 90)\n",
    "        feats[\"a_ear_right_vel_500ms\"]       = vel(\"ear_right\", 15)\n",
    "        feats[\"a_ear_right_vel_1000ms\"]      = vel(\"ear_right\", 30)\n",
    "        feats[\"a_ear_right_vel_2000ms\"]      = vel(\"ear_right\", 60)\n",
    "        feats[\"a_ear_right_vel_3000ms\"]      = vel(\"ear_right\", 90)\n",
    "\n",
    "        return feats\n",
    "\n",
    "    def _feat_shortburst_social(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Short-burst social features (10–30 frames) đặc biệt cho attack / chase / escape.\n",
    "        Chỉ dùng được khi có target_ctx.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- Lấy lại vài quantity cơ bản từ pairwise/avoidance ---\n",
    "        # vector Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # unit vector\n",
    "        rel_dist_safe = np.where(rel_dist == 0, 1e-6, rel_dist)\n",
    "        u_vec = rel_vec / rel_dist_safe[:, None]\n",
    "    \n",
    "        # velocity dọc trục nối (approach speed)\n",
    "        a_vel = ctx.vel\n",
    "        t_vel = target_ctx.vel\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)                # +: lao vào target\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1)             # +: target lao vào agent\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)    # +: lại gần nhau\n",
    "    \n",
    "        a_along_s = pd.Series(a_along, index=idx, dtype=\"float32\")\n",
    "        t_along_s = pd.Series(t_along, index=idx, dtype=\"float32\")\n",
    "        rel_along_s = pd.Series(rel_along, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # speed agent / target\n",
    "        a_speed = ctx.speed_series\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\"\n",
    "        )\n",
    "    \n",
    "        # heading_rel_cos ~ escape / approach\n",
    "        # vector body của agent\n",
    "        # (reuse idea từ _feat_pairwise)\n",
    "        # head ~ nose, tail ~ tail_base/body_center\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\", \"body_center\"])\n",
    "        head_a = parts_a.get(\"nose\")\n",
    "        tail_a = parts_a.get(\"tail_base\") if parts_a.get(\"tail_base\") is not None else parts_a.get(\"body_center\")\n",
    "    \n",
    "        if head_a is not None and tail_a is not None:\n",
    "            body_vec_a = head_a - tail_a\n",
    "            dot = np.sum(body_vec_a * rel_vec, axis=1)\n",
    "            mag = np.linalg.norm(body_vec_a, axis=1) * rel_dist_safe\n",
    "            heading_cos = np.clip(dot / (mag + 1e-6), -1.0, 1.0)\n",
    "            heading_cos_s = pd.Series(heading_cos, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            heading_cos_s = zero()\n",
    "    \n",
    "        # --- Rolling window 10, 20, 30 frames (ở fps gốc) ---\n",
    "        for w30 in [10, 20, 30]:\n",
    "            ws = self._scale(w30)\n",
    "            min_p = max(1, ws // 3)\n",
    "    \n",
    "            # Attack-like: approach mạnh, khoảng cách giảm nhanh\n",
    "            feats[f\"sb_att_approach_mean_{w30}\"] = a_along_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_att_rel_along_mean_{w30}\"] = rel_along_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_att_dist_delta_{w30}\"] = (rel_dist_s - rel_dist_s.shift(ws)).fillna(0.0)\n",
    "    \n",
    "            # Chase-like: agent & target đều nhanh, dist tương đối nhỏ\n",
    "            feats[f\"sb_chase_speed_agent_mean_{w30}\"] = a_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_chase_speed_target_mean_{w30}\"] = t_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_chase_dist_mean_{w30}\"] = rel_dist_s.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            # Escape-like: heading ngược, dist tăng nhanh\n",
    "            feats[f\"sb_esc_heading_cos_mean_{w30}\"] = heading_cos_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_esc_dist_gain_{w30}\"] = (rel_dist_s.shift(-ws) - rel_dist_s).fillna(0.0)\n",
    "    \n",
    "        # clip & fillna\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = v.replace([np.inf, -np.inf], np.nan).fillna(0.0).astype(\"float32\")\n",
    "    \n",
    "        return feats\n",
    "\n",
    "\n",
    "    def _feat_follow_pattern(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng hành vi FOLLOW:\n",
    "          - Agent ở gần target\n",
    "          - Cùng hướng (body + velocity)\n",
    "          - Tốc độ vừa phải\n",
    "          - Khoảng cách tương đối ổn định trong 0.5–1s\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 1. CÁC ĐẠI LƯỢNG CƠ BẢN ---\n",
    "        # Vector Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # Speed agent/target\n",
    "        a_speed = ctx.speed_series.astype(\"float32\")\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "    \n",
    "        # Body vector: nose - tail/body_center\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\", \"body_center\"])\n",
    "        parts_t = self._extract_parts_dict(target_ctx, [\"nose\", \"tail_base\", \"body_center\"])\n",
    "    \n",
    "        def body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"nose\")\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if tail is None:\n",
    "                tail = parts_dict.get(\"body_center\")\n",
    "            if head is None or tail is None:\n",
    "                return None\n",
    "            return head - tail\n",
    "    \n",
    "        a_body = body_vec(parts_a)\n",
    "        t_body = body_vec(parts_t)\n",
    "    \n",
    "        if a_body is not None and t_body is not None:\n",
    "            dot_bt = np.sum(a_body * t_body, axis=1)\n",
    "            mag_bt = np.linalg.norm(a_body, axis=1) * np.linalg.norm(t_body, axis=1)\n",
    "            cos_body = np.clip(dot_bt / (mag_bt + 1e-6), -1.0, 1.0)\n",
    "            cos_body_s = pd.Series(cos_body, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            cos_body_s = zero()\n",
    "    \n",
    "        # Velocity hướng\n",
    "        a_vel = ctx.vel\n",
    "        t_vel = target_ctx.vel\n",
    "        a_speed_np = np.linalg.norm(a_vel, axis=1)\n",
    "        t_speed_np = np.linalg.norm(t_vel, axis=1)\n",
    "        moving_mask = (a_speed_np > 1e-3) & (t_speed_np > 1e-3)\n",
    "    \n",
    "        # cos giữa hướng velocity 2 con\n",
    "        dot_v = np.sum(a_vel * t_vel, axis=1)\n",
    "        mag_v = a_speed_np * t_speed_np + 1e-6\n",
    "        cos_vel = np.zeros_like(dot_v, dtype=\"float32\")\n",
    "        cos_vel[moving_mask] = np.clip(dot_v[moving_mask] / mag_v[moving_mask], -1.0, 1.0)\n",
    "        cos_vel_s = pd.Series(cos_vel, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 2. WINDOW NGẮN (FOLLOW LÀ PATTERN DÀI HƠN ATTACK) ---\n",
    "        for w30 in [15, 30, 60]:   # ~0.5s, 1s, 2s\n",
    "            ws = self._scale(w30)\n",
    "            min_p = max(ws // 3, 1)\n",
    "    \n",
    "            # Khoảng cách trung bình & độ dao động\n",
    "            m_dist = rel_dist_s.rolling(ws, min_periods=min_p).mean()\n",
    "            s_dist = rel_dist_s.rolling(ws, min_periods=min_p).std()\n",
    "    \n",
    "            # Cùng hướng (body + velocity)\n",
    "            m_cos_body = cos_body_s.rolling(ws, min_periods=min_p).mean()\n",
    "            m_cos_vel  = cos_vel_s.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            # Tốc độ vừa phải\n",
    "            m_sp_a = a_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            m_sp_t = t_speed.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            feats[f\"follow_dist_mean_{w30}\"] = m_dist\n",
    "            feats[f\"follow_dist_std_{w30}\"]  = s_dist\n",
    "            feats[f\"follow_cos_body_mean_{w30}\"] = m_cos_body\n",
    "            feats[f\"follow_cos_vel_mean_{w30}\"]  = m_cos_vel\n",
    "            feats[f\"follow_speed_agent_mean_{w30}\"] = m_sp_a\n",
    "            feats[f\"follow_speed_target_mean_{w30}\"] = m_sp_t\n",
    "    \n",
    "        # Clean\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "        \n",
    "\n",
    "    def _feat_pairwise(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng tương tác cặp đôi (Pairwise): Khoảng cách, Tốc độ tiếp cận.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None: \n",
    "            return feats\n",
    "\n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "\n",
    "        # --- 1. KHOẢNG CÁCH CƠ BẢN (DISTANCES) ---\n",
    "        # Vector nối Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        feats[\"rel_dist\"] = pd.Series(dist, index=idx, dtype=\"float32\")\n",
    "\n",
    "        # --- 2. KHOẢNG CÁCH CHI TIẾT (NOSE-TO-PART) ---\n",
    "        # Lấy các bộ phận quan trọng\n",
    "        my_parts = self._extract_parts_dict(ctx, [\"nose\"])\n",
    "        target_parts = self._extract_parts_dict(target_ctx, \n",
    "            [\"nose\", \"tail_base\", \"body_center\", \"ear_left\", \"ear_right\"])\n",
    "\n",
    "        def dist_ab(pt_a, pt_b):\n",
    "            if pt_a is None or pt_b is None: return zero()\n",
    "            d = np.linalg.norm(pt_a - pt_b, axis=1)\n",
    "            return pd.Series(d, index=idx, dtype=\"float32\")\n",
    "\n",
    "        an, tn = my_parts[\"nose\"], target_parts[\"nose\"]\n",
    "        feats[\"dist_nose_nose\"] = dist_ab(an, tn)\n",
    "        feats[\"dist_nose_tail\"] = dist_ab(an, target_parts[\"tail_base\"])\n",
    "        feats[\"dist_nose_body\"] = dist_ab(an, target_parts[\"body_center\"])\n",
    "        feats[\"dist_nose_el\"]   = dist_ab(an, target_parts[\"ear_left\"])\n",
    "        feats[\"dist_nose_er\"]   = dist_ab(an, target_parts[\"ear_right\"])\n",
    "        # feats[\"dist_nose_tll\"]  = dist_ab(an, target_parts[\"lateral_left\"])\n",
    "        # feats[\"dist_nose_tlr\"]  = dist_ab(an, target_parts[\"lateral_right\"])\n",
    "        # feats[\"dist_nose_tt\"]  = dist_ab(an, target_parts[\"tail_tip\"])\n",
    "\n",
    "        # --- 3. ĐỊNH HƯỚNG & GÓC NHÌN (ORIENTATION & GAZE) ---\n",
    "        # Helper lấy vector cơ thể (Mũi - Đuôi/Thân)\n",
    "        def get_body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"nose\")\n",
    "            # Ưu tiên đuôi, nếu ko có thì dùng thân\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if tail is None: tail = parts_dict.get(\"body_center\") # Fallback\n",
    "            \n",
    "            if head is not None and tail is not None:\n",
    "                return head - tail\n",
    "            return None\n",
    "\n",
    "        a_vec = get_body_vec(my_parts)\n",
    "        t_vec = get_body_vec(target_parts)\n",
    "\n",
    "        # A. Body Cosine: Hai con cùng chiều hay ngược chiều?\n",
    "        if a_vec is not None and t_vec is not None:\n",
    "            dot = np.sum(a_vec * t_vec, axis=1)\n",
    "            mags = np.linalg.norm(a_vec, axis=1) * np.linalg.norm(t_vec, axis=1)\n",
    "            feats[\"body_cosine\"] = pd.Series(\n",
    "                np.clip(dot / (mags + 1e-6), -1.0, 1.0), index=idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"body_cosine\"] = zero()\n",
    "\n",
    "        # B. Gaze Cosine: Tôi có đang nhìn về phía Target không?\n",
    "        # Vector ánh nhìn = Target_Pos - My_Pos = rel_vec\n",
    "        if a_vec is not None:\n",
    "            dot_gaze = np.sum(a_vec * rel_vec, axis=1)\n",
    "            mag_a = np.linalg.norm(a_vec, axis=1)\n",
    "            # dist đã tính ở bước 1\n",
    "            feats[\"gaze_cosine\"] = pd.Series(\n",
    "                np.clip(dot_gaze / (mag_a * dist + 1e-6), -1.0, 1.0),\n",
    "                index=idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"gaze_cosine\"] = zero()\n",
    "\n",
    "        # --- 4. PHÂN RÃ VẬN TỐC (VELOCITY DECOMPOSITION) - CHÌA KHÓA CHO AVOID/ESCAPE ---\n",
    "        # Vector đơn vị hướng về địch (u)\n",
    "        dist_safe = dist.copy()\n",
    "        dist_safe[dist_safe == 0] = 1e-6\n",
    "        u_vec = rel_vec / dist_safe[:, None]\n",
    "\n",
    "        # a_vel và t_vel lấy từ Context\n",
    "        a_vel, t_vel = ctx.vel, target_ctx.vel\n",
    "\n",
    "        # A. Approach Speed (Vận tốc dọc trục nối 2 con)\n",
    "        # Dương: Lao vào nhau | Âm: Chạy ra xa nhau\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1) # Target hướng ngược lại\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)\n",
    "\n",
    "        # B. Lateral Speed (Vận tốc ngang - Vuông góc trục nối)\n",
    "        # Vector chiếu: v_proj = (v . u) * u\n",
    "        a_proj = a_along[:, None] * u_vec\n",
    "        a_lat_vec = a_vel - a_proj\n",
    "        a_lat_speed = np.linalg.norm(a_lat_vec, axis=1)\n",
    "\n",
    "        feats[\"approach_speed_agent\"]  = pd.Series(a_along, index=idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_target\"] = pd.Series(t_along, index=idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_rel\"]    = pd.Series(rel_along, index=idx, dtype=\"float32\")\n",
    "        feats[\"lateral_speed_agent\"]   = pd.Series(a_lat_speed, index=idx, dtype=\"float32\")\n",
    "        return feats\n",
    "        \n",
    "\n",
    "    # --- Methods tương thích ---\n",
    "    \n",
    "    def build_pose_tensor(self, tracking: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Chuyển dữ liệu tracking (DataFrame) sang Tensor [Frames, Mice, 2] và Dict chi tiết.\n",
    "        \"\"\"\n",
    "        tracking = tracking.sort_values(\"video_frame\")\n",
    "        frames = np.sort(tracking[\"video_frame\"].unique())\n",
    "        \n",
    "        pvid = tracking.pivot(\n",
    "            index=\"video_frame\", \n",
    "            columns=[\"mouse_id\", \"bodypart\"], \n",
    "            values=[\"x\", \"y\"]\n",
    "        )\n",
    "        pvid = pvid.reorder_levels([1, 2, 0], axis=1).sort_index(axis=1).astype(\"float32\")\n",
    "        mouse_ids = list(pvid.columns.get_level_values(0).unique())\n",
    "        pos = np.full((len(frames), len(mouse_ids), 2), np.nan, dtype=np.float32)\n",
    "        per_mouse_df = {}\n",
    "        \n",
    "        for i, mid in enumerate(mouse_ids):\n",
    "            single = pvid[mid]\n",
    "            per_mouse_df[mid] = single\n",
    "            \n",
    "            if \"body_center\" in single.columns.get_level_values(0):\n",
    "                cx = single[\"body_center\"][\"x\"]\n",
    "                cy = single[\"body_center\"][\"y\"]\n",
    "            else:\n",
    "                cx = single.xs(\"x\", level=1, axis=1).mean(axis=1)\n",
    "                cy = single.xs(\"y\", level=1, axis=1).mean(axis=1)\n",
    "            \n",
    "            pos[:, i, 0] = cx.reindex(frames).values\n",
    "            pos[:, i, 1] = cy.reindex(frames).values\n",
    "            \n",
    "        return frames, mouse_ids, pos, per_mouse_df\n",
    "\n",
    "    def extract_agent_target(\n",
    "        self, \n",
    "        frames: np.ndarray, \n",
    "        mouse_ids: List[Any], \n",
    "        pos: np.ndarray, \n",
    "        agent_id: Any, \n",
    "        target_id: Any, \n",
    "        per_mouse_df: Dict = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Trích xuất đặc trưng cho cặp (Agent, Target).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            aid_idx = mouse_ids.index(agent_id)\n",
    "        except ValueError:\n",
    "            return pd.DataFrame() \n",
    "\n",
    "        # 1. Build Agent Context\n",
    "        ctx_agent = self._build_context(\n",
    "            frames, \n",
    "            pos[:, aid_idx, :], \n",
    "            per_mouse_df.get(agent_id) if per_mouse_df else None\n",
    "        )\n",
    "\n",
    "        # 2. Build Target Context\n",
    "        ctx_target = None\n",
    "        if self.cfg.use_pairwise and target_id is not None and target_id in mouse_ids:\n",
    "             tid_idx = mouse_ids.index(target_id)\n",
    "             ctx_target = self._build_context(\n",
    "                 frames, \n",
    "                 pos[:, tid_idx, :], \n",
    "                 per_mouse_df.get(target_id) if per_mouse_df else None\n",
    "             )\n",
    "\n",
    "        # 3. Run all features\n",
    "        all_data = {}\n",
    "        for func_name, func in self.feature_registry.items():\n",
    "            out_dict = func(ctx_agent, target_ctx=ctx_target)\n",
    "            all_data.update(out_dict)\n",
    "\n",
    "        df_out = pd.DataFrame(all_data, index=ctx_agent.idx)\n",
    "        df_out = df_out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        \n",
    "        return df_out.reindex(sorted(df_out.columns), axis=1)\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# === IMPORT MODEL & OPTUNA ===\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "# Cấu hình\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "np.seterr(invalid=\"ignore\", divide=\"ignore\")\n",
    "\n",
    "# Metric\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/usr/lib/mabe-f-beta\")\n",
    "try:\n",
    "    from metric import score\n",
    "except ImportError:\n",
    "    def score(*args, **kwargs): return 0.0\n",
    "\n",
    "# =========================================================\n",
    "# 1. CẤU HÌNH & SEED\n",
    "# =========================================================\n",
    "SEED = 42\n",
    "def seed_everything(seed=42):\n",
    "    np.random.seed(seed)\n",
    "seed_everything(SEED)\n",
    "\n",
    "INPUT_DIR = Path(\"/kaggle/input/MABe-mouse-behavior-detection\")\n",
    "TRAIN_TRACKING_DIR = INPUT_DIR / \"train_tracking\"\n",
    "TRAIN_ANNOTATION_DIR = INPUT_DIR / \"train_annotation\"\n",
    "TEST_TRACKING_DIR = INPUT_DIR / \"test_tracking\"\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "RESULTS_DIR = Path(r\"/kaggle/input/results-ensemble-optuna1\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "SELF_BEHAVIORS = [\"biteobject\", \"climb\", \"dig\", \"exploreobject\", \"freeze\", \"genitalgroom\", \"huddle\", \"rear\", \"rest\", \"run\", \"selfgroom\"]\n",
    "PAIR_BEHAVIORS = [\"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\", \"chase\", \"chaseattack\", \"defend\", \"disengage\", \"dominance\", \"dominancegroom\", \"dominancemount\", \"ejaculate\", \"escape\", \"flinch\", \"follow\", \"intromit\", \"mount\", \"reciprocalsniff\", \"shepherd\", \"sniff\", \"sniffbody\", \"sniffface\", \"sniffgenital\", \"submit\", \"tussle\"]\n",
    "BAD_VIDEOS = []\n",
    "\n",
    "# =========================================================\n",
    "# 2. DATA LOADING & PREPARATION (NO CACHE)\n",
    "# =========================================================\n",
    "\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    return pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "\n",
    "def get_video_params(video_id: Any, meta: pd.DataFrame) -> Tuple[float, float]:\n",
    "    row = meta.loc[meta[\"video_id\"] == video_id]\n",
    "    if row.empty: return 30.0, 1.0\n",
    "    row = row.iloc[0]\n",
    "    return float(row[\"frames_per_second\"]), float(row[\"pix_per_cm_approx\"])\n",
    "\n",
    "def load_tracking(lab_id: str, video_id: Any, is_test=False) -> pd.DataFrame:\n",
    "    d = TEST_TRACKING_DIR if is_test else TRAIN_TRACKING_DIR\n",
    "    path = d / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists(): raise FileNotFoundError(path)\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def load_annotation(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    path = TRAIN_ANNOTATION_DIR / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists(): return pd.DataFrame(columns=[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "    return pd.read_parquet(path)[[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]]\n",
    "\n",
    "# Hàm lấy feature KHÔNG CACHE để tránh tràn RAM\n",
    "def get_frame_features_no_cache(lab_id, video_id, agent_id, target_id, meta, is_test=False):\n",
    "    if is_test:\n",
    "        row = meta[meta[\"video_id\"] == video_id].iloc[0]\n",
    "        fps, pix = float(row[\"frames_per_second\"]), float(row[\"pix_per_cm_approx\"])\n",
    "        pix = pix if np.isfinite(pix) and pix > 0 else 1.0\n",
    "    else:\n",
    "        fps, pix = get_video_params(video_id, meta)\n",
    "\n",
    "    tracking = load_tracking(lab_id, video_id, is_test)\n",
    "    \n",
    "    # === GỌI CLASS FeatureExtractor (Đã có ở cell trước) ===\n",
    "    fe = FeatureExtractor(fps=fps, pix_per_cm=pix, smooth_sigma=1.0, use_pairwise=True)\n",
    "    \n",
    "    frames, mouse_ids, pos, per_mouse_df = fe.build_pose_tensor(tracking)\n",
    "    \n",
    "    features_df = fe.extract_agent_target(\n",
    "        frames=frames, mouse_ids=mouse_ids, pos=pos,\n",
    "        agent_id=agent_id, target_id=target_id, per_mouse_df=per_mouse_df\n",
    "    )\n",
    "    features_df.index = frames\n",
    "    return frames, features_df\n",
    "\n",
    "def build_frame_dataset_for_lab_behavior(lab_id, behavior, train_meta, mode=\"self\"):\n",
    "    videos = train_meta[train_meta[\"lab_id\"] == lab_id][\"video_id\"].unique().tolist()\n",
    "    index_list, feature_list, label_list = [], [], []\n",
    "\n",
    "    for video_id in videos:\n",
    "        ann = load_annotation(lab_id, video_id)\n",
    "        if ann.empty: continue\n",
    "        \n",
    "        ann_bhv = ann[ann[\"action\"] == behavior]\n",
    "        if ann_bhv.empty: continue\n",
    "\n",
    "        pairs = ann_bhv[[\"agent_id\", \"target_id\"]].drop_duplicates().values.tolist()\n",
    "        for (agent_id, target_id) in pairs:\n",
    "            target_id_use = agent_id if mode == \"self\" else target_id\n",
    "            \n",
    "            # Lấy features (tính trực tiếp)\n",
    "            frames, feat_df = get_frame_features_no_cache(lab_id, video_id, agent_id, target_id_use, train_meta)\n",
    "\n",
    "            ann_pair = ann_bhv[(ann_bhv[\"agent_id\"] == agent_id) & (ann_bhv[\"target_id\"] == target_id)]\n",
    "            if ann_pair.empty and mode == \"self\": ann_pair = ann_bhv[ann_bhv[\"agent_id\"] == agent_id]\n",
    "\n",
    "            pos_frames = set()\n",
    "            for _, r in ann_pair.iterrows(): pos_frames.update(range(int(r[\"start_frame\"]), int(r[\"stop_frame\"])))\n",
    "            \n",
    "            if not pos_frames: continue\n",
    "            label = np.isin(frames, list(pos_frames)).astype(\"int8\")\n",
    "            if label.sum() == 0: continue\n",
    "\n",
    "            # Lưu vào list và reset index ngay để giảm memory overhead\n",
    "            index_list.append(pd.DataFrame({\"video_id\": video_id, \"agent_id\": agent_id, \"target_id\": target_id, \"video_frame\": frames}))\n",
    "            feature_list.append(feat_df.reset_index(drop=True))\n",
    "            label_list.append(label)\n",
    "            \n",
    "            # Dọn dẹp ngay\n",
    "            del frames, feat_df, label\n",
    "\n",
    "    if not index_list: return pd.DataFrame(), pd.DataFrame(), np.zeros(0, dtype=\"int8\")\n",
    "    \n",
    "    return pd.concat(index_list, ignore_index=True), pd.concat(feature_list, ignore_index=True), np.concatenate(label_list).astype(\"int8\")\n",
    "\n",
    "# =========================================================\n",
    "# 3. TRAINING & ENSEMBLE HELPERS\n",
    "# =========================================================\n",
    "\n",
    "def train_catboost_fold(X_tr, y_tr, X_va, y_va, sw=1.0):\n",
    "    p = {\n",
    "        'iterations': 1000, 'learning_rate': 0.05, 'depth': 6, 'scale_pos_weight': sw,\n",
    "        'task_type': 'GPU', 'devices': '0', 'verbose': 0, 'allow_writing_files': False,\n",
    "        'l2_leaf_reg': 5, 'bootstrap_type': 'Bernoulli', 'subsample': 0.8, 'random_seed': SEED\n",
    "    }\n",
    "    m = cb.CatBoostClassifier(**p)\n",
    "    m.fit(cb.Pool(X_tr, y_tr), eval_set=cb.Pool(X_va, y_va), early_stopping_rounds=20, use_best_model=True)\n",
    "    return m\n",
    "\n",
    "def train_lightgbm_fold(X_tr, y_tr, X_va, y_va, sw=1.0):\n",
    "    p = {\n",
    "        'objective': 'binary', 'metric': 'binary_logloss', 'learning_rate': 0.05,\n",
    "        'max_depth': 6, 'num_leaves': 31, 'scale_pos_weight': sw, 'device': 'gpu',\n",
    "        'verbosity': -1, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8,\n",
    "        'subsample_freq': 1, 'seed': SEED\n",
    "    }\n",
    "    m = lgb.train(p, lgb.Dataset(X_tr, y_tr), 1000, valid_sets=[lgb.Dataset(X_va, y_va)], callbacks=[lgb.early_stopping(20, verbose=False)])\n",
    "    return m\n",
    "\n",
    "def optimize_ensemble_weights(oof_dict, y_true):\n",
    "    models = list(oof_dict.keys())\n",
    "    def obj(trial):\n",
    "        w = [trial.suggest_float(m, 0.0, 1.0) for m in models]\n",
    "        s = sum(w) + 1e-6; w = [x/s for x in w]\n",
    "        p = np.zeros_like(y_true, dtype=float)\n",
    "        for i, m in enumerate(models): p += oof_dict[m] * w[i]\n",
    "        th = trial.suggest_float(\"th\", 0.1, 0.9)\n",
    "        return f1_score(y_true, (p >= th).astype(int), zero_division=0)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "    study.optimize(obj, n_trials=50)\n",
    "    best = study.best_params\n",
    "    th = best.pop(\"th\")\n",
    "    rw = [best[m] for m in models]; s = sum(rw)+1e-6\n",
    "    return {m: w/s for m, w in zip(models, rw)}, th\n",
    "\n",
    "def train_validate_ensemble(lab_id, behavior, indices, features, labels):\n",
    "    res_dir = RESULTS_DIR / lab_id / behavior\n",
    "    res_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if len(labels) == 0 or labels.sum() == 0: return 0.0\n",
    "\n",
    "    X = features.values.astype(\"float32\")\n",
    "    y = labels.astype(\"int8\")\n",
    "    groups = indices[\"video_id\"].values\n",
    "    \n",
    "    oof_preds = {m: np.zeros(len(y), dtype=\"float32\") for m in [\"xgb\", \"cat\", \"lgb\"]}\n",
    "    folds = np.ones(len(y), dtype=\"int8\") * -1\n",
    "\n",
    "    cv = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "    for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups=groups)):\n",
    "        print(f\"   Fold {fold}...\", end=\" \")\n",
    "        fd_dir = res_dir / f\"fold_{fold}\"; fd_dir.mkdir(parents=True, exist_ok=True)\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]; X_va, y_va = X[va_idx], y[va_idx]\n",
    "        pos = y_tr.sum(); neg = len(y_tr) - pos\n",
    "        sw = float(neg/pos) if pos > 0 else 1.0\n",
    "\n",
    "        # 1. XGBoost\n",
    "        dtr = xgb.QuantileDMatrix(X_tr, label=y_tr, feature_names=features.columns.tolist(), max_bin=64)\n",
    "        dva = xgb.DMatrix(X_va, label=y_va, feature_names=features.columns.tolist())\n",
    "        xp = {\n",
    "            \"objective\":\"binary:logistic\", \"eval_metric\":\"logloss\", \"device\":\"cuda\", \n",
    "            \"tree_method\":\"hist\", \"learning_rate\":0.05, \"max_depth\":6, \"scale_pos_weight\":sw,\n",
    "            \"min_child_weight\":5, \"subsample\":0.8, \"colsample_bytree\":0.8, \"max_bin\":64, \"seed\": SEED\n",
    "        }\n",
    "        \n",
    "        # === ĐÃ THÊM 'evals=' VÀO DÒNG DƯỚI ===\n",
    "        mx = xgb.train(\n",
    "            params=xp, \n",
    "            dtrain=dtr, \n",
    "            num_boost_round=1000, \n",
    "            evals=[(dva, \"valid\")],\n",
    "            callbacks=[xgb.callback.EarlyStopping(rounds=20, save_best=True)], \n",
    "            verbose_eval=False\n",
    "        )\n",
    "        mx.save_model(fd_dir / \"model_xgb.json\")\n",
    "        oof_preds[\"xgb\"][va_idx] = mx.predict(dva)\n",
    "\n",
    "        # 2. CatBoost\n",
    "        mc = train_catboost_fold(X_tr, y_tr, X_va, y_va, sw)\n",
    "        mc.save_model(str(fd_dir / \"model_cat.cbm\"))\n",
    "        oof_preds[\"cat\"][va_idx] = mc.predict_proba(X_va)[:,1]\n",
    "\n",
    "        # 3. LightGBM\n",
    "        ml = train_lightgbm_fold(X_tr, y_tr, X_va, y_va, sw)\n",
    "        ml.save_model(fd_dir / \"model_lgb.txt\")\n",
    "        oof_preds[\"lgb\"][va_idx] = ml.predict(X_va)\n",
    "        folds[va_idx] = fold\n",
    "        \n",
    "        print(\"Done.\")\n",
    "        del X_tr, y_tr, X_va, y_va, dtr, dva, mx, mc, ml\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"   Optimizing Weights...\", end=\" \")\n",
    "    weights, th = optimize_ensemble_weights(oof_preds, y)\n",
    "    with open(res_dir / \"ensemble_params.json\", \"w\") as f: json.dump({\"weights\": weights, \"threshold\": th}, f)\n",
    "    \n",
    "    final_pred = sum(oof_preds[m] * weights[m] for m in weights)\n",
    "    final_lbl = (final_pred >= th).astype(\"int8\")\n",
    "    \n",
    "    # Save OOF\n",
    "    df = indices.copy(); df[\"fold\"] = folds; df[\"pred\"] = final_pred; df[\"lbl\"] = final_lbl\n",
    "    df.to_parquet(res_dir / \"oof.parquet\", index=False)\n",
    "    \n",
    "    f1 = f1_score(y, final_lbl, zero_division=0)\n",
    "    print(f\"Best F1: {f1:.4f} (Th={th:.2f}, W={weights})\")\n",
    "    (res_dir / \"f1.txt\").write_text(f\"{f1:.6f}\")\n",
    "    return float(f1)\n",
    "\n",
    "# =========================================================\n",
    "# 4. INFERENCE\n",
    "# =========================================================\n",
    "\n",
    "def load_ensemble_models(lab_id, behavior):\n",
    "    base = RESULTS_DIR / lab_id / behavior\n",
    "    if not base.exists(): return []\n",
    "    models = []\n",
    "    for fd in sorted(base.glob(\"fold_*\")):\n",
    "        if not (fd / \"model_xgb.json\").exists(): continue\n",
    "        \n",
    "        xgb_b = xgb.Booster(); xgb_b.load_model(str(fd / \"model_xgb.json\"))\n",
    "        cat_m = cb.CatBoostClassifier(); \n",
    "        try: cat_m.load_model(str(fd / \"model_cat.cbm\"))\n",
    "        except: cat_m = None\n",
    "        try: lgb_m = lgb.Booster(model_file=str(fd / \"model_lgb.txt\"))\n",
    "        except: lgb_m = None\n",
    "        models.append({\"xgb\": xgb_b, \"cat\": cat_m, \"lgb\": lgb_m})\n",
    "    return models\n",
    "\n",
    "def predict_behaviors_for_pair(lab_id, video_id, aid, tid, behaviors, test_meta):\n",
    "    if lab_id != \"BoisterousParrot\": return None\n",
    "    frames, feat_df = get_frame_features_no_cache(lab_id, video_id, aid, tid, test_meta, is_test=True)\n",
    "    if feat_df.empty: return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "    \n",
    "    scores = {}\n",
    "    for bhv in behaviors:\n",
    "        base = RESULTS_DIR / lab_id / bhv\n",
    "        if not (base / \"ensemble_params.json\").exists(): continue\n",
    "        with open(base / \"ensemble_params.json\") as f: p = json.load(f)\n",
    "        ws, th = p[\"weights\"], p[\"threshold\"]\n",
    "        \n",
    "        folds = load_ensemble_models(lab_id, bhv)\n",
    "        if not folds: continue\n",
    "        \n",
    "        cols = folds[0][\"xgb\"].feature_names\n",
    "        X = pd.DataFrame(0.0, index=feat_df.index, columns=cols, dtype=np.float32)\n",
    "        c = list(set(cols) & set(feat_df.columns))\n",
    "        if c: X[c] = feat_df[c]\n",
    "        dtest = xgb.DMatrix(X, feature_names=cols)\n",
    "        \n",
    "        agg = np.zeros(len(feat_df), dtype=np.float32)\n",
    "        for m in folds:\n",
    "            px = m[\"xgb\"].predict(dtest)\n",
    "            pc = m[\"cat\"].predict_proba(X)[:,1] if m[\"cat\"] else np.zeros_like(px)\n",
    "            pl = m[\"lgb\"].predict(X) if m[\"lgb\"] else np.zeros_like(px)\n",
    "            \n",
    "            avg = px*ws.get(\"xgb\", 0.33) + pc*ws.get(\"cat\", 0.33) + pl*ws.get(\"lgb\", 0.33)\n",
    "            agg += avg * (avg >= th).astype(\"int8\")\n",
    "        \n",
    "        if folds: scores[bhv] = agg / len(folds)\n",
    "        \n",
    "        del X, dtest\n",
    "        gc.collect()\n",
    "\n",
    "    if not scores: return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "    \n",
    "    bl = list(scores.keys()); mat = np.vstack([scores[b] for b in bl]).T\n",
    "    lbls = np.where(mat.max(1)==0, \"none\", np.array(bl)[mat.argmax(1)])\n",
    "    \n",
    "    segs = []; prev = \"none\"; start = None; pf = None\n",
    "    for f, l in zip(frames, lbls):\n",
    "        if l != prev:\n",
    "            if prev != \"none\": segs.append({\"video_id\": int(video_id), \"action\": prev, \"start_frame\": int(start), \"stop_frame\": int(pf)+1})\n",
    "            prev = l; start = f\n",
    "        pf = f\n",
    "    if prev != \"none\": segs.append({\"video_id\": int(video_id), \"action\": prev, \"start_frame\": int(start), \"stop_frame\": int(pf)+1})\n",
    "    \n",
    "    return pd.DataFrame(segs)\n",
    "\n",
    "# =========================================================\n",
    "# 5. MAIN\n",
    "# =========================================================\n",
    "target_lab = \"BoisterousParrot\"\n",
    "\n",
    "print(\"\\n=== START INFERENCE ===\")\n",
    "test_meta = pd.read_csv(INPUT_DIR / \"test.csv\")\n",
    "test_meta = test_meta[test_meta[\"lab_id\"] == target_lab].reset_index(drop=True)\n",
    "\n",
    "trained = sorted([p.name for p in (RESULTS_DIR/target_lab).iterdir() if p.is_dir()])\n",
    "sb, pb = [b for b in trained if b in SELF_BEHAVIORS], [b for b in trained if b in PAIR_BEHAVIORS]\n",
    "\n",
    "all_segs = []\n",
    "def fid(i): return str(i) if str(i).startswith(\"mouse\") else f\"mouse{i}\"\n",
    "\n",
    "for vid in sorted(test_meta[\"video_id\"].unique()):\n",
    "    print(f\"Predicting Video {vid}...\")\n",
    "    tr = load_tracking(target_lab, vid, is_test=True)\n",
    "    mids = sorted(tr[\"mouse_id\"].unique())\n",
    "    \n",
    "    if sb:\n",
    "        for m in mids:\n",
    "            df = predict_behaviors_for_pair(target_lab, vid, m, m, sb, test_meta)\n",
    "            if df is not None and not df.empty:\n",
    "                df[\"agent_id\"] = fid(m); df[\"target_id\"] = \"self\"\n",
    "                all_segs.append(df)\n",
    "    if pb and len(mids) > 1:\n",
    "        for a, t in itertools.permutations(mids, 2):\n",
    "            df = predict_behaviors_for_pair(target_lab, vid, a, t, pb, test_meta)\n",
    "            if df is not None and not df.empty:\n",
    "                df[\"agent_id\"] = fid(a); df[\"target_id\"] = fid(t)\n",
    "                all_segs.append(df)\n",
    "    del tr\n",
    "    gc.collect()\n",
    "\n",
    "cols = [\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "\n",
    "if all_segs:\n",
    "    sub2 = pd.concat(all_segs, ignore_index=True)\n",
    "    sub2 = sub2[cols].sort_values([\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\"]).reset_index(drop=True)    \n",
    "    sub2.insert(0, \"row_id\", np.arange(len(sub2), dtype=np.int64))\n",
    "else:\n",
    "    sub2 = pd.DataFrame(columns=[\"row_id\"] + cols)\n",
    "\n",
    "sub2.to_csv(WORKING_DIR / \"submission2.csv\", index=False)\n",
    "print(f\"\\nDone! Saved submission to {WORKING_DIR / 'submission2.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d5229",
   "metadata": {
    "papermill": {
     "duration": 0.017829,
     "end_time": "2025-12-13T17:39:16.416964",
     "exception": false,
     "start_time": "2025-12-13T17:39:16.399135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ElegantMink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b962a981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:16.455205Z",
     "iopub.status.busy": "2025-12-13T17:39:16.454588Z",
     "iopub.status.idle": "2025-12-13T17:39:16.571813Z",
     "shell.execute_reply": "2025-12-13T17:39:16.571155Z"
    },
    "papermill": {
     "duration": 0.137775,
     "end_time": "2025-12-13T17:39:16.573086",
     "exception": false,
     "start_time": "2025-12-13T17:39:16.435311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "\n",
    "# 1) Xóa mọi thứ trong /kaggle/working trừ .csv\n",
    "for path in WORKING_DIR.iterdir():\n",
    "    # giữ lại file .csv\n",
    "    if path.is_file() and path.suffix == \".csv\":\n",
    "        continue\n",
    "\n",
    "    if path.is_file():\n",
    "        try:\n",
    "            path.unlink()\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove file {path}: {e}\")\n",
    "    elif path.is_dir():\n",
    "        try:\n",
    "            shutil.rmtree(path, ignore_errors=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove dir {path}: {e}\")\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edf021d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:16.618808Z",
     "iopub.status.busy": "2025-12-13T17:39:16.618586Z",
     "iopub.status.idle": "2025-12-13T17:39:16.792758Z",
     "shell.execute_reply": "2025-12-13T17:39:16.791741Z"
    },
    "papermill": {
     "duration": 0.200561,
     "end_time": "2025-12-13T17:39:16.794262",
     "exception": false,
     "start_time": "2025-12-13T17:39:16.593701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đọc test.csv cho lab ElegantMink ...\n",
      "Behaviors (self) dùng để predict: []\n",
      "Behaviors (pair) dùng để predict: ['allogroom', 'attack', 'attemptmount', 'ejaculate', 'intromit', 'mount', 'sniff']\n",
      "Saved ElegantMink submission to /kaggle/working/submission3.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "np.seterr(invalid=\"ignore\", divide=\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    \"\"\"\n",
    "    Chứa cấu hình tham số (Hyperparameters).\n",
    "    \"\"\"\n",
    "    fps: float = 30.0\n",
    "    pix_per_cm: float = 1.0\n",
    "    smooth_sigma: float = 1.0\n",
    "    use_pairwise: bool = True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. AGENT CONTEXT\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class AgentContext:\n",
    "    \"\"\"\n",
    "    Container chứa dữ liệu đã tiền xử lý của một con chuột.\n",
    "    Giúp tránh việc tính toán lại vận tốc/gia tốc nhiều lần.\n",
    "    \"\"\"\n",
    "    idx: pd.Index          # Index frame\n",
    "    pos: np.ndarray        # [F, 2] cm\n",
    "    vel: np.ndarray        # [F, 2] cm/s\n",
    "    speed: np.ndarray      # [F, 1] cm/s\n",
    "    acc: np.ndarray        # [F, 2] cm/s^2\n",
    "    \n",
    "    cx: pd.Series          # Series tọa độ X (để dùng rolling)\n",
    "    cy: pd.Series          # Series tọa độ Y\n",
    "    speed_series: pd.Series # Series tốc độ\n",
    "    \n",
    "    raw_df: Optional[pd.DataFrame] = None # Dữ liệu gốc các bộ phận \n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. FEATURE EXTRACTOR\n",
    "# =============================================================================\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Class trích xuất đặc trưng hành vi từ dữ liệu tracking.\n",
    "    \"\"\"\n",
    "    def __init__(self, fps: float, pix_per_cm: float, smooth_sigma: float = 1.0, use_pairwise: bool = True):\n",
    "        # Map tham số từ init vào Config\n",
    "        self.cfg = FeatureConfig(\n",
    "            fps=float(fps), \n",
    "            pix_per_cm=float(pix_per_cm), \n",
    "            smooth_sigma=smooth_sigma,\n",
    "            use_pairwise=use_pairwise\n",
    "        )\n",
    "        \n",
    "        # Đăng ký các hàm feature sẽ chạy\n",
    "        self.feature_registry = {\n",
    "            \"kinematics\": self._feat_basic_kinematics,\n",
    "            \"multiscale\": self._feat_multiscale,\n",
    "            \"long_range\": self._feat_long_range,\n",
    "            \"cumulative\": self._feat_cumulative,\n",
    "            \"curvature\": self._feat_curvature,\n",
    "            \"speed_asym\": self._feat_speed_asym,\n",
    "            \"gauss_shift\": self._feat_gauss_shift,\n",
    "            \"pose_shape\": self._feat_pose_shape,\n",
    "            \"pairwise\": self._feat_pairwise,\n",
    "            \"follow\": self._feat_follow_pattern,\n",
    "            \"short\": self._feat_shortburst_social,\n",
    "            \"a\": self._feat_attack_sniff,\n",
    "            \"b\": self._feat_climb,\n",
    "            \"c\": self._feat_ejaculate_temporal\n",
    "        }\n",
    "\n",
    "    # --- Helpers ---\n",
    "    def _scale(self, n_frames_30fps: int) -> int:\n",
    "        \"\"\"Quy đổi số frame từ chuẩn 30fps sang fps thực tế của video.\"\"\"\n",
    "        return max(1, int(round(n_frames_30fps * self.cfg.fps / 30.0)))\n",
    "\n",
    "    def _to_cm(self, arr):\n",
    "        \"\"\"Chuyển pixel -> cm.\"\"\"\n",
    "        return arr / self.cfg.pix_per_cm\n",
    "\n",
    "    def _smooth(self, x):\n",
    "        \"\"\"Làm mượt dữ liệu bằng Gaussian filter.\"\"\"\n",
    "        if self.cfg.smooth_sigma is None or x.shape[0] < 3: return x\n",
    "        if np.all(np.isnan(x)): return x\n",
    "        return gaussian_filter1d(x, sigma=self.cfg.smooth_sigma, axis=0, mode=\"nearest\")\n",
    "\n",
    "    def _forward_fill_nan(self, pos):\n",
    "        \"\"\"\n",
    "        Điền dữ liệu thiếu (NaN) bằng giá trị hợp lệ trước đó (Forward Fill).\n",
    "        \"\"\"\n",
    "        if np.all(np.isnan(pos)):\n",
    "            return np.zeros_like(pos)\n",
    "\n",
    "        pos_ffill = pos.copy()\n",
    "        mask = np.any(~np.isnan(pos_ffill), axis=1)\n",
    "        if not mask.any():\n",
    "            return np.zeros_like(pos_ffill)\n",
    "\n",
    "        valid_idx = np.where(mask)[0]\n",
    "        first, last = valid_idx[0], valid_idx[-1]\n",
    "        pos_ffill[:first] = pos_ffill[first]\n",
    "        pos_ffill[last + 1:] = pos_ffill[last]\n",
    "        df_temp = pd.DataFrame(pos_ffill)\n",
    "        df_temp = df_temp.ffill()\n",
    "        return df_temp.to_numpy()\n",
    "    \n",
    "    def _speed_series(self, cx: pd.Series, cy: pd.Series) -> pd.Series:\n",
    "        dx = cx.diff()\n",
    "        dy = cy.diff()\n",
    "        v = np.hypot(dx, dy).fillna(0.0) * self.cfg.fps\n",
    "        return v.astype(\"float32\")\n",
    "    \n",
    "    def _roll_future_mean(self, s: pd.Series, w: int, min_p: int = 1) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).mean().iloc[::-1]\n",
    "\n",
    "    def _roll_future_var(self, s: pd.Series, w: int, min_p: int = 2) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).var().iloc[::-1]\n",
    "\n",
    "    # --- Core Logic ---\n",
    "    def _compute_kinematics(self, pos_px: np.ndarray):\n",
    "        \"\"\"\n",
    "        Tính toán vật lý cơ bản: Pos(cm), Vel, Speed, Acc.\n",
    "        Input: Array [Frames, 2] (pixel).\n",
    "        Output: Tuple (pos_cm, vel, speed, acc).\n",
    "        \"\"\"\n",
    "        pos_ffill = self._forward_fill_nan(pos_px)\n",
    "        pos_cm = self._to_cm(pos_ffill.astype(np.float32))\n",
    "        pos_cm = self._smooth(pos_cm)                                               # [F, 2]\n",
    "\n",
    "        dt = 1.0 / self.cfg.fps\n",
    "        vel = np.zeros_like(pos_cm, dtype=np.float32)\n",
    "        vel[1:] = (pos_cm[1:] - pos_cm[:-1]) / dt                                   # [F, 2: (vx, vy)]\n",
    "        speed = np.linalg.norm(vel, axis=1, keepdims=True).astype(np.float32)       # [F, 1]\n",
    "\n",
    "        acc = np.zeros_like(pos_cm, dtype=np.float32)                          \n",
    "        acc[1:] = (vel[1:] - vel[:-1]) / dt                                         # [F, 2:(ax, ay)]\n",
    "        return pos_cm.astype(np.float32), vel, speed, acc\n",
    "\n",
    "    def _build_context(self, frames, pos_px, mouse_df=None) -> AgentContext:\n",
    "        \"\"\"\n",
    "        Tạo AgentContext chứa đầy đủ thông tin vật lý của 1 con chuột.\n",
    "        \"\"\"\n",
    "        p, v, s, a = self._compute_kinematics(pos_px)\n",
    "        idx = pd.Index(frames, name=\"frame\")\n",
    "        \n",
    "        return AgentContext(\n",
    "            idx=idx, pos=p, vel=v, speed=s, acc=a, \n",
    "            cx=pd.Series(p[:, 0], index=idx), \n",
    "            cy=pd.Series(p[:, 1], index=idx), \n",
    "            speed_series=pd.Series(s[:, 0], index=idx), \n",
    "            raw_df=mouse_df\n",
    "        )\n",
    "\n",
    "    # --- Feature Modules ---\n",
    "    def _feat_basic_kinematics(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Lấy các giá trị thô: tọa độ x, y, vận tốc vx, vy, tốc độ, gia tốc ax, ay.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"a_x\": ctx.pos[:, 0], \"a_y\": ctx.pos[:, 1],\n",
    "            \"a_vx\": ctx.vel[:, 0], \"a_vy\": ctx.vel[:, 1],\n",
    "            \"a_speed\": ctx.speed[:, 0],\n",
    "            \"a_ax\": ctx.acc[:, 0], \"a_ay\": ctx.acc[:, 1]\n",
    "        }\n",
    "\n",
    "    def _feat_multiscale(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tính tốc độ trung bình (Mean) và độ lệch chuẩn (Std) ở đa mức thời gian.\n",
    "        Feature 'sp_ratio' đo độ bùng nổ (Burstiness).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        speed = ctx.speed_series\n",
    "        frame_scales = [10, 40, 160]\n",
    "        for scale in frame_scales:\n",
    "            ws = self._scale(scale)\n",
    "            if len(speed) >= ws:\n",
    "                roller = speed.rolling(ws, min_periods=max(1, ws//4), center=True)\n",
    "                feats[f\"sp_m{scale}\"] = roller.mean().astype(\"float32\")\n",
    "                feats[f\"sp_s{scale}\"] = roller.std().astype(\"float32\")\n",
    "        feats[f\"sp_ratio\"] = feats[\"sp_m10\"] / (feats[\"sp_m160\"] + 1e-6)\n",
    "        return feats \n",
    "        \n",
    "    def _feat_long_range(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng ngữ cảnh dài hạn:\n",
    "        - x_ml, y_ml: Vị trí trung bình trong quá khứ.\n",
    "        - sp_pct: Xếp hạng (percentile) của tốc độ hiện tại so với quá khứ.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        speed = ctx.speed_series\n",
    "\n",
    "        for window in [120, 240]:\n",
    "            ws = self._scale(window)\n",
    "            if len(ctx.cx) >= ws:\n",
    "                feats[f\"x_ml{window}\"] = ctx.cx.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "                feats[f\"y_ml{window}\"] = ctx.cy.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "\n",
    "        for span in [60, 120]:\n",
    "            s = self._scale(span)\n",
    "            feats[f\"x_e{span}\"] = ctx.cx.ewm(span=s, min_periods=1).mean()\n",
    "            feats[f\"y_e{span}\"] = ctx.cy.ewm(span=s, min_periods=1).mean()\n",
    "\n",
    "        for window in [60, 120]:\n",
    "            ws = self._scale(window)\n",
    "            if len(speed) >= ws:\n",
    "                feats[f\"sp_pct{window}\"] = speed.rolling(\n",
    "                    ws, min_periods=max(5, ws // 6), center=True\n",
    "                ).rank(pct=True)\n",
    "        return feats\n",
    "    \n",
    "\n",
    "    def _feat_curvature(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        feats = {}\n",
    "\n",
    "        vel_x, vel_y = ctx.vel[:, 0], ctx.vel[:, 1]\n",
    "        acc_x, acc_y = ctx.acc[:, 0], ctx.acc[:, 1]\n",
    "        cross_prod = vel_x * acc_y - vel_y * acc_x\n",
    "        vel_mag = np.sqrt(vel_x**2 + vel_y**2)\n",
    "        moving_mask = vel_mag > 2.0\n",
    "        vel_mag_safe = np.maximum(vel_mag, 0.1 / self.cfg.fps)\n",
    "        raw_curv = cross_prod / (vel_mag_safe**3)\n",
    "        raw_curv = np.where(moving_mask, raw_curv, 0.0)\n",
    "        min_turn_radius_cm = 0.5\n",
    "        max_k = 1.0 / min_turn_radius_cm\n",
    "        raw_curv = np.clip(raw_curv, -max_k, max_k)\n",
    "        abs_curv = np.abs(raw_curv)\n",
    "        abs_curv_series = pd.Series(abs_curv, index=ctx.idx)\n",
    "\n",
    "        for w in [30, 60]:\n",
    "            ws = self._scale(w)\n",
    "            min_p = max(ws // 3, 1)\n",
    "            feats[f\"curv_mean_{w}\"] = abs_curv_series.rolling(ws, min_periods=min_p).mean()\n",
    "\n",
    "        angle = np.arctan2(vel_y, vel_x)\n",
    "        angle_series = pd.Series(angle, index=ctx.idx)\n",
    "        angle_change = np.abs(angle_series.diff().fillna(0.0))\n",
    "        angle_change = np.where(angle_change > np.pi, 2 * np.pi - angle_change, angle_change)\n",
    "        angle_change_series = pd.Series(angle_change, index=ctx.idx)\n",
    "        angle_change_series = pd.Series(np.where(moving_mask, angle_change_series, 0.0), index=ctx.idx)\n",
    "\n",
    "        ws = self._scale(30)\n",
    "        feats[\"turn_rate_30\"] = angle_change_series.rolling(ws, min_periods=max(ws // 3, 1)).sum()\n",
    "\n",
    "        return feats\n",
    "    \n",
    "    def _feat_cumulative(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tổng quãng đường di chuyển trong một khoảng thời gian dài xung quanh frame hiện tại.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        L = max(1, self._scale(180))\n",
    "        step = np.hypot(ctx.cx.diff(), ctx.cy.diff()).fillna(0.0)\n",
    "        path = step.rolling(2 * L + 1, min_periods=max(5, L // 6), center=True).sum()\n",
    "        feats[\"path_cum180\"] =  path.fillna(0.0).astype(\"float32\")\n",
    "        return feats\n",
    "\n",
    "    def _feat_speed_asym(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Bất đối xứng tốc độ (Tương lai - Quá khứ).\n",
    "        \"\"\"\n",
    "        w = max(3, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        v_past = v.rolling(w, min_periods=1).mean()\n",
    "        v_fut = self._roll_future_mean(v, w, min_p=1)\n",
    "        return {\"spd_asym_1s\": (v_fut - v_past).fillna(0.0)}\n",
    "    \n",
    "    def _feat_gauss_shift(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Độ lệch Gaussian (KL Divergence) giữa quá khứ và tương lai.\n",
    "        Đo lường sự thay đổi trạng thái thống kê.\n",
    "        \"\"\"\n",
    "        w = max(5, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        mu_p = v.rolling(w, min_periods=1).mean()\n",
    "        va_p = v.rolling(w, min_periods=1).var().clip(lower=1e-6)\n",
    "        mu_f = self._roll_future_mean(v, w, min_p=1)\n",
    "        va_f = self._roll_future_var(v, w, min_p=1).clip(lower=1e-6)\n",
    "\n",
    "        kl_pf = 0.5 * (\n",
    "            (va_p / va_f) + ((mu_f - mu_p) ** 2) / va_f - 1.0 + np.log(va_f / va_p)\n",
    "        )\n",
    "        kl_fp = 0.5 * (\n",
    "            (va_f / va_p) + ((mu_p - mu_f) ** 2) / va_p - 1.0 + np.log(va_p / va_f)\n",
    "        )\n",
    "        return {\n",
    "            \"spd_symkl_1s\": (kl_pf + kl_fp).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        }\n",
    "    \n",
    "    def _extract_part(self, ctx: AgentContext, part: str) -> Optional[np.ndarray]:\n",
    "        if ctx.raw_df is None: return None\n",
    "        if part not in ctx.raw_df.columns.get_level_values(0): return None\n",
    "        try:\n",
    "            sub_df = ctx.raw_df.xs(part, axis=1, level=0)[[\"x\", \"y\"]].reindex(ctx.idx)\n",
    "        except KeyError: return None\n",
    "        raw = sub_df.to_numpy()\n",
    "        raw = self._forward_fill_nan(raw)\n",
    "        cm = self._to_cm(raw.astype(np.float32))\n",
    "        return self._smooth(cm)\n",
    "    \n",
    "    def _extract_parts_dict(self, ctx: AgentContext, parts: List[str] = None) -> Dict[str, Optional[np.ndarray]]:\n",
    "        out = {}\n",
    "        for p in parts:\n",
    "            out[p] = self._extract_part(ctx, p)\n",
    "        return out\n",
    "        \n",
    "    def _feat_pose_shape(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Placeholder cho các đặc trưng hình dáng (Elongation, Body Angle...).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "\n",
    "        def zero(): return pd.Series(0.0, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        def dist(k1, k2):\n",
    "            p1, p2 = parts.get(k1), parts.get(k2)\n",
    "            if p1 is None or p2 is None: return zero()\n",
    "            d = np.linalg.norm(p1 - p2, axis=1)\n",
    "            return pd.Series(d, index=ctx.idx, dtype=\"float32\")\n",
    "        \n",
    "        def body_angle():\n",
    "            if parts.get(\"nose\") is None: return zero()\n",
    "            if parts.get(\"neck\") is None: return zero()\n",
    "            if parts.get(\"tail_base\") is None: return zero()\n",
    "\n",
    "            v1 = parts.get(\"nose\") - parts.get(\"neck\")\n",
    "            v2 = parts.get(\"tail_base\") - parts.get(\"neck\")\n",
    "            dot_product = np.sum(v1 * v2, axis=1)\n",
    "            mag = np.linalg.norm(v1, axis=1) * np.linalg.norm(v2, axis=1)\n",
    "            cos_angle = np.clip(dot_product / (mag + 1e-6), -1.0, 1.0).astype(\"float32\")\n",
    "            return cos_angle\n",
    "        \n",
    "        def elongation():\n",
    "            if parts.get(\"nose\")          is None: return zero()\n",
    "            if parts.get(\"tail_base\")     is None: return zero()\n",
    "            if parts.get(\"hip_left\")  is None: return zero()\n",
    "            if parts.get(\"hip_right\") is None: return zero()\n",
    "\n",
    "            d1 = dist(\"nose\", \"tail_base\")\n",
    "            d2 = dist(\"hip_left\", \"hip_right\")\n",
    "            elongation = d1 / (d2 + 1e-6).astype(\"float32\")\n",
    "            return elongation\n",
    "        \n",
    "        def part_speed(part: str, n_frames_30fps: int) -> Dict:\n",
    "            part_pos = self._extract_part(ctx, part)\n",
    "            if part_pos is None: return zero()\n",
    "            \n",
    "            s_x = pd.Series(part_pos[:, 0], index=ctx.idx)\n",
    "            s_y = pd.Series(part_pos[:, 1], index=ctx.idx)\n",
    "            raw_speed = self._speed_series(s_x, s_y)\n",
    "\n",
    "            ws = self._scale(n_frames_30fps)\n",
    "            val = raw_speed.rolling(ws, min_periods=1, center=True).mean()\n",
    "            return val.astype(\"float32\")\n",
    "\n",
    "\n",
    "        target_parts = [\"nose\", \"hip_left\", \"hip_right\", \"ear_left\", \"ear_right\", \"tail_base\", \"neck\"]\n",
    "        \n",
    "        parts = self._extract_parts_dict(ctx, target_parts)\n",
    "\n",
    "        feats[\"aa_nose_tailbase_dist\"]       = dist(\"nose\", \"tail_base\")\n",
    "        feats[\"aa_earleft_tailbase_dist\"]    = dist(\"ear_left\", \"tail_base\")\n",
    "        feats[\"aa_earright_tailbase_dist\"]   = dist(\"ear_right\", \"tail_base\")\n",
    "        feats[\"aa_nose_earleft_dist\"]        = dist(\"ear_left\", \"nose\")\n",
    "        feats[\"aa_nose_ear_right_dist\"]      = dist(\"ear_right\", \"nose\")\n",
    "        feats[\"aa_nose_hip_left_dist\"]       = dist(\"nose\", \"hip_left\")\n",
    "        feats[\"aa_nose_hip_right_dist\"]      = dist(\"nose\", \"hip_right\")\n",
    "        feats[\"aa_neck_tailbase_dist\"] = dist(\"neck\", \"tail_base\")\n",
    "        \n",
    "        # feats[\"a_elongation\"]                = elongation()\n",
    "        feats[\"a_bodyangle\"]                 = body_angle()\n",
    "\n",
    "        a_tail_base_vel_500ms     = part_speed(\"tail_base\", 15)\n",
    "        a_tail_base_vel_1000ms    = part_speed(\"tail_base\", 30)\n",
    "        a_tail_base_vel_2000ms    = part_speed(\"tail_base\", 60)\n",
    "        a_tail_base_vel_3000ms    = part_speed(\"tail_base\", 90)\n",
    "\n",
    "\n",
    "        a_hip_left_vel_500ms          = part_speed(\"hip_left\", 15)\n",
    "        a_hip_left_vel_1000ms         = part_speed(\"hip_left\", 30)\n",
    "        a_hip_left_vel_2000ms         = part_speed(\"hip_left\", 60)\n",
    "        a_hip_left_vel_3000ms         = part_speed(\"hip_left\", 90)\n",
    "\n",
    "        a_hip_right_vel_500ms          = part_speed(\"hip_left\", 15)\n",
    "        a_hip_right_vel_1000ms         = part_speed(\"hip_left\", 30)\n",
    "        a_hip_right_vel_2000ms         = part_speed(\"hip_left\", 60)\n",
    "        a_hip_right_vel_3000ms         = part_speed(\"hip_left\", 90)\n",
    "\n",
    "        feats[\"a_upper_vel_500ms\"]            = (a_tail_base_vel_500ms + a_hip_left_vel_500ms + a_hip_right_vel_500ms)/3.0\n",
    "        feats[\"a_upper_vel_1000ms\"]           = (a_tail_base_vel_1000ms + a_hip_left_vel_1000ms + a_hip_right_vel_1000ms)/3.0\n",
    "        feats[\"a_upper_vel_2000ms\"]           = (a_tail_base_vel_2000ms + a_hip_left_vel_2000ms + a_hip_right_vel_2000ms)/3.0\n",
    "        feats[\"a_upper_vel_3000ms\"]           = (a_tail_base_vel_3000ms + a_hip_left_vel_3000ms + a_hip_right_vel_3000ms)/3.0\n",
    "\n",
    "\n",
    "        feats[\"a_nose_vel_500ms\"]            = part_speed(\"nose\", 15)\n",
    "        feats[\"a_nose_vel_1000ms\"]           = part_speed(\"nose\", 30)\n",
    "        feats[\"a_nose_vel_2000ms\"]           = part_speed(\"nose\", 60)\n",
    "        feats[\"a_nose_vel_3000ms\"]           = part_speed(\"nose\", 90)\n",
    "\n",
    "        # feats[\"a_ear_right_vel_500ms\"]       = part_speed(\"hip_right\", 15)\n",
    "        # feats[\"a_ear_right_vel_1000ms\"]      = part_speed(\"hip_right\", 30)\n",
    "        # feats[\"a_ear_right_vel_2000ms\"]      = part_speed(\"hip_right\", 60)\n",
    "        # feats[\"a_ear_right_vel_3000ms\"]      = part_speed(\"hip_right\", 90)\n",
    "        # feats[\"a_ear_left_vel_500ms\"]        = part_speed(\"ear_left\", 15)\n",
    "        # feats[\"a_ear_left_vel_1000ms\"]       = part_speed(\"ear_left\", 30)\n",
    "        # feats[\"a_ear_left_vel_2000ms\"]       = part_speed(\"ear_left\", 60)\n",
    "        # feats[\"a_ear_left_vel_3000ms\"]       = part_speed(\"ear_left\", 90)\n",
    "        \n",
    "        return feats\n",
    "\n",
    "    def _feat_attack_sniff(\n",
    "        self,\n",
    "        ctx: AgentContext,\n",
    "        target_ctx: AgentContext = None,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng phân biệt attack vs sniff cho lab 2-mouse (agent=1, target=2).\n",
    "    \n",
    "        Ý tưởng:\n",
    "          - attack: speed 2 con biến động mạnh, đổi hướng nhiều, body overlap cao.\n",
    "          - sniff : mũi gần cổ/thân, overlap thấp hơn, motion nhẹ/ổn định hơn.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "    \n",
    "        def zero():\n",
    "            return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "\n",
    "        # helper khoảng cách\n",
    "        def dist(p1, p2):\n",
    "            if p1 is None or p2 is None:\n",
    "                return zero()\n",
    "            d = np.linalg.norm(p1 - p2, axis=1)\n",
    "            return pd.Series(d, index=idx, dtype=\"float32\")\n",
    "\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\"])\n",
    "        parts_t = self._extract_parts_dict(target_ctx, [\"nose\", \"tail_base\"])\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 2) ĐIỂM ĐẠI DIỆN THÂN (BODY CENTER) CHO MỖI CON\n",
    "        #    dùng trung bình neck – hips – tail_base\n",
    "        # ---------------------------------------------------------\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 4) MỨC ĐỘ “BẠO LỰC”: DAO ĐỘNG TỐC ĐỘ & ĐỔI HƯỚNG\n",
    "        # ---------------------------------------------------------\n",
    "        # speed 2 con từ velocity\n",
    "        a_speed = pd.Series(\n",
    "            np.linalg.norm(ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "\n",
    "        ws_05 = self._scale(15)  # ~0.5s\n",
    "        mp_05 = max(ws_05 // 3, 1)\n",
    "    \n",
    "        feats[\"as_a_speed_std_05\"] = (\n",
    "            a_speed.rolling(ws_05, min_periods=mp_05).std().fillna(0.0).astype(\"float32\")\n",
    "        )\n",
    "        feats[\"as_t_speed_std_05\"] = (\n",
    "            t_speed.rolling(ws_05, min_periods=mp_05).std().fillna(0.0).astype(\"float32\")\n",
    "        )\n",
    "        feats[\"as_speed_std_sum_05\"] = (\n",
    "            feats[\"as_a_speed_std_05\"] + feats[\"as_t_speed_std_05\"]\n",
    "        )\n",
    "    \n",
    "        # Đổi hướng (jerk góc) của agent\n",
    "        a_angle = np.arctan2(ctx.vel[:, 1], ctx.vel[:, 0])\n",
    "        a_angle_diff = np.abs(np.diff(a_angle))\n",
    "        a_angle_diff = np.where(\n",
    "            a_angle_diff > np.pi, 2 * np.pi - a_angle_diff, a_angle_diff\n",
    "        )\n",
    "        a_angle_diff = np.concatenate([[0.0], a_angle_diff])\n",
    "        a_angle_diff_s = pd.Series(a_angle_diff, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        feats[\"as_a_turn_jerk_05\"] = (\n",
    "            a_angle_diff_s.rolling(ws_05, min_periods=mp_05)\n",
    "            .sum()\n",
    "            .fillna(0.0)\n",
    "            .astype(\"float32\")\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 5) XẤP XỈ OVERLAP CƠ THỂ (BODY OVERLAP)\n",
    "        #    dùng bbox từ các bộ phận thân\n",
    "        # ---------------------------------------------------------\n",
    "        def build_bbox(parts: Dict[str, Optional[np.ndarray]]):\n",
    "            arrs = []\n",
    "            for k in [\"nose\", \"hip_left\", \"hip_right\", \"ear_left\", \"ear_right\", \"tail_base\"]:\n",
    "                if parts.get(k) is not None:\n",
    "                    arrs.append(parts[k])\n",
    "            if not arrs:\n",
    "                return None\n",
    "            stack = np.stack(arrs, axis=1)  # [F, K, 2]\n",
    "            xs = stack[:, :, 0]\n",
    "            ys = stack[:, :, 1]\n",
    "            xmin = np.nanmin(xs, axis=1)\n",
    "            xmax = np.nanmax(xs, axis=1)\n",
    "            ymin = np.nanmin(ys, axis=1)\n",
    "            ymax = np.nanmax(ys, axis=1)\n",
    "            return np.stack([xmin, ymin, xmax, ymax], axis=1).astype(\"float32\")\n",
    "    \n",
    "        def iou_box(box1: np.ndarray, box2: np.ndarray):\n",
    "            # box: [F, 4] = (xmin, ymin, xmax, ymax)\n",
    "            x1 = np.maximum(box1[:, 0], box2[:, 0])\n",
    "            y1 = np.maximum(box1[:, 1], box2[:, 1])\n",
    "            x2 = np.minimum(box1[:, 2], box2[:, 2])\n",
    "            y2 = np.minimum(box1[:, 3], box2[:, 3])\n",
    "    \n",
    "            inter_w = np.clip(x2 - x1, 0.0, None)\n",
    "            inter_h = np.clip(y2 - y1, 0.0, None)\n",
    "            inter = inter_w * inter_h\n",
    "    \n",
    "            area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "            area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "            union = area1 + area2 - inter + 1e-6\n",
    "            iou = inter / union\n",
    "            return iou.astype(\"float32\")\n",
    "\n",
    "        bbox_a = build_bbox(parts_a)\n",
    "        bbox_t = build_bbox(parts_t)\n",
    "        if bbox_a is not None and bbox_t is not None:\n",
    "            iou = iou_box(bbox_a, bbox_t)\n",
    "            iou_s = pd.Series(iou, index=idx, dtype=\"float32\")\n",
    "    \n",
    "            feats[\"as_body_iou\"] = iou_s\n",
    "    \n",
    "            ws_1s = self._scale(30)\n",
    "            mp_1s = max(ws_1s // 3, 1)\n",
    "            feats[\"as_body_iou_mean_1s\"] = (\n",
    "                iou_s.rolling(ws_1s, min_periods=mp_1s).mean().fillna(0.0).astype(\"float32\")\n",
    "            )\n",
    "        else:\n",
    "            feats[\"as_body_iou\"] = zero()\n",
    "            feats[\"as_body_iou_mean_1s\"] = zero()\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 6) DỌN NẠN NaN / Inf\n",
    "        # ---------------------------------------------------------\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "\n",
    "    def _feat_climb(self, ctx: AgentContext, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Feature chuyên cho hành vi climb trong arena hình chữ nhật (33 x 19 cm).\n",
    "    \n",
    "        Ý tưởng:\n",
    "          - Chuột đi gần tường: dist_wall giảm nhanh.\n",
    "          - Khi climb: sát tường (dist_wall nhỏ), v_normal ~ 0,\n",
    "            nhưng vẫn có v_tangent (bò ngang trên tường / di chuyển dọc biên).\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        idx = ctx.idx\n",
    "    \n",
    "        def zero() -> pd.Series:\n",
    "            return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 1. Arena size (cm) ---\n",
    "        # Nếu bạn đã set trong FeatureConfig thì dùng:\n",
    "        # W = self.cfg.arena_width_cm or 33.0\n",
    "        # H = self.cfg.arena_height_cm or 19.0\n",
    "        # Ở đây fix luôn cho lab này:\n",
    "        W = 28.0\n",
    "        H = 18.0\n",
    "        parts = self._extract_parts_dict(ctx, [\"nose\"])\n",
    "        head = parts.get(\"nose\")\n",
    "        \n",
    "        if head is not None:\n",
    "            # head đã ở đơn vị cm (vì _extract_part đã to_cm + smooth)\n",
    "            cx = pd.Series(head[:, 0], index=idx)\n",
    "            cy = pd.Series(head[:, 1], index=idx)\n",
    "        else:\n",
    "            # fallback: nếu không có head thì dùng body_center như cũ\n",
    "            cx = ctx.cx\n",
    "            cy = ctx.cy\n",
    "\n",
    "\n",
    "        # # --- 2. Khoảng cách tới 4 bức tường ---\n",
    "        # cx = ctx.cx  # Series\n",
    "        # cy = ctx.cy  # Series\n",
    "    \n",
    "        dist_left   = cx - 0.0\n",
    "        dist_right  = W - cx\n",
    "        dist_bottom = cy - 0.0\n",
    "        dist_top    = H - cy\n",
    "    \n",
    "        d_all = np.stack(\n",
    "            [dist_left.values, dist_right.values, dist_bottom.values, dist_top.values],\n",
    "            axis=1,  # [F, 4]\n",
    "        )\n",
    "    \n",
    "        dist_wall = np.min(d_all, axis=1)          # khoảng cách tới tường gần nhất\n",
    "        wall_idx  = np.argmin(d_all, axis=1)       # 0:left, 1:right, 2:bottom, 3:top\n",
    "    \n",
    "        dist_wall_s = pd.Series(dist_wall, index=idx, dtype=\"float32\")\n",
    "        feats[\"climb_dist_wall\"] = dist_wall_s\n",
    "    \n",
    "        # --- 3. Vận tốc theo NORMAL & TANGENT của tường gần nhất ---\n",
    "        vx = ctx.vel[:, 0]\n",
    "        vy = ctx.vel[:, 1]\n",
    "    \n",
    "        # normal hướng VÀO trong arena từ tường\n",
    "        nx = np.zeros_like(vx, dtype=\"float32\")\n",
    "        ny = np.zeros_like(vy, dtype=\"float32\")\n",
    "\n",
    "        # left  wall (x=0)    → normal = (+1, 0)\n",
    "        # right wall (x=W)    → normal = (-1, 0)\n",
    "        # bottom wall (y=0)   → normal = (0, +1)\n",
    "        # top wall (y=H)      → normal = (0, -1)\n",
    "        nx[wall_idx == 0] =  1.0\n",
    "        nx[wall_idx == 1] = -1.0\n",
    "        ny[wall_idx == 2] =  1.0\n",
    "        ny[wall_idx == 3] = -1.0\n",
    "    \n",
    "        # v_normal = v ⋅ n\n",
    "        v_normal = vx * nx + vy * ny\n",
    "    \n",
    "        # thành phần song song tường: v_tan = v - (v⋅n)n\n",
    "        v_proj_x = v_normal * nx\n",
    "        v_proj_y = v_normal * ny\n",
    "        v_tan_x = vx - v_proj_x\n",
    "        v_tan_y = vy - v_proj_y\n",
    "        v_tangent = np.sqrt(v_tan_x ** 2 + v_tan_y ** 2)\n",
    "    \n",
    "        v_normal_s  = pd.Series(v_normal,  index=idx, dtype=\"float32\")\n",
    "        v_tangent_s = pd.Series(v_tangent, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        feats[\"climb_normal_vel\"]  = v_normal_s\n",
    "        feats[\"climb_tangent_vel\"] = v_tangent_s\n",
    "    \n",
    "        # --- 4. Approach speed: dist_wall giảm mạnh (lao vào tường) ---\n",
    "        ws = self._scale(15)  # ~0.5s (15 frame ở 30fps)\n",
    "        min_p = max(ws // 3, 1)\n",
    "\n",
    "        # diff_dw > 0 khi dist_wall giảm (đi về phía tường)\n",
    "        diff_dw = -dist_wall_s.diff().fillna(0.0)  # dấu trừ để \"giảm\" → dương\n",
    "        approach = diff_dw.rolling(ws, min_periods=min_p).mean()\n",
    "        feats[\"climb_approach_speed_wall\"] = approach.astype(\"float32\")\n",
    "    \n",
    "        # --- 5. Stick score: sát tường + không còn lao vào (v_normal nhỏ) ---\n",
    "        # gần tường\n",
    "        thr_cm = 3.0  # tuỳ chỉnh (3cm sát tường)\n",
    "        near_wall = (dist_wall_s < thr_cm).astype(\"float32\")\n",
    "    \n",
    "        # ít lao vào nữa: |v_normal| nhỏ\n",
    "        stick = near_wall * (1.0 / (1.0 + v_normal_s.abs()))\n",
    "\n",
    "        # Nếu muốn climb thực sự có chút chuyển động dọc tường:\n",
    "        # yêu cầu v_tangent > một ngưỡng nhỏ (ví dụ 0.5 cm/s)\n",
    "        stick = stick * (v_tangent_s > 0.5).astype(\"float32\")\n",
    "    \n",
    "        feats[\"climb_wall_stick_score\"] = stick.astype(\"float32\")\n",
    "    \n",
    "        # --- 6. Clean NaN/Inf ---\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "\n",
    "\n",
    "    def _feat_pairwise(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng tương tác cặp đôi (Pairwise): Khoảng cách, Tốc độ tiếp cận.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        if target_ctx is None: \n",
    "            return feats\n",
    "\n",
    "        def zero(): return pd.Series(0.0, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        def dist_ab(pt_a, pt_b):\n",
    "            if pt_a is None or pt_b is None: return zero()\n",
    "            d = np.linalg.norm(pt_a - pt_b, axis=1)\n",
    "            return pd.Series(d, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        feats[\"rel_dist\"] = pd.Series(dist, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        # Khoảng cách\n",
    "        my_parts = self._extract_parts_dict(ctx, [\"nose\", \"ear_left\", \"ear_right\", \"body_center\", \"tail_base\", \"hip_left\", \"hip_right\", \"neck\"])\n",
    "        target_parts = self._extract_parts_dict(target_ctx, [\"nose\", \"ear_left\", \"ear_right\", \"body_center\", \"tail_base\", \"hip_left\", \"hip_right\", \"neck\"])\n",
    "\n",
    "        an, tn = my_parts[\"nose\"], target_parts[\"nose\"]\n",
    "        feats[\"dist_nose_nose\"] = dist_ab(an, tn)\n",
    "        feats[\"dist_nose_tail\"] = dist_ab(an, target_parts[\"tail_base\"])\n",
    "        feats[\"dist_nose_el\"]   = dist_ab(an, target_parts[\"ear_left\"])\n",
    "        feats[\"dist_nose_er\"]   = dist_ab(an, target_parts[\"ear_right\"])\n",
    "        feats[\"dist_nose_hip_l\"] = dist_ab(an, target_parts[\"hip_left\"])\n",
    "        feats[\"dist_nose_hip_r\"] = dist_ab(an, target_parts[\"hip_right\"])\n",
    "        feats[\"dist_nose_neck\"] = dist_ab(an, target_parts[\"neck\"])\n",
    "\n",
    "        \n",
    "        #  Hướng - góc nhìn\n",
    "        def get_body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"nose\")\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if head is not None and tail is not None:\n",
    "                return head - tail\n",
    "            return None\n",
    "\n",
    "        a_vec = get_body_vec(my_parts)\n",
    "        t_vec = get_body_vec(target_parts)\n",
    "\n",
    "        if a_vec is not None and t_vec is not None:\n",
    "            dot = np.sum(a_vec * t_vec, axis=1)\n",
    "            mags = np.linalg.norm(a_vec, axis=1) * np.linalg.norm(t_vec, axis=1)\n",
    "            feats[\"body_cosine\"] = pd.Series(\n",
    "                np.clip(dot / (mags + 1e-6), -1.0, 1.0), index=ctx.idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"body_cosine\"] = zero()\n",
    "\n",
    "        # Vector ánh nhìn = Target_Pos - My_Pos = rel_vec\n",
    "        if a_vec is not None:\n",
    "            dot_gaze = np.sum(a_vec * rel_vec, axis=1)\n",
    "            mag_a = np.linalg.norm(a_vec, axis=1)\n",
    "            feats[\"gaze_cosine\"] = pd.Series(\n",
    "                np.clip(dot_gaze / (mag_a * dist + 1e-6), -1.0, 1.0),\n",
    "                index=ctx.idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"gaze_cosine\"] = zero()\n",
    "\n",
    "        # Vector đơn vị hướng về địch (u)\n",
    "        dist_safe = dist.copy()\n",
    "        dist_safe[dist_safe == 0] = 1e-6\n",
    "        u_vec = rel_vec / dist_safe[:, None]\n",
    "\n",
    "        # a_vel và t_vel lấy từ Context\n",
    "        a_vel, t_vel = ctx.vel, target_ctx.vel\n",
    "\n",
    "        # A. Approach Speed (Vận tốc dọc trục nối 2 con)\n",
    "        # Dương: Lao vào nhau | Âm: Chạy ra xa nhau\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1) # Target hướng ngược lại\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)\n",
    "\n",
    "        # B. Lateral Speed (Vận tốc ngang - Vuông góc trục nối)\n",
    "        # Vector chiếu: v_proj = (v . u) * u\n",
    "        a_proj = a_along[:, None] * u_vec\n",
    "        a_lat_vec = a_vel - a_proj\n",
    "        a_lat_speed = np.linalg.norm(a_lat_vec, axis=1)\n",
    "\n",
    "        feats[\"approach_speed_agent\"]  = pd.Series(a_along, index=ctx.idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_target\"] = pd.Series(t_along, index=ctx.idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_rel\"]    = pd.Series(rel_along, index=ctx.idx, dtype=\"float32\")\n",
    "        feats[\"lateral_speed_agent\"]   = pd.Series(a_lat_speed, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        return feats\n",
    "\n",
    "\n",
    "    def _feat_ejaculate_temporal(\n",
    "        self,\n",
    "        ctx: AgentContext,\n",
    "        target_ctx: AgentContext = None,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng cho hành vi 'ejaculate' (pair):\n",
    "          - 2 con dính sát, agent gần vùng đuôi/genital của target.\n",
    "          - Trước đó có giai đoạn hoạt động mạnh (mount/intromit/thrust).\n",
    "          - Thời điểm ejaculate: agent gần như đứng yên nhưng vẫn sát target.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "    \n",
    "        def zero() -> pd.Series:\n",
    "            return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 1. PARTS: APPROX GENITAL & BODY\n",
    "        # -------------------------------------------------\n",
    "        # Agent: dùng body_center + nose\n",
    "        parts_a = self._extract_parts_dict(\n",
    "            ctx,\n",
    "            [\"nose\", \"body_center\", \"tail_base\", \"hip_left\", \"hip_right\"]\n",
    "        )\n",
    "        # Target: genital ~ tail_base, thân ~ body_center\n",
    "        parts_t = self._extract_parts_dict(\n",
    "            target_ctx,\n",
    "            [\"body_center\", \"tail_base\"]\n",
    "        )\n",
    "    \n",
    "        a_nose = parts_a.get(\"nose\")\n",
    "        a_bc   = parts_a.get(\"body_center\")\n",
    "        a_tail = parts_a.get(\"tail_base\")\n",
    "        t_bc   = parts_t.get(\"body_center\")\n",
    "        t_tail = parts_t.get(\"tail_base\")\n",
    "\n",
    "        # fallback body_center nếu thiếu\n",
    "        if a_bc is None and a_tail is not None:\n",
    "            a_bc = a_tail\n",
    "        if t_bc is None and t_tail is not None:\n",
    "            t_bc = t_tail\n",
    "    \n",
    "        def dist_series(p1: Optional[np.ndarray],\n",
    "                        p2: Optional[np.ndarray]) -> pd.Series:\n",
    "            if p1 is None or p2 is None:\n",
    "                return zero()\n",
    "            d = np.linalg.norm(p1 - p2, axis=1).astype(\"float32\")\n",
    "            return pd.Series(d, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # khoảng cách thân–thân và agent body → target genital\n",
    "        dist_body = dist_series(a_bc, t_bc)          # \"ôm\" nhau chặt hay không\n",
    "        dist_gen  = dist_series(a_bc, t_tail)       # agent body gần đuôi target\n",
    "        dist_nose_gen = dist_series(a_nose, t_tail) # mũi agent gần genital\n",
    "    \n",
    "        feats[\"ejac_dist_body\"]      = dist_body\n",
    "        feats[\"ejac_dist_gen_body\"]  = dist_gen\n",
    "        feats[\"ejac_dist_gen_nose\"]  = dist_nose_gen\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 2. PROXIMITY SCORE (khoảng cách nhỏ → score lớn)\n",
    "        # -------------------------------------------------\n",
    "        # scale ~ 5 cm, có thể chỉnh nếu arena nhỏ/lớn\n",
    "        prox_body = np.exp(-dist_body.to_numpy() / 5.0).astype(\"float32\")\n",
    "        prox_gen  = 1.0 / (1.0 + dist_gen.to_numpy())\n",
    "        prox_nose = 1.0 / (1.0 + dist_nose_gen.to_numpy())\n",
    "    \n",
    "        feats[\"ejac_prox_body\"] = pd.Series(prox_body, index=idx, dtype=\"float32\")\n",
    "        feats[\"ejac_prox_gen\"]  = pd.Series(prox_gen,  index=idx, dtype=\"float32\")\n",
    "        feats[\"ejac_prox_nose_gen\"] = pd.Series(prox_nose, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 3. BUILD-UP MEMORY: HOẠT ĐỘNG MẠNH TRƯỚC ĐÓ\n",
    "        # -------------------------------------------------\n",
    "        # dung speed của agent nhưng chỉ tính khi đang dính sát body\n",
    "        v = ctx.speed_series  # cm/s\n",
    "        close_mask = (dist_body < 5.0).astype(\"float32\")  # ở rất gần\n",
    "        v_contact = (v * close_mask).astype(\"float32\")\n",
    "\n",
    "        ws_mem = self._scale(90)  # ~3s\n",
    "        ws_mem = max(ws_mem, 1)\n",
    "    \n",
    "        ejac_mem = (\n",
    "            v_contact.rolling(ws_mem, min_periods=1)\n",
    "                     .max()\n",
    "                     .fillna(0.0)\n",
    "                     .astype(\"float32\")\n",
    "        )\n",
    "        feats[\"ejac_activity_memory_3s\"] = ejac_mem\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 4. HIỆN TẠI: ĐỨNG YÊN NHƯNG VẪN DÍNH SÁT\n",
    "        # -------------------------------------------------\n",
    "        # agent gần như đứng yên\n",
    "        is_still = (v < 1.5).astype(\"float32\")  # ngưỡng speed thấp, tuỳ lab\n",
    "        feats[\"ejac_is_still\"] = is_still\n",
    "    \n",
    "        # khoảng cách ổn định (không kéo xa/đẩy gần quá nhanh)\n",
    "        dist_body_diff = dist_body.diff().abs().fillna(0.0)\n",
    "        feats[\"ejac_dist_body_diff\"] = dist_body_diff.astype(\"float32\")\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 5. FINAL SCORE (gợi ý): cao khi ejaculate\n",
    "        # -------------------------------------------------\n",
    "        # điều kiện:\n",
    "        #  - trước đó hoạt động mạnh (ejac_mem lớn)\n",
    "        #  - bây giờ đứng yên (is_still ~1)\n",
    "        #  - vẫn dính sát, gần vùng genital\n",
    "        prox_comb = (\n",
    "            feats[\"ejac_prox_body\"] *\n",
    "            feats[\"ejac_prox_gen\"]  *\n",
    "            feats[\"ejac_prox_nose_gen\"]\n",
    "        )\n",
    "    \n",
    "        feats[\"ejac_static_score\"] = (\n",
    "            is_still * prox_comb * ejac_mem\n",
    "        ).astype(\"float32\")\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 6. CLEAN NaN / Inf\n",
    "        # -------------------------------------------------\n",
    "        for k, s in feats.items():\n",
    "            feats[k] = (\n",
    "                s.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "\n",
    "\n",
    "    def _feat_follow_pattern(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng hành vi FOLLOW:\n",
    "          - Agent ở gần target\n",
    "          - Cùng hướng (body + velocity)\n",
    "          - Tốc độ vừa phải\n",
    "          - Khoảng cách tương đối ổn định trong 0.5–1s\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 1. CÁC ĐẠI LƯỢNG CƠ BẢN ---\n",
    "        # Vector Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # Speed agent/target\n",
    "        a_speed = ctx.speed_series.astype(\"float32\")\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "    \n",
    "        # Body vector: nose - tail/body_center\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"nose\", \"ear_left\", \"ear_right\", \"body_center\", \"tail_base\", \"hip_left\", \"hip_right\", \"neck\"])\n",
    "        parts_t = self._extract_parts_dict(target_ctx, [\"nose\", \"ear_left\", \"ear_right\", \"body_center\", \"tail_base\", \"hip_left\", \"hip_right\", \"neck\"])\n",
    "    \n",
    "        def body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"nose\")\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if head is None or tail is None:\n",
    "                return None\n",
    "            return head - tail\n",
    "    \n",
    "        a_body = body_vec(parts_a)\n",
    "        t_body = body_vec(parts_t)\n",
    "    \n",
    "        if a_body is not None and t_body is not None:\n",
    "            dot_bt = np.sum(a_body * t_body, axis=1)\n",
    "            mag_bt = np.linalg.norm(a_body, axis=1) * np.linalg.norm(t_body, axis=1)\n",
    "            cos_body = np.clip(dot_bt / (mag_bt + 1e-6), -1.0, 1.0)\n",
    "            cos_body_s = pd.Series(cos_body, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            cos_body_s = zero()\n",
    "    \n",
    "        # Velocity hướng\n",
    "        a_vel = ctx.vel\n",
    "        t_vel = target_ctx.vel\n",
    "        a_speed_np = np.linalg.norm(a_vel, axis=1)\n",
    "        t_speed_np = np.linalg.norm(t_vel, axis=1)\n",
    "        moving_mask = (a_speed_np > 1e-3) & (t_speed_np > 1e-3)\n",
    "    \n",
    "        # cos giữa hướng velocity 2 con\n",
    "        dot_v = np.sum(a_vel * t_vel, axis=1)\n",
    "        mag_v = a_speed_np * t_speed_np + 1e-6\n",
    "        cos_vel = np.zeros_like(dot_v, dtype=\"float32\")\n",
    "        cos_vel[moving_mask] = np.clip(dot_v[moving_mask] / mag_v[moving_mask], -1.0, 1.0)\n",
    "        cos_vel_s = pd.Series(cos_vel, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 2. WINDOW NGẮN (FOLLOW LÀ PATTERN DÀI HƠN ATTACK) ---\n",
    "        for w30 in [15, 30, 60]:   # ~0.5s, 1s, 2s\n",
    "            ws = self._scale(w30)\n",
    "            min_p = max(ws // 3, 1)\n",
    "    \n",
    "            # Khoảng cách trung bình & độ dao động\n",
    "            m_dist = rel_dist_s.rolling(ws, min_periods=min_p).mean()\n",
    "            s_dist = rel_dist_s.rolling(ws, min_periods=min_p).std()\n",
    "    \n",
    "            # Cùng hướng (body + velocity)\n",
    "            m_cos_body = cos_body_s.rolling(ws, min_periods=min_p).mean()\n",
    "            m_cos_vel  = cos_vel_s.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            # Tốc độ vừa phải\n",
    "            m_sp_a = a_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            m_sp_t = t_speed.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            feats[f\"follow_dist_mean_{w30}\"] = m_dist\n",
    "            feats[f\"follow_dist_std_{w30}\"]  = s_dist\n",
    "            feats[f\"follow_cos_body_mean_{w30}\"] = m_cos_body\n",
    "            feats[f\"follow_cos_vel_mean_{w30}\"]  = m_cos_vel\n",
    "            feats[f\"follow_speed_agent_mean_{w30}\"] = m_sp_a\n",
    "            feats[f\"follow_speed_target_mean_{w30}\"] = m_sp_t\n",
    "    \n",
    "        # Clean\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "    \n",
    "    def _feat_shortburst_social(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        feats = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- Lấy lại vài quantity cơ bản từ pairwise/avoidance ---\n",
    "        # vector Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # unit vector\n",
    "        rel_dist_safe = np.where(rel_dist == 0, 1e-6, rel_dist)\n",
    "        u_vec = rel_vec / rel_dist_safe[:, None]\n",
    "    \n",
    "        # velocity dọc trục nối (approach speed)\n",
    "        a_vel = ctx.vel\n",
    "        t_vel = target_ctx.vel\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)                # +: lao vào target\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1)             # +: target lao vào agent\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)    # +: lại gần nhau\n",
    "    \n",
    "        a_along_s = pd.Series(a_along, index=idx, dtype=\"float32\")\n",
    "        t_along_s = pd.Series(t_along, index=idx, dtype=\"float32\")\n",
    "        rel_along_s = pd.Series(rel_along, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # speed agent / target\n",
    "        a_speed = ctx.speed_series\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\"\n",
    "        )\n",
    "    \n",
    "        # heading_rel_cos ~ escape / approach\n",
    "        # vector body của agent\n",
    "        # (reuse idea từ _feat_pairwise)\n",
    "        # head ~ nose, tail ~ tail_base/body_center\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\"])\n",
    "        head_a = parts_a.get(\"nose\")\n",
    "        tail_a = parts_a.get(\"tail_base\")\n",
    "    \n",
    "        if head_a is not None and tail_a is not None:\n",
    "            body_vec_a = head_a - tail_a\n",
    "            dot = np.sum(body_vec_a * rel_vec, axis=1)\n",
    "            mag = np.linalg.norm(body_vec_a, axis=1) * rel_dist_safe\n",
    "            heading_cos = np.clip(dot / (mag + 1e-6), -1.0, 1.0)\n",
    "            heading_cos_s = pd.Series(heading_cos, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            heading_cos_s = zero()\n",
    "    \n",
    "        # --- Rolling window 10, 20, 30 frames (ở fps gốc) ---\n",
    "        for w30 in [10, 20, 30]:\n",
    "            ws = self._scale(w30)\n",
    "            min_p = max(1, ws // 3)\n",
    "    \n",
    "            # Attack-like: approach mạnh, khoảng cách giảm nhanh\n",
    "            feats[f\"sb_att_approach_mean_{w30}\"] = a_along_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_att_rel_along_mean_{w30}\"] = rel_along_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_att_dist_delta_{w30}\"] = (rel_dist_s - rel_dist_s.shift(ws)).fillna(0.0)\n",
    "    \n",
    "            # Chase-like: agent & target đều nhanh, dist tương đối nhỏ\n",
    "            feats[f\"sb_chase_speed_agent_mean_{w30}\"] = a_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_chase_speed_target_mean_{w30}\"] = t_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_chase_dist_mean_{w30}\"] = rel_dist_s.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            # Escape-like: heading ngược, dist tăng nhanh\n",
    "            feats[f\"sb_esc_heading_cos_mean_{w30}\"] = heading_cos_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_esc_dist_gain_{w30}\"] = (rel_dist_s.shift(-ws) - rel_dist_s).fillna(0.0)\n",
    "    \n",
    "        # clip & fillna\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = v.replace([np.inf, -np.inf], np.nan).fillna(0.0).astype(\"float32\")\n",
    "    \n",
    "        return feats\n",
    "\n",
    "    \n",
    "    def build_pose_tensor(self, tracking: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Chuyển dữ liệu tracking (DataFrame) sang Tensor [Frames, Mice, 2] và Dict chi tiết.\n",
    "        \"\"\"\n",
    "        tracking = tracking.sort_values(\"video_frame\")\n",
    "        frames = np.sort(tracking[\"video_frame\"].unique())\n",
    "        \n",
    "        pvid = tracking.pivot(\n",
    "            index=\"video_frame\", \n",
    "            columns=[\"mouse_id\", \"bodypart\"], \n",
    "            values=[\"x\", \"y\"]\n",
    "        )\n",
    "        pvid = pvid.reorder_levels([1, 2, 0], axis=1).sort_index(axis=1).astype(\"float32\")\n",
    "        mouse_ids = list(pvid.columns.get_level_values(0).unique())\n",
    "        pos = np.full((len(frames), len(mouse_ids), 2), np.nan, dtype=np.float32)\n",
    "        per_mouse_df = {}\n",
    "        \n",
    "        for i, mid in enumerate(mouse_ids):\n",
    "            single = pvid[mid]\n",
    "            per_mouse_df[mid] = single\n",
    "            \n",
    "            if \"body_center\" in single.columns.get_level_values(0):\n",
    "                cx = single[\"body_center\"][\"x\"]\n",
    "                cy = single[\"body_center\"][\"y\"]\n",
    "            else:\n",
    "                cx = single.xs(\"x\", level=1, axis=1).mean(axis=1)\n",
    "                cy = single.xs(\"y\", level=1, axis=1).mean(axis=1)\n",
    "            \n",
    "            pos[:, i, 0] = cx.reindex(frames).values\n",
    "            pos[:, i, 1] = cy.reindex(frames).values\n",
    "            \n",
    "        return frames, mouse_ids, pos, per_mouse_df\n",
    "\n",
    "    def extract_agent_target(\n",
    "        self, \n",
    "        frames: np.ndarray, \n",
    "        mouse_ids: List[Any], \n",
    "        pos: np.ndarray, \n",
    "        agent_id: Any, \n",
    "        target_id: Any, \n",
    "        per_mouse_df: Dict = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Trích xuất đặc trưng cho cặp (Agent, Target).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            aid_idx = mouse_ids.index(agent_id)\n",
    "        except ValueError:\n",
    "            return pd.DataFrame() \n",
    "\n",
    "        # 1. Build Agent Context\n",
    "        ctx_agent = self._build_context(\n",
    "            frames, \n",
    "            pos[:, aid_idx, :], \n",
    "            per_mouse_df.get(agent_id) if per_mouse_df else None\n",
    "        )\n",
    "\n",
    "        # 2. Build Target Context\n",
    "        ctx_target = None\n",
    "        if self.cfg.use_pairwise and target_id is not None and target_id in mouse_ids:\n",
    "             tid_idx = mouse_ids.index(target_id)\n",
    "             ctx_target = self._build_context(\n",
    "                 frames, \n",
    "                 pos[:, tid_idx, :], \n",
    "                 per_mouse_df.get(target_id) if per_mouse_df else None\n",
    "             )\n",
    "\n",
    "        # 3. Run all features\n",
    "        all_data = {}\n",
    "        for func_name, func in self.feature_registry.items():\n",
    "            out_dict = func(ctx_agent, target_ctx=ctx_target)\n",
    "            all_data.update(out_dict)\n",
    "\n",
    "        df_out = pd.DataFrame(all_data, index=ctx_agent.idx)\n",
    "        df_out = df_out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        \n",
    "        return df_out.reindex(sorted(df_out.columns), axis=1)\n",
    "\n",
    "\n",
    "# =============================================================================================\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# (Trên Kaggle) dùng metric chính thức\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/usr/lib/mabe-f-beta\")\n",
    "from metric import score   # hàm score(submission_df, dataset_df)\n",
    "\n",
    "# =========================================================\n",
    "# 1. ĐƯỜNG DẪN & CẤU HÌNH\n",
    "# =========================================================\n",
    "\n",
    "INPUT_DIR = Path(\"/kaggle/input/MABe-mouse-behavior-detection\")\n",
    "TRAIN_TRACKING_DIR = INPUT_DIR / \"train_tracking\"\n",
    "TRAIN_ANNOTATION_DIR = INPUT_DIR / \"train_annotation\"\n",
    "TEST_TRACKING_DIR = INPUT_DIR / \"test_tracking\"\n",
    "\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "RESULTS_DIR = Path(r\"/kaggle/input/results-xgb-fe\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "INDEX_COLS = [\"video_id\", \"agent_id\", \"target_id\", \"video_frame\"]\n",
    "\n",
    "# hành vi “self” vs “pair” giống notebook (có thể chỉnh nếu muốn)\n",
    "SELF_BEHAVIORS = [\n",
    "    \"biteobject\", \"climb\", \"dig\", \"exploreobject\", \"freeze\",\n",
    "    \"genitalgroom\", \"huddle\", \"rear\", \"rest\", \"run\", \"selfgroom\",\n",
    "]\n",
    "PAIR_BEHAVIORS = [\n",
    "    \"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\",\n",
    "    \"chase\", \"chaseattack\", \"defend\", \"disengage\", \"dominance\",\n",
    "    \"dominancegroom\", \"dominancemount\", \"ejaculate\", \"escape\",\n",
    "    \"flinch\", \"follow\", \"intromit\", \"mount\", \"reciprocalsniff\",\n",
    "    \"shepherd\", \"sniff\", \"sniffbody\", \"sniffface\", \"sniffgenital\",\n",
    "    \"submit\", \"tussle\",\n",
    "]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. ĐỌC METADATA & HELPER\n",
    "# =========================================================\n",
    "\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    train_meta = pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "    return train_meta\n",
    "\n",
    "\n",
    "def get_video_params(video_id: Any, meta: pd.DataFrame) -> Tuple[float, float]:\n",
    "    \"\"\"Lấy fps, pix_per_cm cho video từ train.csv.\"\"\"\n",
    "    row = meta.loc[meta[\"video_id\"] == video_id]\n",
    "    if row.empty:\n",
    "        raise KeyError(f\"video_id={video_id} không có trong train.csv\")\n",
    "    row = row.iloc[0]\n",
    "\n",
    "    # giống notebook: cột \"frames per second\" & \"pix per cm (approx)\"\n",
    "    fps = float(row[\"frames_per_second\"])\n",
    "    pix_per_cm = float(row[\"pix_per_cm_approx\"])\n",
    "    if not np.isfinite(pix_per_cm) or pix_per_cm <= 0:\n",
    "        pix_per_cm = 1.0\n",
    "    return fps, pix_per_cm\n",
    "\n",
    "\n",
    "def load_tracking(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    \"\"\"Đọc tracking parquet → pandas (schema: video_frame, mouse_id, bodypart, x, y).\"\"\"\n",
    "    path = TRAIN_TRACKING_DIR / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    df = pd.read_parquet(path)\n",
    "    return df\n",
    "\n",
    "def load_tracking_test(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    \"\"\"Đọc tracking parquet của test → pandas.\"\"\"\n",
    "    path = INPUT_DIR / \"test_tracking\" / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "def load_annotation(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    \"\"\"Đọc annotation (agent_id, target_id, action, start_frame, stop_frame).\"\"\"\n",
    "    path = TRAIN_ANNOTATION_DIR / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists():\n",
    "        # không có label cho video này\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "        )\n",
    "    ann = pd.read_parquet(path)\n",
    "    return ann[[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. TÍNH FEATURE PER-FRAME BẰNG FEATUREEXTRACTOR\n",
    "# =========================================================\n",
    "\n",
    "# Cache: (lab, video, agent, target) -> (frames, feature_df)\n",
    "_feature_cache: Dict[Tuple[str, int, int, int], Tuple[np.ndarray, pd.DataFrame]] = {}\n",
    "\n",
    "\n",
    "def get_frame_features_for_pair(\n",
    "    lab_id: str,\n",
    "    video_id: int,\n",
    "    agent_id: int,\n",
    "    target_id: int,\n",
    "    meta: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Tính (hoặc lấy cache) feature per-frame cho 1 video + (agent, target).\n",
    "    Trả về: frames [F], features_df [F, D]\n",
    "    \"\"\"\n",
    "    key = (str(lab_id), int(video_id), int(agent_id), int(target_id))\n",
    "    if key in _feature_cache:\n",
    "        return _feature_cache[key]\n",
    "\n",
    "    fps, pix_per_cm = get_video_params(video_id, meta)\n",
    "    tracking = load_tracking(lab_id, video_id)\n",
    "\n",
    "    fe = FeatureExtractor(\n",
    "        fps=fps,\n",
    "        pix_per_cm=pix_per_cm,\n",
    "        smooth_sigma=1.0,\n",
    "        use_pairwise=True,\n",
    "    )\n",
    "\n",
    "    frames, mouse_ids, pos, per_mouse_df = fe.build_pose_tensor(tracking)\n",
    "\n",
    "    # agent/target có thể là cùng chuột (self) hoặc khác chuột (pair)\n",
    "    features_df: pd.DataFrame = fe.extract_agent_target(\n",
    "        frames=frames,\n",
    "        mouse_ids=mouse_ids,\n",
    "        pos=pos,\n",
    "        agent_id=agent_id,\n",
    "        target_id=target_id,\n",
    "        per_mouse_df=per_mouse_df,\n",
    "    )\n",
    "    # index chính là frame\n",
    "    features_df.index = frames\n",
    "\n",
    "    _feature_cache[key] = (frames, features_df)\n",
    "    return frames, features_df\n",
    "\n",
    "_feature_cache: Dict[Tuple[str, int, Any, Any], Tuple[np.ndarray, pd.DataFrame]] = {}\n",
    "\n",
    "def get_frame_features_for_pair_test(\n",
    "    lab_id: str,\n",
    "    video_id: int,\n",
    "    agent_id: Any,\n",
    "    target_id: Any,\n",
    "    test_meta: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Feature per-frame cho test (video_id, agent, target).\n",
    "    Trả về: frames [F], features_df [F, D]\n",
    "    \"\"\"\n",
    "    key = (f\"test_{lab_id}\", int(video_id), agent_id, target_id)\n",
    "    if key in _feature_cache:\n",
    "        return _feature_cache[key]\n",
    "\n",
    "    # Lấy fps, pix_per_cm_approx từ test.csv\n",
    "    row = test_meta[test_meta[\"video_id\"] == video_id].iloc[0]\n",
    "    fps = float(row[\"frames_per_second\"])\n",
    "    pix_per_cm = float(row[\"pix_per_cm_approx\"])\n",
    "    if not np.isfinite(pix_per_cm) or pix_per_cm <= 0:\n",
    "        pix_per_cm = 1.0\n",
    "\n",
    "    tracking = load_tracking_test(lab_id, video_id)\n",
    "\n",
    "    fe = FeatureExtractor(\n",
    "        fps=fps,\n",
    "        pix_per_cm=pix_per_cm,\n",
    "        smooth_sigma=1.0,\n",
    "        use_pairwise=True,\n",
    "    )\n",
    "\n",
    "    frames, mouse_ids, pos, per_mouse_df = fe.build_pose_tensor(tracking)\n",
    "\n",
    "    features_df = fe.extract_agent_target(\n",
    "        frames=frames,\n",
    "        mouse_ids=mouse_ids,\n",
    "        pos=pos,\n",
    "        agent_id=agent_id,\n",
    "        target_id=target_id,\n",
    "        per_mouse_df=per_mouse_df,\n",
    "    )\n",
    "    features_df.index = frames\n",
    "\n",
    "    _feature_cache[key] = (frames, features_df)\n",
    "    return frames, features_df\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. BUILD FRAME-LEVEL DATASET CHO 1 (lab_id, behavior)\n",
    "# =========================================================\n",
    "\n",
    "def build_frame_dataset_for_lab_behavior(\n",
    "    lab_id: str,\n",
    "    behavior: str,\n",
    "    train_meta: pd.DataFrame,\n",
    "    mode: str = \"self\",\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Xây tập frame-level (indices, features, labels) cho 1 (lab, behavior).\n",
    "\n",
    "    indices: DataFrame với cột INDEX_COLS\n",
    "    features: DataFrame per-frame features\n",
    "    labels: np.ndarray nhị phân (0/1)\n",
    "    \"\"\"\n",
    "\n",
    "    videos = (\n",
    "        train_meta[train_meta[\"lab_id\"] == lab_id][\"video_id\"]\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    index_list = []\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for video_id in videos:\n",
    "        ann = load_annotation(lab_id, video_id)\n",
    "        if ann.empty:\n",
    "            continue\n",
    "\n",
    "        # chỉ lấy annotation của behavior này\n",
    "        ann_bhv = ann[ann[\"action\"] == behavior]\n",
    "        if ann_bhv.empty:\n",
    "            continue\n",
    "\n",
    "        # các (agent, target) cần xem\n",
    "        pairs = ann_bhv[[\"agent_id\", \"target_id\"]].drop_duplicates().values.tolist()\n",
    "        for (agent_id, target_id) in pairs:\n",
    "            if mode == \"self\":\n",
    "                target_id_use = agent_id\n",
    "            else:\n",
    "                target_id_use = target_id\n",
    "\n",
    "            frames, feat_df = get_frame_features_for_pair(\n",
    "                lab_id=lab_id,\n",
    "                video_id=video_id,\n",
    "                agent_id=agent_id,\n",
    "                target_id=target_id_use,\n",
    "                meta=train_meta,\n",
    "            )\n",
    "\n",
    "            # label per-frame: frame ∈ bất kỳ [start, stop) của (agent,target,behavior)\n",
    "            ann_pair = ann_bhv[\n",
    "                (ann_bhv[\"agent_id\"] == agent_id)\n",
    "                & (ann_bhv[\"target_id\"] == target_id)\n",
    "            ]\n",
    "            if ann_pair.empty and mode == \"self\":\n",
    "                ann_pair = ann_bhv[ann_bhv[\"agent_id\"] == agent_id]\n",
    "\n",
    "            pos_frames = set()\n",
    "            for _, r in ann_pair.iterrows():\n",
    "                pos_frames.update(range(int(r[\"start_frame\"]), int(r[\"stop_frame\"])))\n",
    "\n",
    "            if len(pos_frames) == 0:\n",
    "                continue\n",
    "\n",
    "            label = np.isin(frames, list(pos_frames)).astype(\"int8\")\n",
    "            if label.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            idx_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"video_id\": video_id,\n",
    "                    \"agent_id\": agent_id,\n",
    "                    \"target_id\": target_id,\n",
    "                    \"video_frame\": frames,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            index_list.append(idx_df)\n",
    "            feature_list.append(feat_df.reset_index(drop=True))\n",
    "            label_list.append(label)\n",
    "\n",
    "    if not index_list:\n",
    "        return (\n",
    "            pd.DataFrame(columns=INDEX_COLS),\n",
    "            pd.DataFrame(),\n",
    "            np.zeros(0, dtype=\"int8\"),\n",
    "        )\n",
    "\n",
    "    indices = pd.concat(index_list, ignore_index=True)\n",
    "    features = pd.concat(feature_list, ignore_index=True)\n",
    "    labels = np.concatenate(label_list).astype(\"int8\")\n",
    "\n",
    "    assert len(indices) == len(features) == len(labels)\n",
    "\n",
    "    return indices, features, labels\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. TRAIN + OOF CHO 1 (lab_id, behavior)\n",
    "# =========================================================\n",
    "\n",
    "def tune_threshold(oof_pred: np.ndarray, y: np.ndarray) -> float:\n",
    "    ths = np.arange(0.0, 1.005, 0.005)\n",
    "    scores = [f1_score(y, (oof_pred >= th), zero_division=0) for th in ths]\n",
    "    return float(ths[int(np.argmax(scores))])\n",
    "\n",
    "#\n",
    "def train_validate_one(\n",
    "    lab_id: str,\n",
    "    behavior: str,\n",
    "    indices: pd.DataFrame,\n",
    "    features: pd.DataFrame,\n",
    "    labels: np.ndarray,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Train XGBoost binary cho 1 (lab, behavior) + lưu OOF prediction.\n",
    "    Trả về: F1 trên toàn bộ OOF (frame-level).\n",
    "    \"\"\"\n",
    "    result_dir = RESULTS_DIR / lab_id / behavior\n",
    "    result_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    n = len(labels)\n",
    "\n",
    "    if n == 0 or labels.sum() == 0:\n",
    "        oof_df = indices.copy()\n",
    "        oof_df[\"fold\"] = -1\n",
    "        oof_df[\"prediction\"] = 0.0\n",
    "        oof_df[\"predicted_label\"] = 0\n",
    "        oof_df.to_parquet(result_dir / \"oof_predictions.parquet\", index=False)\n",
    "        (result_dir / \"f1.txt\").write_text(\"0.0\\n\")\n",
    "        return 0.0\n",
    "\n",
    "    X = features.values.astype(\"float32\")\n",
    "    y = labels.astype(\"int8\")\n",
    "    groups = indices[\"video_id\"].values\n",
    "\n",
    "    folds = np.ones(n, dtype=\"int8\") * -1\n",
    "    oof_pred = np.zeros(n, dtype=\"float32\")\n",
    "    oof_label = np.zeros(n, dtype=\"int8\")\n",
    "\n",
    "    cv = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups=groups)):\n",
    "        fold_dir = result_dir / f\"fold_{fold}\"\n",
    "        fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "        X_va, y_va = X[va_idx], y[va_idx]\n",
    "\n",
    "        # scale_pos_weight\n",
    "        pos = y_tr.sum()\n",
    "        neg = len(y_tr) - pos\n",
    "        scale_pos_weight = float(neg / pos) if pos > 0 else 1.0\n",
    "\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"logloss\",\n",
    "            \"device\": \"cuda\",\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"max_depth\": 6,\n",
    "            \"min_child_weight\": 5,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"scale_pos_weight\": scale_pos_weight,\n",
    "            \"max_bin\": 64,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "\n",
    "        dtrain = xgb.QuantileDMatrix(\n",
    "            X_tr,\n",
    "            label=y_tr,\n",
    "            feature_names=features.columns.tolist(),\n",
    "            max_bin=64,\n",
    "        )\n",
    "        dvalid = xgb.DMatrix(\n",
    "            X_va,\n",
    "            label=y_va,\n",
    "            feature_names=features.columns.tolist(),\n",
    "        )\n",
    "\n",
    "        evals_result: Dict[str, Dict[str, List[float]]] = {}\n",
    "\n",
    "        early_stop = xgb.callback.EarlyStopping(\n",
    "            rounds=10, metric_name=\"logloss\", data_name=\"valid\", maximize=False\n",
    "        )\n",
    "\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=250,\n",
    "            evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "            callbacks=[early_stop],\n",
    "            evals_result=evals_result,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "\n",
    "        pred_va = model.predict(dvalid)\n",
    "        th = tune_threshold(pred_va, y_va)\n",
    "\n",
    "        folds[va_idx] = fold\n",
    "        oof_pred[va_idx] = pred_va\n",
    "        oof_label[va_idx] = (pred_va >= th).astype(\"int8\")\n",
    "\n",
    "        model.save_model(fold_dir / \"model.json\")\n",
    "        with open(fold_dir / \"threshold.txt\", \"w\") as f:\n",
    "            f.write(f\"{th}\\n\")\n",
    "\n",
    "    # lưu OOF\n",
    "    oof_df = indices.copy()\n",
    "    oof_df[\"fold\"] = folds\n",
    "    oof_df[\"prediction\"] = oof_pred\n",
    "    oof_df[\"predicted_label\"] = oof_label\n",
    "    oof_df.to_parquet(result_dir / \"oof_predictions.parquet\", index=False)\n",
    "\n",
    "    f1 = f1_score(y, oof_label, zero_division=0)\n",
    "    (result_dir / \"f1.txt\").write_text(f\"{f1:.6f}\\n\")\n",
    "    return float(f1)\n",
    "\n",
    "def load_models_for_behavior_infer(lab_id: str, behavior: str):\n",
    "    \"\"\"\n",
    "    Đọc các fold model + threshold cho (lab, behavior) từ RESULTS_DIR.\n",
    "    Dùng cho inference (test).\n",
    "    \"\"\"\n",
    "    base_dir = RESULTS_DIR / lab_id / behavior\n",
    "    if not base_dir.exists():\n",
    "        return []\n",
    "\n",
    "    models = []\n",
    "    for fold_dir in sorted(base_dir.glob(\"fold_*\")):\n",
    "        model_file = fold_dir / \"model.json\"\n",
    "        thr_file = fold_dir / \"threshold.txt\"\n",
    "        if not model_file.exists():\n",
    "            continue\n",
    "\n",
    "        booster = xgb.Booster()\n",
    "        booster.load_model(str(model_file))\n",
    "\n",
    "        if thr_file.exists():\n",
    "            thr = float(thr_file.read_text().strip())\n",
    "        else:\n",
    "            thr = 0.5\n",
    "\n",
    "        models.append((booster, thr))\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. LOOP QUA TẤT CẢ BEHAVIORS TRONG 1 LAB\n",
    "#    (train_all_labs_behaviors vẫn giữ nguyên, nhưng main\n",
    "#     sẽ filter train_meta chỉ còn 1 lab)\n",
    "# =========================================================\n",
    "\n",
    "def train_all_labs_behaviors(train_meta: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Loop qua từng lab trong train_meta (ở đây main đã filter chỉ còn 1 lab):\n",
    "      - đọc annotation của tất cả video\n",
    "      - lấy unique action xuất hiện trong lab đó\n",
    "      - train 1 model/frame-level cho từng (lab, action)\n",
    "    \"\"\"\n",
    "    labs = train_meta[\"lab_id\"].unique().tolist()\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    for lab_id in labs:\n",
    "        # tập video của lab này\n",
    "        videos = train_meta[train_meta[\"lab_id\"] == lab_id][\"video_id\"].unique().tolist()\n",
    "\n",
    "        # gom toàn bộ action thực sự có trong annotation của lab này\n",
    "        behaviors_set = set()\n",
    "        for vid in videos:\n",
    "            ann = load_annotation(lab_id, vid)\n",
    "            if ann.empty:\n",
    "                continue\n",
    "            behaviors_set.update(ann[\"action\"].unique().tolist())\n",
    "\n",
    "        behaviors = sorted(behaviors_set)\n",
    "        print(f\"\\n===== LAB {lab_id}: {len(behaviors)} behaviors =====\")\n",
    "\n",
    "        for behavior in behaviors:\n",
    "            # if behavior != \"submit\": continue\n",
    "\n",
    "            mode = \"self\" if behavior in SELF_BEHAVIORS else \"pair\"\n",
    "\n",
    "            print(f\"\\n=== LAB={lab_id} | behavior={behavior} | mode={mode} ===\")\n",
    "            indices, features, labels = build_frame_dataset_for_lab_behavior(\n",
    "                lab_id=str(lab_id),\n",
    "                behavior=behavior,\n",
    "                train_meta=train_meta,\n",
    "                mode=mode,\n",
    "            )\n",
    "            print(\n",
    "                f\"frames: {len(labels):,}, positives: {labels.sum():,}, features: \"\n",
    "                f\"{features.shape[1] if not features.empty else 0}\"\n",
    "            )\n",
    "\n",
    "            if len(labels) == 0:\n",
    "                print(\" -> skip (no samples)\")\n",
    "                continue\n",
    "\n",
    "            f1 = train_validate_one(str(lab_id), behavior, indices, features, labels)\n",
    "            elapsed = time.perf_counter() - start_time\n",
    "            print(f\" -> OOF F1 (frame-level): {f1:.3f} | elapsed={elapsed/60:.1f} min\")\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. GOM OOF PREDICTION → SEGMENT & TÍNH SCORE()\n",
    "# =========================================================\n",
    "\n",
    "def build_oof_submission_from_parquet(\n",
    "    target_lab_id: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Đọc tất cả oof_predictions.parquet trong RESULTS_DIR,\n",
    "    gom thành frame-level table rồi nối thành segment-level prediction\n",
    "    giống inference notebook (simplified).\n",
    "\n",
    "    Nếu target_lab_id != None thì chỉ lấy OOF của lab đó\n",
    "    (vd \"AdaptableSnail\").\n",
    "    \"\"\"\n",
    "    oof_files = list(RESULTS_DIR.glob(\"*/**/oof_predictions.parquet\"))\n",
    "    if not oof_files:\n",
    "        raise RuntimeError(\"Không tìm thấy OOF parquet, hãy train trước.\")\n",
    "\n",
    "    frame_preds = []\n",
    "\n",
    "    for path in oof_files:\n",
    "        # path: results_xgb_fe/lab/behavior/oof_predictions.parquet\n",
    "        parts = path.parts\n",
    "        behavior = parts[-2]\n",
    "        lab_id = parts[-3]\n",
    "\n",
    "        # chỉ lấy file thuộc lab mong muốn (nếu có)\n",
    "        if target_lab_id is not None and lab_id != target_lab_id:\n",
    "            continue\n",
    "\n",
    "        df = pd.read_parquet(path)\n",
    "        df = df[INDEX_COLS + [\"prediction\"]].copy()\n",
    "        df[\"lab_id\"] = lab_id\n",
    "        df[\"action\"] = behavior\n",
    "        frame_preds.append(df)\n",
    "\n",
    "    if not frame_preds:\n",
    "        raise RuntimeError(\n",
    "            f\"Không có OOF predictions nào cho lab_id={target_lab_id}\"\n",
    "        )\n",
    "\n",
    "    frame_df = pd.concat(frame_preds, ignore_index=True)\n",
    "\n",
    "    # sắp xếp\n",
    "    frame_df = frame_df.sort_values(\n",
    "        [\"lab_id\", \"video_id\", \"agent_id\", \"target_id\", \"action\", \"video_frame\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Convert frame-level prob -> hard label + segments\n",
    "    segments = []\n",
    "    for (lab_id, video_id, agent_id, target_id, action), group in frame_df.groupby(\n",
    "        [\"lab_id\", \"video_id\", \"agent_id\", \"target_id\", \"action\"], sort=False\n",
    "    ):\n",
    "        frames = group[\"video_frame\"].values\n",
    "        scores = group[\"prediction\"].values\n",
    "\n",
    "        # dùng một threshold fix (vd 0.5) cho demo\n",
    "        # (hoặc bạn có thể lưu threshold per (lab,behavior) và apply)\n",
    "        hard = scores >= 0.5\n",
    "\n",
    "        in_seg = False\n",
    "        start = None\n",
    "        prev_f = None\n",
    "\n",
    "        for f, h in zip(frames, hard):\n",
    "            if h and not in_seg:\n",
    "                in_seg = True\n",
    "                start = int(f)\n",
    "            elif (not h) and in_seg:\n",
    "                stop = int(prev_f + 1)  # [start, stop)\n",
    "                segments.append(\n",
    "                    {\n",
    "                        \"lab_id\": lab_id,\n",
    "                        \"video_id\": int(video_id),\n",
    "                        \"agent_id\": int(agent_id),\n",
    "                        \"target_id\": int(target_id),\n",
    "                        \"action\": action,\n",
    "                        \"start_frame\": start,\n",
    "                        \"stop_frame\": stop,\n",
    "                    }\n",
    "                )\n",
    "                in_seg = False\n",
    "            prev_f = f\n",
    "\n",
    "        if in_seg:\n",
    "            stop = int(frames[-1] + 1)\n",
    "            segments.append(\n",
    "                {\n",
    "                    \"lab_id\": lab_id,\n",
    "                    \"video_id\": int(video_id),\n",
    "                    \"agent_id\": int(agent_id),\n",
    "                    \"target_id\": int(target_id),\n",
    "                    \"action\": action,\n",
    "                    \"start_frame\": start,\n",
    "                    \"stop_frame\": stop,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not segments:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"lab_id\",\n",
    "                \"video_id\",\n",
    "                \"agent_id\",\n",
    "                \"target_id\",\n",
    "                \"action\",\n",
    "                \"start_frame\",\n",
    "                \"stop_frame\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    submission = pd.DataFrame(segments)\n",
    "    submission = submission.sort_values(\n",
    "        [\"lab_id\", \"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return submission\n",
    "\n",
    "BAD_VIDEOS = []\n",
    "\n",
    "def compute_validation_score(\n",
    "    submission: pd.DataFrame,\n",
    "    lab_id: Optional[str] = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Gọi metric `score()` chính thức trên train set.\n",
    "    Nếu lab_id != None → chỉ validate trên lab đó.\n",
    "    \"\"\"\n",
    "    # ===== THAY ĐỔI Ở ĐÂY =====\n",
    "    # Không dùng train.csv, mà phải đọc toàn bộ annotations\n",
    "    train_meta = pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "    \n",
    "    if lab_id is not None:\n",
    "        train_meta = train_meta[train_meta[\"lab_id\"] == lab_id].reset_index(drop=True)\n",
    "\n",
    "    if BAD_VIDEOS:\n",
    "        train_meta = train_meta[~train_meta[\"video_id\"].isin(BAD_VIDEOS)]\n",
    "    \n",
    "    # Đọc tất cả annotation files\n",
    "    all_annotations = []\n",
    "    for _, row in train_meta.iterrows():\n",
    "        lab = row[\"lab_id\"]\n",
    "        vid = row[\"video_id\"]\n",
    "        ann = load_annotation(lab, vid)\n",
    "        if not ann.empty:\n",
    "            ann[\"lab_id\"] = lab\n",
    "            ann[\"video_id\"] = vid\n",
    "            ann[\"behaviors_labeled\"] = row[\"behaviors_labeled\"]\n",
    "            all_annotations.append(ann)\n",
    "    \n",
    "    if not all_annotations:\n",
    "        print(\"Không có annotation nào để validate!\")\n",
    "        return 0.0\n",
    "    \n",
    "    dataset = pd.concat(all_annotations, ignore_index=True)\n",
    "    \n",
    "    # Filter submission theo lab nếu cần\n",
    "    if lab_id is not None:\n",
    "        submission = submission[submission[\"lab_id\"] == lab_id].reset_index(drop=True)\n",
    "    \n",
    "    # ===== GỌI METRIC =====\n",
    "    s = score(dataset, submission, row_id_column_name=\"row_id\")\n",
    "\n",
    "    print(\n",
    "        f\"Official validation score\"\n",
    "        f\"{' (lab=' + lab_id + ')' if lab_id is not None else ''}: {s:.6f}\"\n",
    "    )\n",
    "    return float(s)\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. MAIN\n",
    "# =========================================================\n",
    "def str_to_mouse_id(s: str) -> int:\n",
    "    if s == \"self\":\n",
    "        return -1\n",
    "    return int(str(s).replace(\"mouse\", \"\"))\n",
    "\n",
    "\n",
    "def predict_behaviors_for_pair(\n",
    "    lab_id: str,\n",
    "    video_id: int,\n",
    "    agent_internal_id: Any,\n",
    "    target_internal_id: Any,\n",
    "    behaviors: List[str],\n",
    "    test_meta: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Chạy inference cho 1 cặp (video, agent_internal_id, target_internal_id)\n",
    "    với list behaviors (cùng mode: all self hoặc all pair).\n",
    "    Trả về segment-level DataFrame: video_id, action, start_frame, stop_frame.\n",
    "    \"\"\"\n",
    "    if lab_id != \"ElegantMink\": return None\n",
    "    frames, feat_df = get_frame_features_for_pair_test(\n",
    "        lab_id=lab_id,\n",
    "        video_id=video_id,\n",
    "        agent_id=agent_internal_id,\n",
    "        target_id=target_internal_id,\n",
    "        test_meta=test_meta,\n",
    "    )\n",
    "    if feat_df.empty:\n",
    "        return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "\n",
    "    feat_df = feat_df.astype(\"float32\")\n",
    "    n_frames = len(feat_df)\n",
    "\n",
    "    scores_per_behavior = {}\n",
    "    for behavior in behaviors:\n",
    "        models = load_models_for_behavior_infer(lab_id, behavior)\n",
    "        if not models:\n",
    "            continue\n",
    "\n",
    "        req_feats = models[0][0].feature_names\n",
    "        # Build X_test với đúng bộ feature của model\n",
    "        X_test = pd.DataFrame(\n",
    "            0.0,\n",
    "            index=feat_df.index,\n",
    "            columns=req_feats,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        common = list(set(req_feats) & set(feat_df.columns))\n",
    "        if common:\n",
    "            X_test[common] = feat_df[common]\n",
    "\n",
    "        dtest = xgb.DMatrix(X_test, feature_names=req_feats)\n",
    "\n",
    "        agg_scores = np.zeros(n_frames, dtype=np.float32)\n",
    "        for booster, thr in models:\n",
    "            probs = booster.predict(dtest)\n",
    "            labels = (probs >= thr).astype(np.int8)\n",
    "            agg_scores += probs * labels\n",
    "\n",
    "        agg_scores /= max(len(models), 1)\n",
    "        scores_per_behavior[behavior] = agg_scores\n",
    "\n",
    "        del dtest, X_test\n",
    "        gc.collect()\n",
    "\n",
    "    if not scores_per_behavior:\n",
    "        return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "\n",
    "    beh_list = list(scores_per_behavior.keys())\n",
    "    score_mat = np.vstack([scores_per_behavior[b] for b in beh_list]).T  # [F, B]\n",
    "\n",
    "    max_idx = score_mat.argmax(axis=1)\n",
    "    max_scores = score_mat.max(axis=1)\n",
    "    labels = np.where(max_scores == 0.0, \"none\", np.array(beh_list)[max_idx])\n",
    "\n",
    "    # frame-level → segment\n",
    "    segments = []\n",
    "    prev_lab = \"none\"\n",
    "    prev_start = None\n",
    "    prev_f = None\n",
    "\n",
    "    for f, lab in zip(frames, labels):\n",
    "        if lab != prev_lab:\n",
    "            if prev_lab != \"none\":\n",
    "                segments.append(\n",
    "                    {\n",
    "                        \"video_id\": int(video_id),\n",
    "                        \"action\": prev_lab,\n",
    "                        \"start_frame\": int(prev_start),\n",
    "                        \"stop_frame\": int(prev_f + 1),\n",
    "                    }\n",
    "                )\n",
    "            prev_lab = lab\n",
    "            prev_start = f\n",
    "        prev_f = f\n",
    "\n",
    "    if prev_lab != \"none\":\n",
    "        segments.append(\n",
    "            {\n",
    "                \"video_id\": int(video_id),\n",
    "                \"action\": prev_lab,\n",
    "                \"start_frame\": int(prev_start),\n",
    "                \"stop_frame\": int(prev_f + 1),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not segments:\n",
    "        return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "\n",
    "    return pd.DataFrame(segments)\n",
    "\n",
    "\n",
    "\n",
    "target_lab = \"ElegantMink\"\n",
    "\n",
    "print(f\"Đọc test.csv cho lab {target_lab} ...\")\n",
    "test_meta = pd.read_csv(INPUT_DIR / \"test.csv\")\n",
    "test_meta = test_meta[test_meta[\"lab_id\"] == target_lab].reset_index(drop=True)\n",
    "\n",
    "# Lấy danh sách behavior đã train (thư mục con trong RESULTS_DIR/AdaptableSnail)\n",
    "lab_result_dir = RESULTS_DIR / target_lab\n",
    "if lab_result_dir.exists():\n",
    "    trained_behaviors = sorted(\n",
    "        [p.name for p in lab_result_dir.iterdir() if p.is_dir()]\n",
    "    )\n",
    "else:\n",
    "    trained_behaviors = []\n",
    "\n",
    "self_behaviors_in_lab = [b for b in trained_behaviors if b in SELF_BEHAVIORS]\n",
    "pair_behaviors_in_lab = [b for b in trained_behaviors if b in PAIR_BEHAVIORS]\n",
    "\n",
    "print(\"Behaviors (self) dùng để predict:\", self_behaviors_in_lab)\n",
    "print(\"Behaviors (pair) dùng để predict:\", pair_behaviors_in_lab)\n",
    "\n",
    "all_segments = []\n",
    "\n",
    "# Loop từng video test của lab\n",
    "for video_id in sorted(test_meta[\"video_id\"].unique()):\n",
    "    print(f\"Predict video_id={video_id} ...\")\n",
    "\n",
    "    tracking = load_tracking_test(target_lab, video_id)\n",
    "    mouse_ids_internal = sorted(tracking[\"mouse_id\"].unique().tolist())\n",
    "\n",
    "    # Map internal mouse_id -> string để đưa vào submission\n",
    "    def to_submit_id(mid):\n",
    "        s = str(mid)\n",
    "        return s if s.startswith(\"mouse\") else f\"mouse{s}\"\n",
    "\n",
    "    # SELF behaviors: agent == target (self)\n",
    "    if self_behaviors_in_lab:\n",
    "        for mid in mouse_ids_internal:\n",
    "            seg_df = predict_behaviors_for_pair(\n",
    "                lab_id=target_lab,\n",
    "                video_id=video_id,\n",
    "                agent_internal_id=mid,\n",
    "                target_internal_id=mid,  # self\n",
    "                behaviors=self_behaviors_in_lab,\n",
    "                test_meta=test_meta,\n",
    "            )\n",
    "            if not seg_df.empty:\n",
    "                seg_df[\"agent_id\"] = to_submit_id(mid)\n",
    "                seg_df[\"target_id\"] = \"self\"\n",
    "                all_segments.append(seg_df)\n",
    "\n",
    "    # PAIR behaviors: mọi cặp agent != target\n",
    "    if pair_behaviors_in_lab and len(mouse_ids_internal) > 1:\n",
    "        for agent_internal, target_internal in itertools.permutations(\n",
    "            mouse_ids_internal, 2\n",
    "        ):\n",
    "            seg_df = predict_behaviors_for_pair(\n",
    "                lab_id=target_lab,\n",
    "                video_id=video_id,\n",
    "                agent_internal_id=agent_internal,\n",
    "                target_internal_id=target_internal,\n",
    "                behaviors=pair_behaviors_in_lab,\n",
    "                test_meta=test_meta,\n",
    "            )\n",
    "            if not seg_df.empty:\n",
    "                seg_df[\"agent_id\"] = to_submit_id(agent_internal)\n",
    "                seg_df[\"target_id\"] = to_submit_id(target_internal)\n",
    "                all_segments.append(seg_df)\n",
    "\n",
    "# Gộp tất cả segments → submission.csv\n",
    "if all_segments:\n",
    "    submission3 = pd.concat(all_segments, ignore_index=True)\n",
    "    submission3 = submission3[\n",
    "        [\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "    ]\n",
    "    submission3 = submission3.sort_values(\n",
    "        [\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\"]\n",
    "    ).reset_index(drop=True)\n",
    "else:\n",
    "    # DataFrame rỗng, KHÔNG dummy row\n",
    "    submission3 = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"video_id\",\n",
    "            \"agent_id\",\n",
    "            \"target_id\",\n",
    "            \"action\",\n",
    "            \"start_frame\",\n",
    "            \"stop_frame\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Thêm row_id (kể cả khi rỗng)\n",
    "submission3.insert(0, \"row_id\", np.arange(len(submission3), dtype=np.int64))\n",
    "\n",
    "sub_path = WORKING_DIR / \"submission3.csv\"\n",
    "submission3.to_csv(sub_path, index=False)\n",
    "print(f\"Saved ElegantMink submission to {sub_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c710f15",
   "metadata": {
    "papermill": {
     "duration": 0.018203,
     "end_time": "2025-12-13T17:39:16.833614",
     "exception": false,
     "start_time": "2025-12-13T17:39:16.815411",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GroovyShrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db3c21e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:16.870132Z",
     "iopub.status.busy": "2025-12-13T17:39:16.869474Z",
     "iopub.status.idle": "2025-12-13T17:39:16.992202Z",
     "shell.execute_reply": "2025-12-13T17:39:16.991466Z"
    },
    "papermill": {
     "duration": 0.142212,
     "end_time": "2025-12-13T17:39:16.993532",
     "exception": false,
     "start_time": "2025-12-13T17:39:16.851320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "\n",
    "# 1) Xóa mọi thứ trong /kaggle/working trừ .csv\n",
    "for path in WORKING_DIR.iterdir():\n",
    "    # giữ lại file .csv\n",
    "    if path.is_file() and path.suffix == \".csv\":\n",
    "        continue\n",
    "\n",
    "    if path.is_file():\n",
    "        try:\n",
    "            path.unlink()\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove file {path}: {e}\")\n",
    "    elif path.is_dir():\n",
    "        try:\n",
    "            shutil.rmtree(path, ignore_errors=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove dir {path}: {e}\")\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63caa75b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:17.033020Z",
     "iopub.status.busy": "2025-12-13T17:39:17.032794Z",
     "iopub.status.idle": "2025-12-13T17:39:17.190542Z",
     "shell.execute_reply": "2025-12-13T17:39:17.189643Z"
    },
    "papermill": {
     "duration": 0.179759,
     "end_time": "2025-12-13T17:39:17.191735",
     "exception": false,
     "start_time": "2025-12-13T17:39:17.011976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đọc test.csv cho lab GroovyShrew ...\n",
      "Behaviors (self) dùng để predict: ['climb', 'dig', 'rear', 'rest', 'run', 'selfgroom']\n",
      "Behaviors (pair) dùng để predict: ['approach', 'attemptmount', 'defend', 'escape', 'sniff', 'sniffgenital']\n",
      "Saved GroovyShrew submission to /kaggle/working/submission4.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "np.seterr(invalid=\"ignore\", divide=\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    \"\"\"\n",
    "    Chứa cấu hình tham số (Hyperparameters).\n",
    "    \"\"\"\n",
    "    fps: float = 30.0\n",
    "    pix_per_cm: float = 1.0\n",
    "    smooth_sigma: float = 1.0\n",
    "    use_pairwise: bool = True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. AGENT CONTEXT\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class AgentContext:\n",
    "    \"\"\"\n",
    "    Container chứa dữ liệu đã tiền xử lý của một con chuột.\n",
    "    Giúp tránh việc tính toán lại vận tốc/gia tốc nhiều lần.\n",
    "    \"\"\"\n",
    "    idx: pd.Index          # Index frame\n",
    "    pos: np.ndarray        # [F, 2] cm\n",
    "    vel: np.ndarray        # [F, 2] cm/s\n",
    "    speed: np.ndarray      # [F, 1] cm/s\n",
    "    acc: np.ndarray        # [F, 2] cm/s^2\n",
    "    \n",
    "    cx: pd.Series          # Series tọa độ X (để dùng rolling)\n",
    "    cy: pd.Series          # Series tọa độ Y\n",
    "    speed_series: pd.Series # Series tốc độ\n",
    "    \n",
    "    raw_df: Optional[pd.DataFrame] = None # Dữ liệu gốc các bộ phận \n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. FEATURE EXTRACTOR\n",
    "# =============================================================================\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Class trích xuất đặc trưng hành vi từ dữ liệu tracking.\n",
    "    \"\"\"\n",
    "    def __init__(self, fps: float, pix_per_cm: float, smooth_sigma: float = 1.0, use_pairwise: bool = True):\n",
    "        # Map tham số từ init vào Config\n",
    "        self.cfg = FeatureConfig(\n",
    "            fps=float(fps), \n",
    "            pix_per_cm=float(pix_per_cm), \n",
    "            smooth_sigma=smooth_sigma,\n",
    "            use_pairwise=use_pairwise\n",
    "        )\n",
    "        \n",
    "        # Đăng ký các hàm feature sẽ chạy\n",
    "        self.feature_registry = {\n",
    "            \"kinematics\": self._feat_basic_kinematics,\n",
    "            \"multiscale\": self._feat_multiscale,\n",
    "            \"long_range\": self._feat_long_range,\n",
    "            \"cumulative\": self._feat_cumulative,\n",
    "            \"curvature\": self._feat_curvature,\n",
    "            \"speed_asym\": self._feat_speed_asym,\n",
    "            \"gauss_shift\": self._feat_gauss_shift,\n",
    "            \"pose_shape\": self._feat_pose_shape,\n",
    "            \"pairwise\": self._feat_pairwise,\n",
    "            \"follow\": self._feat_follow_pattern,\n",
    "            \"short\": self._feat_shortburst_social,\n",
    "            \"a\": self._feat_attack_sniff,\n",
    "            \"b\": self._feat_climb\n",
    "        }\n",
    "\n",
    "    # --- Helpers ---\n",
    "    def _scale(self, n_frames_30fps: int) -> int:\n",
    "        \"\"\"Quy đổi số frame từ chuẩn 30fps sang fps thực tế của video.\"\"\"\n",
    "        return max(1, int(round(n_frames_30fps * self.cfg.fps / 30.0)))\n",
    "\n",
    "    def _to_cm(self, arr):\n",
    "        \"\"\"Chuyển pixel -> cm.\"\"\"\n",
    "        return arr / self.cfg.pix_per_cm\n",
    "\n",
    "    def _smooth(self, x):\n",
    "        \"\"\"Làm mượt dữ liệu bằng Gaussian filter.\"\"\"\n",
    "        if self.cfg.smooth_sigma is None or x.shape[0] < 3: return x\n",
    "        if np.all(np.isnan(x)): return x\n",
    "        return gaussian_filter1d(x, sigma=self.cfg.smooth_sigma, axis=0, mode=\"nearest\")\n",
    "\n",
    "    def _forward_fill_nan(self, pos):\n",
    "        \"\"\"\n",
    "        Điền dữ liệu thiếu (NaN) bằng giá trị hợp lệ trước đó (Forward Fill).\n",
    "        \"\"\"\n",
    "        if np.all(np.isnan(pos)):\n",
    "            return np.zeros_like(pos)\n",
    "\n",
    "        pos_ffill = pos.copy()\n",
    "        mask = np.any(~np.isnan(pos_ffill), axis=1)\n",
    "        if not mask.any():\n",
    "            return np.zeros_like(pos_ffill)\n",
    "\n",
    "        valid_idx = np.where(mask)[0]\n",
    "        first, last = valid_idx[0], valid_idx[-1]\n",
    "        pos_ffill[:first] = pos_ffill[first]\n",
    "        pos_ffill[last + 1:] = pos_ffill[last]\n",
    "        df_temp = pd.DataFrame(pos_ffill)\n",
    "        df_temp = df_temp.ffill()\n",
    "        return df_temp.to_numpy()\n",
    "    \n",
    "    def _speed_series(self, cx: pd.Series, cy: pd.Series) -> pd.Series:\n",
    "        dx = cx.diff()\n",
    "        dy = cy.diff()\n",
    "        v = np.hypot(dx, dy).fillna(0.0) * self.cfg.fps\n",
    "        return v.astype(\"float32\")\n",
    "    \n",
    "    def _roll_future_mean(self, s: pd.Series, w: int, min_p: int = 1) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).mean().iloc[::-1]\n",
    "\n",
    "    def _roll_future_var(self, s: pd.Series, w: int, min_p: int = 2) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).var().iloc[::-1]\n",
    "\n",
    "    # --- Core Logic ---\n",
    "    def _compute_kinematics(self, pos_px: np.ndarray):\n",
    "        \"\"\"\n",
    "        Tính toán vật lý cơ bản: Pos(cm), Vel, Speed, Acc.\n",
    "        Input: Array [Frames, 2] (pixel).\n",
    "        Output: Tuple (pos_cm, vel, speed, acc).\n",
    "        \"\"\"\n",
    "        pos_ffill = self._forward_fill_nan(pos_px)\n",
    "        pos_cm = self._to_cm(pos_ffill.astype(np.float32))\n",
    "        pos_cm = self._smooth(pos_cm)                                               # [F, 2]\n",
    "\n",
    "        dt = 1.0 / self.cfg.fps\n",
    "        vel = np.zeros_like(pos_cm, dtype=np.float32)\n",
    "        vel[1:] = (pos_cm[1:] - pos_cm[:-1]) / dt                                   # [F, 2: (vx, vy)]\n",
    "        speed = np.linalg.norm(vel, axis=1, keepdims=True).astype(np.float32)       # [F, 1]\n",
    "\n",
    "        acc = np.zeros_like(pos_cm, dtype=np.float32)                          \n",
    "        acc[1:] = (vel[1:] - vel[:-1]) / dt                                         # [F, 2:(ax, ay)]\n",
    "        return pos_cm.astype(np.float32), vel, speed, acc\n",
    "\n",
    "    def _build_context(self, frames, pos_px, mouse_df=None) -> AgentContext:\n",
    "        \"\"\"\n",
    "        Tạo AgentContext chứa đầy đủ thông tin vật lý của 1 con chuột.\n",
    "        \"\"\"\n",
    "        p, v, s, a = self._compute_kinematics(pos_px)\n",
    "        idx = pd.Index(frames, name=\"frame\")\n",
    "        \n",
    "        return AgentContext(\n",
    "            idx=idx, pos=p, vel=v, speed=s, acc=a, \n",
    "            cx=pd.Series(p[:, 0], index=idx), \n",
    "            cy=pd.Series(p[:, 1], index=idx), \n",
    "            speed_series=pd.Series(s[:, 0], index=idx), \n",
    "            raw_df=mouse_df\n",
    "        )\n",
    "\n",
    "    # --- Feature Modules ---\n",
    "    def _feat_basic_kinematics(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Lấy các giá trị thô: tọa độ x, y, vận tốc vx, vy, tốc độ, gia tốc ax, ay.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"a_x\": ctx.pos[:, 0], \"a_y\": ctx.pos[:, 1],\n",
    "            \"a_vx\": ctx.vel[:, 0], \"a_vy\": ctx.vel[:, 1],\n",
    "            \"a_speed\": ctx.speed[:, 0],\n",
    "            \"a_ax\": ctx.acc[:, 0], \"a_ay\": ctx.acc[:, 1]\n",
    "        }\n",
    "\n",
    "    def _feat_multiscale(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tính tốc độ trung bình (Mean) và độ lệch chuẩn (Std) ở đa mức thời gian.\n",
    "        Feature 'sp_ratio' đo độ bùng nổ (Burstiness).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        speed = ctx.speed_series\n",
    "        frame_scales = [10, 40, 160]\n",
    "        for scale in frame_scales:\n",
    "            ws = self._scale(scale)\n",
    "            if len(speed) >= ws:\n",
    "                roller = speed.rolling(ws, min_periods=max(1, ws//4), center=True)\n",
    "                feats[f\"sp_m{scale}\"] = roller.mean().astype(\"float32\")\n",
    "                feats[f\"sp_s{scale}\"] = roller.std().astype(\"float32\")\n",
    "        feats[f\"sp_ratio\"] = feats[\"sp_m10\"] / (feats[\"sp_m160\"] + 1e-6)\n",
    "        return feats \n",
    "        \n",
    "    def _feat_long_range(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng ngữ cảnh dài hạn:\n",
    "        - x_ml, y_ml: Vị trí trung bình trong quá khứ.\n",
    "        - sp_pct: Xếp hạng (percentile) của tốc độ hiện tại so với quá khứ.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        speed = ctx.speed_series\n",
    "\n",
    "        for window in [120, 240]:\n",
    "            ws = self._scale(window)\n",
    "            if len(ctx.cx) >= ws:\n",
    "                feats[f\"x_ml{window}\"] = ctx.cx.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "                feats[f\"y_ml{window}\"] = ctx.cy.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "\n",
    "        for span in [60, 120]:\n",
    "            s = self._scale(span)\n",
    "            feats[f\"x_e{span}\"] = ctx.cx.ewm(span=s, min_periods=1).mean()\n",
    "            feats[f\"y_e{span}\"] = ctx.cy.ewm(span=s, min_periods=1).mean()\n",
    "\n",
    "        for window in [60, 120]:\n",
    "            ws = self._scale(window)\n",
    "            if len(speed) >= ws:\n",
    "                feats[f\"sp_pct{window}\"] = speed.rolling(\n",
    "                    ws, min_periods=max(5, ws // 6), center=True\n",
    "                ).rank(pct=True)\n",
    "        return feats\n",
    "    \n",
    "\n",
    "    def _feat_curvature(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        feats = {}\n",
    "\n",
    "        vel_x, vel_y = ctx.vel[:, 0], ctx.vel[:, 1]\n",
    "        acc_x, acc_y = ctx.acc[:, 0], ctx.acc[:, 1]\n",
    "        cross_prod = vel_x * acc_y - vel_y * acc_x\n",
    "        vel_mag = np.sqrt(vel_x**2 + vel_y**2)\n",
    "        moving_mask = vel_mag > 2.0\n",
    "        vel_mag_safe = np.maximum(vel_mag, 0.1 / self.cfg.fps)\n",
    "        raw_curv = cross_prod / (vel_mag_safe**3)\n",
    "        raw_curv = np.where(moving_mask, raw_curv, 0.0)\n",
    "        min_turn_radius_cm = 0.5\n",
    "        max_k = 1.0 / min_turn_radius_cm\n",
    "        raw_curv = np.clip(raw_curv, -max_k, max_k)\n",
    "        abs_curv = np.abs(raw_curv)\n",
    "        abs_curv_series = pd.Series(abs_curv, index=ctx.idx)\n",
    "\n",
    "        for w in [30, 60]:\n",
    "            ws = self._scale(w)\n",
    "            min_p = max(ws // 3, 1)\n",
    "            feats[f\"curv_mean_{w}\"] = abs_curv_series.rolling(ws, min_periods=min_p).mean()\n",
    "\n",
    "        angle = np.arctan2(vel_y, vel_x)\n",
    "        angle_series = pd.Series(angle, index=ctx.idx)\n",
    "        angle_change = np.abs(angle_series.diff().fillna(0.0))\n",
    "        angle_change = np.where(angle_change > np.pi, 2 * np.pi - angle_change, angle_change)\n",
    "        angle_change_series = pd.Series(angle_change, index=ctx.idx)\n",
    "        angle_change_series = pd.Series(np.where(moving_mask, angle_change_series, 0.0), index=ctx.idx)\n",
    "\n",
    "        ws = self._scale(30)\n",
    "        feats[\"turn_rate_30\"] = angle_change_series.rolling(ws, min_periods=max(ws // 3, 1)).sum()\n",
    "\n",
    "        return feats\n",
    "    \n",
    "    def _feat_cumulative(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tổng quãng đường di chuyển trong một khoảng thời gian dài xung quanh frame hiện tại.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        L = max(1, self._scale(180))\n",
    "        step = np.hypot(ctx.cx.diff(), ctx.cy.diff()).fillna(0.0)\n",
    "        path = step.rolling(2 * L + 1, min_periods=max(5, L // 6), center=True).sum()\n",
    "        feats[\"path_cum180\"] =  path.fillna(0.0).astype(\"float32\")\n",
    "        return feats\n",
    "\n",
    "    def _feat_speed_asym(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Bất đối xứng tốc độ (Tương lai - Quá khứ).\n",
    "        \"\"\"\n",
    "        w = max(3, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        v_past = v.rolling(w, min_periods=1).mean()\n",
    "        v_fut = self._roll_future_mean(v, w, min_p=1)\n",
    "        return {\"spd_asym_1s\": (v_fut - v_past).fillna(0.0)}\n",
    "    \n",
    "    def _feat_gauss_shift(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Độ lệch Gaussian (KL Divergence) giữa quá khứ và tương lai.\n",
    "        Đo lường sự thay đổi trạng thái thống kê.\n",
    "        \"\"\"\n",
    "        w = max(5, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        mu_p = v.rolling(w, min_periods=1).mean()\n",
    "        va_p = v.rolling(w, min_periods=1).var().clip(lower=1e-6)\n",
    "        mu_f = self._roll_future_mean(v, w, min_p=1)\n",
    "        va_f = self._roll_future_var(v, w, min_p=1).clip(lower=1e-6)\n",
    "\n",
    "        kl_pf = 0.5 * (\n",
    "            (va_p / va_f) + ((mu_f - mu_p) ** 2) / va_f - 1.0 + np.log(va_f / va_p)\n",
    "        )\n",
    "        kl_fp = 0.5 * (\n",
    "            (va_f / va_p) + ((mu_p - mu_f) ** 2) / va_p - 1.0 + np.log(va_p / va_f)\n",
    "        )\n",
    "        return {\n",
    "            \"spd_symkl_1s\": (kl_pf + kl_fp).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        }\n",
    "    \n",
    "    def _extract_part(self, ctx: AgentContext, part: str) -> Optional[np.ndarray]:\n",
    "        if ctx.raw_df is None: return None\n",
    "        if part not in ctx.raw_df.columns.get_level_values(0): return None\n",
    "        try:\n",
    "            sub_df = ctx.raw_df.xs(part, axis=1, level=0)[[\"x\", \"y\"]].reindex(ctx.idx)\n",
    "        except KeyError: return None\n",
    "        raw = sub_df.to_numpy()\n",
    "        raw = self._forward_fill_nan(raw)\n",
    "        cm = self._to_cm(raw.astype(np.float32))\n",
    "        return self._smooth(cm)\n",
    "    \n",
    "    def _extract_parts_dict(self, ctx: AgentContext, parts: List[str] = None) -> Dict[str, Optional[np.ndarray]]:\n",
    "        out = {}\n",
    "        for p in parts:\n",
    "            out[p] = self._extract_part(ctx, p)\n",
    "        return out\n",
    "        \n",
    "    def _feat_pose_shape(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Placeholder cho các đặc trưng hình dáng (Elongation, Body Angle...).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "\n",
    "        def zero(): return pd.Series(0.0, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        def dist(k1, k2):\n",
    "            p1, p2 = parts.get(k1), parts.get(k2)\n",
    "            if p1 is None or p2 is None: return zero()\n",
    "            d = np.linalg.norm(p1 - p2, axis=1)\n",
    "            return pd.Series(d, index=ctx.idx, dtype=\"float32\")\n",
    "        \n",
    "        def elongation():\n",
    "            if parts.get(\"nose\")          is None: return zero()\n",
    "            if parts.get(\"tail_base\")     is None: return zero()\n",
    "            if parts.get(\"ear_left\")  is None: return zero()\n",
    "            if parts.get(\"ear_right\") is None: return zero()\n",
    "\n",
    "            d1 = dist(\"nose\", \"tail_base\")\n",
    "            d2 = dist(\"ear_left\", \"ear_right\")\n",
    "            elongation = d1 / (d2 + 1e-6).astype(\"float32\")\n",
    "            return elongation\n",
    "        \n",
    "        def part_speed(part: str, n_frames_30fps: int) -> Dict:\n",
    "            part_pos = self._extract_part(ctx, part)\n",
    "            if part_pos is None: return zero()\n",
    "            \n",
    "            s_x = pd.Series(part_pos[:, 0], index=ctx.idx)\n",
    "            s_y = pd.Series(part_pos[:, 1], index=ctx.idx)\n",
    "            raw_speed = self._speed_series(s_x, s_y)\n",
    "\n",
    "            ws = self._scale(n_frames_30fps)\n",
    "            val = raw_speed.rolling(ws, min_periods=1, center=True).mean()\n",
    "            return val.astype(\"float32\")\n",
    "\n",
    "\n",
    "        target_parts = [\"head\", \"ear_left\", \"ear_right\", \"tail_base\"]\n",
    "        \n",
    "        parts = self._extract_parts_dict(ctx, target_parts)\n",
    "\n",
    "        feats[\"aa_head_tailbase_dist\"]       = dist(\"head\", \"tail_base\")\n",
    "        feats[\"aa_earleft_tailbase_dist\"]    = dist(\"ear_left\", \"tail_base\")\n",
    "        feats[\"aa_earright_tailbase_dist\"]   = dist(\"ear_right\", \"tail_base\")\n",
    "        feats[\"aa_head_earleft_dist\"]        = dist(\"ear_left\", \"head\")\n",
    "        feats[\"aa_head_ear_right_dist\"]      = dist(\"ear_right\", \"head\")\n",
    "        \n",
    "        feats[\"a_elongation\"]                = elongation()\n",
    "        feats[\"a_tail_base_vel_500ms\"]       = part_speed(\"tail_base\", 15)\n",
    "        feats[\"a_tail_base_vel_1000ms\"]      = part_speed(\"tail_base\", 30)\n",
    "        feats[\"a_tail_base_vel_2000ms\"]      = part_speed(\"tail_base\", 60)\n",
    "        feats[\"a_tail_base_vel_3000ms\"]      = part_speed(\"tail_base\", 90)\n",
    "        feats[\"a_head_vel_500ms\"]            = part_speed(\"head\", 15)\n",
    "        feats[\"a_head_vel_1000ms\"]           = part_speed(\"head\", 30)\n",
    "        feats[\"a_head_vel_2000ms\"]           = part_speed(\"head\", 60)\n",
    "        feats[\"a_head_vel_3000ms\"]           = part_speed(\"head\", 90)\n",
    "\n",
    "        feats[\"a_ear_right_vel_500ms\"]       = part_speed(\"ear_right\", 15)\n",
    "        feats[\"a_ear_right_vel_1000ms\"]      = part_speed(\"ear_right\", 30)\n",
    "        feats[\"a_ear_right_vel_2000ms\"]      = part_speed(\"ear_right\", 60)\n",
    "        feats[\"a_ear_right_vel_3000ms\"]       = part_speed(\"ear_right\", 90)\n",
    "        feats[\"a_ear_left_vel_500ms\"]        = part_speed(\"ear_left\", 15)\n",
    "        feats[\"a_ear_left_vel_1000ms\"]       = part_speed(\"ear_left\", 30)\n",
    "        feats[\"a_ear_left_vel_2000ms\"]       = part_speed(\"ear_left\", 60)\n",
    "        feats[\"a_ear_left_vel_3000ms\"]       = part_speed(\"ear_left\", 90)\n",
    "        \n",
    "        return feats\n",
    "\n",
    "    def _feat_attack_sniff(\n",
    "        self,\n",
    "        ctx: AgentContext,\n",
    "        target_ctx: AgentContext = None,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng phân biệt attack vs sniff cho lab 2-mouse (agent=1, target=2).\n",
    "    \n",
    "        Ý tưởng:\n",
    "          - attack: speed 2 con biến động mạnh, đổi hướng nhiều, body overlap cao.\n",
    "          - sniff : mũi gần cổ/thân, overlap thấp hơn, motion nhẹ/ổn định hơn.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "    \n",
    "        def zero():\n",
    "            return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "\n",
    "        # helper khoảng cách\n",
    "        def dist(p1, p2):\n",
    "            if p1 is None or p2 is None:\n",
    "                return zero()\n",
    "            d = np.linalg.norm(p1 - p2, axis=1)\n",
    "            return pd.Series(d, index=idx, dtype=\"float32\")\n",
    "\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"head\", \"tail_base\"])\n",
    "        parts_t = self._extract_parts_dict(target_ctx, [\"head\", \"tail_base\"])\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 2) ĐIỂM ĐẠI DIỆN THÂN (BODY CENTER) CHO MỖI CON\n",
    "        #    dùng trung bình neck – hips – tail_base\n",
    "        # ---------------------------------------------------------\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 4) MỨC ĐỘ “BẠO LỰC”: DAO ĐỘNG TỐC ĐỘ & ĐỔI HƯỚNG\n",
    "        # ---------------------------------------------------------\n",
    "        # speed 2 con từ velocity\n",
    "        a_speed = pd.Series(\n",
    "            np.linalg.norm(ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "\n",
    "        ws_05 = self._scale(15)  # ~0.5s\n",
    "        mp_05 = max(ws_05 // 3, 1)\n",
    "    \n",
    "        feats[\"as_a_speed_std_05\"] = (\n",
    "            a_speed.rolling(ws_05, min_periods=mp_05).std().fillna(0.0).astype(\"float32\")\n",
    "        )\n",
    "        feats[\"as_t_speed_std_05\"] = (\n",
    "            t_speed.rolling(ws_05, min_periods=mp_05).std().fillna(0.0).astype(\"float32\")\n",
    "        )\n",
    "        feats[\"as_speed_std_sum_05\"] = (\n",
    "            feats[\"as_a_speed_std_05\"] + feats[\"as_t_speed_std_05\"]\n",
    "        )\n",
    "    \n",
    "        # Đổi hướng (jerk góc) của agent\n",
    "        a_angle = np.arctan2(ctx.vel[:, 1], ctx.vel[:, 0])\n",
    "        a_angle_diff = np.abs(np.diff(a_angle))\n",
    "        a_angle_diff = np.where(\n",
    "            a_angle_diff > np.pi, 2 * np.pi - a_angle_diff, a_angle_diff\n",
    "        )\n",
    "        a_angle_diff = np.concatenate([[0.0], a_angle_diff])\n",
    "        a_angle_diff_s = pd.Series(a_angle_diff, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        feats[\"as_a_turn_jerk_05\"] = (\n",
    "            a_angle_diff_s.rolling(ws_05, min_periods=mp_05)\n",
    "            .sum()\n",
    "            .fillna(0.0)\n",
    "            .astype(\"float32\")\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 5) XẤP XỈ OVERLAP CƠ THỂ (BODY OVERLAP)\n",
    "        #    dùng bbox từ các bộ phận thân\n",
    "        # ---------------------------------------------------------\n",
    "        def build_bbox(parts: Dict[str, Optional[np.ndarray]]):\n",
    "            arrs = []\n",
    "            for k in [\"head\", \"ear_left\", \"ear_right\", \"tail_base\"]:\n",
    "                if parts.get(k) is not None:\n",
    "                    arrs.append(parts[k])\n",
    "            if not arrs:\n",
    "                return None\n",
    "            stack = np.stack(arrs, axis=1)  # [F, K, 2]\n",
    "            xs = stack[:, :, 0]\n",
    "            ys = stack[:, :, 1]\n",
    "            xmin = np.nanmin(xs, axis=1)\n",
    "            xmax = np.nanmax(xs, axis=1)\n",
    "            ymin = np.nanmin(ys, axis=1)\n",
    "            ymax = np.nanmax(ys, axis=1)\n",
    "            return np.stack([xmin, ymin, xmax, ymax], axis=1).astype(\"float32\")\n",
    "    \n",
    "        def iou_box(box1: np.ndarray, box2: np.ndarray):\n",
    "            # box: [F, 4] = (xmin, ymin, xmax, ymax)\n",
    "            x1 = np.maximum(box1[:, 0], box2[:, 0])\n",
    "            y1 = np.maximum(box1[:, 1], box2[:, 1])\n",
    "            x2 = np.minimum(box1[:, 2], box2[:, 2])\n",
    "            y2 = np.minimum(box1[:, 3], box2[:, 3])\n",
    "    \n",
    "            inter_w = np.clip(x2 - x1, 0.0, None)\n",
    "            inter_h = np.clip(y2 - y1, 0.0, None)\n",
    "            inter = inter_w * inter_h\n",
    "    \n",
    "            area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "            area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "            union = area1 + area2 - inter + 1e-6\n",
    "            iou = inter / union\n",
    "            return iou.astype(\"float32\")\n",
    "\n",
    "        bbox_a = build_bbox(parts_a)\n",
    "        bbox_t = build_bbox(parts_t)\n",
    "        if bbox_a is not None and bbox_t is not None:\n",
    "            iou = iou_box(bbox_a, bbox_t)\n",
    "            iou_s = pd.Series(iou, index=idx, dtype=\"float32\")\n",
    "    \n",
    "            feats[\"as_body_iou\"] = iou_s\n",
    "    \n",
    "            ws_1s = self._scale(30)\n",
    "            mp_1s = max(ws_1s // 3, 1)\n",
    "            feats[\"as_body_iou_mean_1s\"] = (\n",
    "                iou_s.rolling(ws_1s, min_periods=mp_1s).mean().fillna(0.0).astype(\"float32\")\n",
    "            )\n",
    "        else:\n",
    "            feats[\"as_body_iou\"] = zero()\n",
    "            feats[\"as_body_iou_mean_1s\"] = zero()\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 6) DỌN NẠN NaN / Inf\n",
    "        # ---------------------------------------------------------\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "\n",
    "    def _feat_climb(self, ctx: AgentContext, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Feature chuyên cho hành vi climb trong arena hình chữ nhật (33 x 19 cm).\n",
    "    \n",
    "        Ý tưởng:\n",
    "          - Chuột đi gần tường: dist_wall giảm nhanh.\n",
    "          - Khi climb: sát tường (dist_wall nhỏ), v_normal ~ 0,\n",
    "            nhưng vẫn có v_tangent (bò ngang trên tường / di chuyển dọc biên).\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        idx = ctx.idx\n",
    "    \n",
    "        def zero() -> pd.Series:\n",
    "            return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 1. Arena size (cm) ---\n",
    "        # Nếu bạn đã set trong FeatureConfig thì dùng:\n",
    "        # W = self.cfg.arena_width_cm or 33.0\n",
    "        # H = self.cfg.arena_height_cm or 19.0\n",
    "        # Ở đây fix luôn cho lab này:\n",
    "        W = 33.0\n",
    "        H = 19.0\n",
    "        parts = self._extract_parts_dict(ctx, [\"head\"])\n",
    "        head = parts.get(\"head\")\n",
    "        \n",
    "        if head is not None:\n",
    "            # head đã ở đơn vị cm (vì _extract_part đã to_cm + smooth)\n",
    "            cx = pd.Series(head[:, 0], index=idx)\n",
    "            cy = pd.Series(head[:, 1], index=idx)\n",
    "        else:\n",
    "            # fallback: nếu không có head thì dùng body_center như cũ\n",
    "            cx = ctx.cx\n",
    "            cy = ctx.cy\n",
    "\n",
    "\n",
    "        # # --- 2. Khoảng cách tới 4 bức tường ---\n",
    "        # cx = ctx.cx  # Series\n",
    "        # cy = ctx.cy  # Series\n",
    "    \n",
    "        dist_left   = cx - 0.0\n",
    "        dist_right  = W - cx\n",
    "        dist_bottom = cy - 0.0\n",
    "        dist_top    = H - cy\n",
    "    \n",
    "        d_all = np.stack(\n",
    "            [dist_left.values, dist_right.values, dist_bottom.values, dist_top.values],\n",
    "            axis=1,  # [F, 4]\n",
    "        )\n",
    "    \n",
    "        dist_wall = np.min(d_all, axis=1)          # khoảng cách tới tường gần nhất\n",
    "        wall_idx  = np.argmin(d_all, axis=1)       # 0:left, 1:right, 2:bottom, 3:top\n",
    "    \n",
    "        dist_wall_s = pd.Series(dist_wall, index=idx, dtype=\"float32\")\n",
    "        feats[\"climb_dist_wall\"] = dist_wall_s\n",
    "    \n",
    "        # --- 3. Vận tốc theo NORMAL & TANGENT của tường gần nhất ---\n",
    "        vx = ctx.vel[:, 0]\n",
    "        vy = ctx.vel[:, 1]\n",
    "    \n",
    "        # normal hướng VÀO trong arena từ tường\n",
    "        nx = np.zeros_like(vx, dtype=\"float32\")\n",
    "        ny = np.zeros_like(vy, dtype=\"float32\")\n",
    "\n",
    "        # left  wall (x=0)    → normal = (+1, 0)\n",
    "        # right wall (x=W)    → normal = (-1, 0)\n",
    "        # bottom wall (y=0)   → normal = (0, +1)\n",
    "        # top wall (y=H)      → normal = (0, -1)\n",
    "        nx[wall_idx == 0] =  1.0\n",
    "        nx[wall_idx == 1] = -1.0\n",
    "        ny[wall_idx == 2] =  1.0\n",
    "        ny[wall_idx == 3] = -1.0\n",
    "    \n",
    "        # v_normal = v ⋅ n\n",
    "        v_normal = vx * nx + vy * ny\n",
    "    \n",
    "        # thành phần song song tường: v_tan = v - (v⋅n)n\n",
    "        v_proj_x = v_normal * nx\n",
    "        v_proj_y = v_normal * ny\n",
    "        v_tan_x = vx - v_proj_x\n",
    "        v_tan_y = vy - v_proj_y\n",
    "        v_tangent = np.sqrt(v_tan_x ** 2 + v_tan_y ** 2)\n",
    "    \n",
    "        v_normal_s  = pd.Series(v_normal,  index=idx, dtype=\"float32\")\n",
    "        v_tangent_s = pd.Series(v_tangent, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        feats[\"climb_normal_vel\"]  = v_normal_s\n",
    "        feats[\"climb_tangent_vel\"] = v_tangent_s\n",
    "    \n",
    "        # --- 4. Approach speed: dist_wall giảm mạnh (lao vào tường) ---\n",
    "        ws = self._scale(15)  # ~0.5s (15 frame ở 30fps)\n",
    "        min_p = max(ws // 3, 1)\n",
    "\n",
    "        # diff_dw > 0 khi dist_wall giảm (đi về phía tường)\n",
    "        diff_dw = -dist_wall_s.diff().fillna(0.0)  # dấu trừ để \"giảm\" → dương\n",
    "        approach = diff_dw.rolling(ws, min_periods=min_p).mean()\n",
    "        feats[\"climb_approach_speed_wall\"] = approach.astype(\"float32\")\n",
    "    \n",
    "        # --- 5. Stick score: sát tường + không còn lao vào (v_normal nhỏ) ---\n",
    "        # gần tường\n",
    "        thr_cm = 3.0  # tuỳ chỉnh (3cm sát tường)\n",
    "        near_wall = (dist_wall_s < thr_cm).astype(\"float32\")\n",
    "    \n",
    "        # ít lao vào nữa: |v_normal| nhỏ\n",
    "        stick = near_wall * (1.0 / (1.0 + v_normal_s.abs()))\n",
    "\n",
    "        # Nếu muốn climb thực sự có chút chuyển động dọc tường:\n",
    "        # yêu cầu v_tangent > một ngưỡng nhỏ (ví dụ 0.5 cm/s)\n",
    "        stick = stick * (v_tangent_s > 0.5).astype(\"float32\")\n",
    "    \n",
    "        feats[\"climb_wall_stick_score\"] = stick.astype(\"float32\")\n",
    "    \n",
    "        # --- 6. Clean NaN/Inf ---\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "\n",
    "\n",
    "    def _feat_pairwise(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng tương tác cặp đôi (Pairwise): Khoảng cách, Tốc độ tiếp cận.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        if target_ctx is None: \n",
    "            return feats\n",
    "\n",
    "        def zero(): return pd.Series(0.0, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        def dist_ab(pt_a, pt_b):\n",
    "            if pt_a is None or pt_b is None: return zero()\n",
    "            d = np.linalg.norm(pt_a - pt_b, axis=1)\n",
    "            return pd.Series(d, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        feats[\"rel_dist\"] = pd.Series(dist, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        # Khoảng cách\n",
    "        my_parts = self._extract_parts_dict(ctx, [\"head\"])\n",
    "        target_parts = self._extract_parts_dict(target_ctx, [\"head\", \"tail_base\", \"ear_left\", \"ear_right\"])\n",
    "\n",
    "        ah, th = my_parts[\"head\"], target_parts[\"head\"]\n",
    "        feats[\"dist_head_head\"] = dist_ab(ah, th)\n",
    "        feats[\"dist_head_tail\"] = dist_ab(ah, target_parts[\"tail_base\"])\n",
    "        feats[\"dist_head_el\"]   = dist_ab(ah, target_parts[\"ear_left\"])\n",
    "        feats[\"dist_head_er\"]   = dist_ab(ah, target_parts[\"ear_right\"])\n",
    "\n",
    "        #  Hướng - góc nhìn\n",
    "        def get_body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"head\")\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if head is not None and tail is not None:\n",
    "                return head - tail\n",
    "            return None\n",
    "\n",
    "        a_vec = get_body_vec(my_parts)\n",
    "        t_vec = get_body_vec(target_parts)\n",
    "\n",
    "        if a_vec is not None and t_vec is not None:\n",
    "            dot = np.sum(a_vec * t_vec, axis=1)\n",
    "            mags = np.linalg.norm(a_vec, axis=1) * np.linalg.norm(t_vec, axis=1)\n",
    "            feats[\"body_cosine\"] = pd.Series(\n",
    "                np.clip(dot / (mags + 1e-6), -1.0, 1.0), index=idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"body_cosine\"] = zero()\n",
    "\n",
    "        # Vector ánh nhìn = Target_Pos - My_Pos = rel_vec\n",
    "        if a_vec is not None:\n",
    "            dot_gaze = np.sum(a_vec * rel_vec, axis=1)\n",
    "            mag_a = np.linalg.norm(a_vec, axis=1)\n",
    "            feats[\"gaze_cosine\"] = pd.Series(\n",
    "                np.clip(dot_gaze / (mag_a * dist + 1e-6), -1.0, 1.0),\n",
    "                index=ctx.idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"gaze_cosine\"] = zero()\n",
    "\n",
    "        # Vector đơn vị hướng về địch (u)\n",
    "        dist_safe = dist.copy()\n",
    "        dist_safe[dist_safe == 0] = 1e-6\n",
    "        u_vec = rel_vec / dist_safe[:, None]\n",
    "\n",
    "        # a_vel và t_vel lấy từ Context\n",
    "        a_vel, t_vel = ctx.vel, target_ctx.vel\n",
    "\n",
    "        # A. Approach Speed (Vận tốc dọc trục nối 2 con)\n",
    "        # Dương: Lao vào nhau | Âm: Chạy ra xa nhau\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1) # Target hướng ngược lại\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)\n",
    "\n",
    "        # B. Lateral Speed (Vận tốc ngang - Vuông góc trục nối)\n",
    "        # Vector chiếu: v_proj = (v . u) * u\n",
    "        a_proj = a_along[:, None] * u_vec\n",
    "        a_lat_vec = a_vel - a_proj\n",
    "        a_lat_speed = np.linalg.norm(a_lat_vec, axis=1)\n",
    "\n",
    "        feats[\"approach_speed_agent\"]  = pd.Series(a_along, index=ctx.idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_target\"] = pd.Series(t_along, index=ctx.idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_rel\"]    = pd.Series(rel_along, index=ctx.idx, dtype=\"float32\")\n",
    "        feats[\"lateral_speed_agent\"]   = pd.Series(a_lat_speed, index=ctx.idx, dtype=\"float32\")\n",
    "        return feats\n",
    "\n",
    "    def _feat_follow_pattern(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng hành vi FOLLOW:\n",
    "          - Agent ở gần target\n",
    "          - Cùng hướng (body + velocity)\n",
    "          - Tốc độ vừa phải\n",
    "          - Khoảng cách tương đối ổn định trong 0.5–1s\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 1. CÁC ĐẠI LƯỢNG CƠ BẢN ---\n",
    "        # Vector Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # Speed agent/target\n",
    "        a_speed = ctx.speed_series.astype(\"float32\")\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "    \n",
    "        # Body vector: nose - tail/body_center\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"head\", \"tail_base\", \"ear_left\", \"ear_right\"])\n",
    "        parts_t = self._extract_parts_dict(target_ctx, [\"head\", \"tail_base\", \"ear_right\", \"ear_left\"])\n",
    "    \n",
    "        def body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"head\")\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if head is None or tail is None:\n",
    "                return None\n",
    "            return head - tail\n",
    "    \n",
    "        a_body = body_vec(parts_a)\n",
    "        t_body = body_vec(parts_t)\n",
    "    \n",
    "        if a_body is not None and t_body is not None:\n",
    "            dot_bt = np.sum(a_body * t_body, axis=1)\n",
    "            mag_bt = np.linalg.norm(a_body, axis=1) * np.linalg.norm(t_body, axis=1)\n",
    "            cos_body = np.clip(dot_bt / (mag_bt + 1e-6), -1.0, 1.0)\n",
    "            cos_body_s = pd.Series(cos_body, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            cos_body_s = zero()\n",
    "    \n",
    "        # Velocity hướng\n",
    "        a_vel = ctx.vel\n",
    "        t_vel = target_ctx.vel\n",
    "        a_speed_np = np.linalg.norm(a_vel, axis=1)\n",
    "        t_speed_np = np.linalg.norm(t_vel, axis=1)\n",
    "        moving_mask = (a_speed_np > 1e-3) & (t_speed_np > 1e-3)\n",
    "    \n",
    "        # cos giữa hướng velocity 2 con\n",
    "        dot_v = np.sum(a_vel * t_vel, axis=1)\n",
    "        mag_v = a_speed_np * t_speed_np + 1e-6\n",
    "        cos_vel = np.zeros_like(dot_v, dtype=\"float32\")\n",
    "        cos_vel[moving_mask] = np.clip(dot_v[moving_mask] / mag_v[moving_mask], -1.0, 1.0)\n",
    "        cos_vel_s = pd.Series(cos_vel, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 2. WINDOW NGẮN (FOLLOW LÀ PATTERN DÀI HƠN ATTACK) ---\n",
    "        for w30 in [15, 30, 60]:   # ~0.5s, 1s, 2s\n",
    "            ws = self._scale(w30)\n",
    "            min_p = max(ws // 3, 1)\n",
    "    \n",
    "            # Khoảng cách trung bình & độ dao động\n",
    "            m_dist = rel_dist_s.rolling(ws, min_periods=min_p).mean()\n",
    "            s_dist = rel_dist_s.rolling(ws, min_periods=min_p).std()\n",
    "    \n",
    "            # Cùng hướng (body + velocity)\n",
    "            m_cos_body = cos_body_s.rolling(ws, min_periods=min_p).mean()\n",
    "            m_cos_vel  = cos_vel_s.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            # Tốc độ vừa phải\n",
    "            m_sp_a = a_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            m_sp_t = t_speed.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            feats[f\"follow_dist_mean_{w30}\"] = m_dist\n",
    "            feats[f\"follow_dist_std_{w30}\"]  = s_dist\n",
    "            feats[f\"follow_cos_body_mean_{w30}\"] = m_cos_body\n",
    "            feats[f\"follow_cos_vel_mean_{w30}\"]  = m_cos_vel\n",
    "            feats[f\"follow_speed_agent_mean_{w30}\"] = m_sp_a\n",
    "            feats[f\"follow_speed_target_mean_{w30}\"] = m_sp_t\n",
    "    \n",
    "        # Clean\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "    \n",
    "    def _feat_shortburst_social(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        feats = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- Lấy lại vài quantity cơ bản từ pairwise/avoidance ---\n",
    "        # vector Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # unit vector\n",
    "        rel_dist_safe = np.where(rel_dist == 0, 1e-6, rel_dist)\n",
    "        u_vec = rel_vec / rel_dist_safe[:, None]\n",
    "    \n",
    "        # velocity dọc trục nối (approach speed)\n",
    "        a_vel = ctx.vel\n",
    "        t_vel = target_ctx.vel\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)                # +: lao vào target\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1)             # +: target lao vào agent\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)    # +: lại gần nhau\n",
    "    \n",
    "        a_along_s = pd.Series(a_along, index=idx, dtype=\"float32\")\n",
    "        t_along_s = pd.Series(t_along, index=idx, dtype=\"float32\")\n",
    "        rel_along_s = pd.Series(rel_along, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # speed agent / target\n",
    "        a_speed = ctx.speed_series\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\"\n",
    "        )\n",
    "    \n",
    "        # heading_rel_cos ~ escape / approach\n",
    "        # vector body của agent\n",
    "        # (reuse idea từ _feat_pairwise)\n",
    "        # head ~ nose, tail ~ tail_base/body_center\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"head\", \"tail_base\"])\n",
    "        head_a = parts_a.get(\"head\")\n",
    "        tail_a = parts_a.get(\"tail_base\")\n",
    "    \n",
    "        if head_a is not None and tail_a is not None:\n",
    "            body_vec_a = head_a - tail_a\n",
    "            dot = np.sum(body_vec_a * rel_vec, axis=1)\n",
    "            mag = np.linalg.norm(body_vec_a, axis=1) * rel_dist_safe\n",
    "            heading_cos = np.clip(dot / (mag + 1e-6), -1.0, 1.0)\n",
    "            heading_cos_s = pd.Series(heading_cos, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            heading_cos_s = zero()\n",
    "    \n",
    "        # --- Rolling window 10, 20, 30 frames (ở fps gốc) ---\n",
    "        for w30 in [10, 20, 30]:\n",
    "            ws = self._scale(w30)\n",
    "            min_p = max(1, ws // 3)\n",
    "    \n",
    "            # Attack-like: approach mạnh, khoảng cách giảm nhanh\n",
    "            feats[f\"sb_att_approach_mean_{w30}\"] = a_along_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_att_rel_along_mean_{w30}\"] = rel_along_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_att_dist_delta_{w30}\"] = (rel_dist_s - rel_dist_s.shift(ws)).fillna(0.0)\n",
    "    \n",
    "            # Chase-like: agent & target đều nhanh, dist tương đối nhỏ\n",
    "            feats[f\"sb_chase_speed_agent_mean_{w30}\"] = a_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_chase_speed_target_mean_{w30}\"] = t_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_chase_dist_mean_{w30}\"] = rel_dist_s.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            # Escape-like: heading ngược, dist tăng nhanh\n",
    "            feats[f\"sb_esc_heading_cos_mean_{w30}\"] = heading_cos_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_esc_dist_gain_{w30}\"] = (rel_dist_s.shift(-ws) - rel_dist_s).fillna(0.0)\n",
    "    \n",
    "        # clip & fillna\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = v.replace([np.inf, -np.inf], np.nan).fillna(0.0).astype(\"float32\")\n",
    "    \n",
    "        return feats\n",
    "\n",
    "\n",
    "\n",
    "    # --- Methods tương thích ---\n",
    "    \n",
    "    def build_pose_tensor(self, tracking: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Chuyển dữ liệu tracking (DataFrame) sang Tensor [Frames, Mice, 2] và Dict chi tiết.\n",
    "        \"\"\"\n",
    "        tracking = tracking.sort_values(\"video_frame\")\n",
    "        frames = np.sort(tracking[\"video_frame\"].unique())\n",
    "        \n",
    "        pvid = tracking.pivot(\n",
    "            index=\"video_frame\", \n",
    "            columns=[\"mouse_id\", \"bodypart\"], \n",
    "            values=[\"x\", \"y\"]\n",
    "        )\n",
    "        pvid = pvid.reorder_levels([1, 2, 0], axis=1).sort_index(axis=1).astype(\"float32\")\n",
    "        mouse_ids = list(pvid.columns.get_level_values(0).unique())\n",
    "        pos = np.full((len(frames), len(mouse_ids), 2), np.nan, dtype=np.float32)\n",
    "        per_mouse_df = {}\n",
    "        \n",
    "        for i, mid in enumerate(mouse_ids):\n",
    "            single = pvid[mid]\n",
    "            per_mouse_df[mid] = single\n",
    "            \n",
    "            if \"body_center\" in single.columns.get_level_values(0):\n",
    "                cx = single[\"body_center\"][\"x\"]\n",
    "                cy = single[\"body_center\"][\"y\"]\n",
    "            else:\n",
    "                cx = single.xs(\"x\", level=1, axis=1).mean(axis=1)\n",
    "                cy = single.xs(\"y\", level=1, axis=1).mean(axis=1)\n",
    "            \n",
    "            pos[:, i, 0] = cx.reindex(frames).values\n",
    "            pos[:, i, 1] = cy.reindex(frames).values\n",
    "            \n",
    "        return frames, mouse_ids, pos, per_mouse_df\n",
    "\n",
    "    def extract_agent_target(\n",
    "        self, \n",
    "        frames: np.ndarray, \n",
    "        mouse_ids: List[Any], \n",
    "        pos: np.ndarray, \n",
    "        agent_id: Any, \n",
    "        target_id: Any, \n",
    "        per_mouse_df: Dict = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Trích xuất đặc trưng cho cặp (Agent, Target).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            aid_idx = mouse_ids.index(agent_id)\n",
    "        except ValueError:\n",
    "            return pd.DataFrame() \n",
    "\n",
    "        # 1. Build Agent Context\n",
    "        ctx_agent = self._build_context(\n",
    "            frames, \n",
    "            pos[:, aid_idx, :], \n",
    "            per_mouse_df.get(agent_id) if per_mouse_df else None\n",
    "        )\n",
    "\n",
    "        # 2. Build Target Context\n",
    "        ctx_target = None\n",
    "        if self.cfg.use_pairwise and target_id is not None and target_id in mouse_ids:\n",
    "             tid_idx = mouse_ids.index(target_id)\n",
    "             ctx_target = self._build_context(\n",
    "                 frames, \n",
    "                 pos[:, tid_idx, :], \n",
    "                 per_mouse_df.get(target_id) if per_mouse_df else None\n",
    "             )\n",
    "\n",
    "        # 3. Run all features\n",
    "        all_data = {}\n",
    "        for func_name, func in self.feature_registry.items():\n",
    "            out_dict = func(ctx_agent, target_ctx=ctx_target)\n",
    "            all_data.update(out_dict)\n",
    "\n",
    "        df_out = pd.DataFrame(all_data, index=ctx_agent.idx)\n",
    "        df_out = df_out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        \n",
    "        return df_out.reindex(sorted(df_out.columns), axis=1)\n",
    "\n",
    "# ===========================================================================================\n",
    "# ===========================================================================================\n",
    "# ===========================================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# (Trên Kaggle) dùng metric chính thức\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/usr/lib/mabe-f-beta\")\n",
    "from metric import score   # hàm score(submission_df, dataset_df)\n",
    "\n",
    "# =========================================================\n",
    "# 1. ĐƯỜNG DẪN & CẤU HÌNH\n",
    "# =========================================================\n",
    "\n",
    "INPUT_DIR = Path(\"/kaggle/input/MABe-mouse-behavior-detection\")\n",
    "TRAIN_TRACKING_DIR = INPUT_DIR / \"train_tracking\"\n",
    "TRAIN_ANNOTATION_DIR = INPUT_DIR / \"train_annotation\"\n",
    "TEST_TRACKING_DIR = INPUT_DIR / \"test_tracking\"\n",
    "\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "RESULTS_DIR = Path(r\"/kaggle/input/results-xgb-fe\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "INDEX_COLS = [\"video_id\", \"agent_id\", \"target_id\", \"video_frame\"]\n",
    "\n",
    "# hành vi “self” vs “pair” giống notebook (có thể chỉnh nếu muốn)\n",
    "SELF_BEHAVIORS = [\n",
    "    \"biteobject\", \"climb\", \"dig\", \"exploreobject\", \"freeze\",\n",
    "    \"genitalgroom\", \"huddle\", \"rear\", \"rest\", \"run\", \"selfgroom\",\n",
    "]\n",
    "PAIR_BEHAVIORS = [\n",
    "    \"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\",\n",
    "    \"chase\", \"chaseattack\", \"defend\", \"disengage\", \"dominance\",\n",
    "    \"dominancegroom\", \"dominancemount\", \"ejaculate\", \"escape\",\n",
    "    \"flinch\", \"follow\", \"intromit\", \"mount\", \"reciprocalsniff\",\n",
    "    \"shepherd\", \"sniff\", \"sniffbody\", \"sniffface\", \"sniffgenital\",\n",
    "    \"submit\", \"tussle\",\n",
    "]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. ĐỌC METADATA & HELPER\n",
    "# =========================================================\n",
    "\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    train_meta = pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "    return train_meta\n",
    "\n",
    "\n",
    "def get_video_params(video_id: Any, meta: pd.DataFrame) -> Tuple[float, float]:\n",
    "    \"\"\"Lấy fps, pix_per_cm cho video từ train.csv.\"\"\"\n",
    "    row = meta.loc[meta[\"video_id\"] == video_id]\n",
    "    if row.empty:\n",
    "        raise KeyError(f\"video_id={video_id} không có trong train.csv\")\n",
    "    row = row.iloc[0]\n",
    "\n",
    "    # giống notebook: cột \"frames per second\" & \"pix per cm (approx)\"\n",
    "    fps = float(row[\"frames_per_second\"])\n",
    "    pix_per_cm = float(row[\"pix_per_cm_approx\"])\n",
    "    if not np.isfinite(pix_per_cm) or pix_per_cm <= 0:\n",
    "        pix_per_cm = 1.0\n",
    "    return fps, pix_per_cm\n",
    "\n",
    "\n",
    "def load_tracking(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    \"\"\"Đọc tracking parquet → pandas (schema: video_frame, mouse_id, bodypart, x, y).\"\"\"\n",
    "    path = TRAIN_TRACKING_DIR / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    df = pd.read_parquet(path)\n",
    "    return df\n",
    "\n",
    "def load_tracking_test(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    \"\"\"Đọc tracking parquet của test → pandas.\"\"\"\n",
    "    path = INPUT_DIR / \"test_tracking\" / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "def load_annotation(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    \"\"\"Đọc annotation (agent_id, target_id, action, start_frame, stop_frame).\"\"\"\n",
    "    path = TRAIN_ANNOTATION_DIR / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists():\n",
    "        # không có label cho video này\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "        )\n",
    "    ann = pd.read_parquet(path)\n",
    "    return ann[[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. TÍNH FEATURE PER-FRAME BẰNG FEATUREEXTRACTOR\n",
    "# =========================================================\n",
    "\n",
    "# Cache: (lab, video, agent, target) -> (frames, feature_df)\n",
    "_feature_cache: Dict[Tuple[str, int, int, int], Tuple[np.ndarray, pd.DataFrame]] = {}\n",
    "\n",
    "\n",
    "def get_frame_features_for_pair(\n",
    "    lab_id: str,\n",
    "    video_id: int,\n",
    "    agent_id: int,\n",
    "    target_id: int,\n",
    "    meta: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Tính (hoặc lấy cache) feature per-frame cho 1 video + (agent, target).\n",
    "    Trả về: frames [F], features_df [F, D]\n",
    "    \"\"\"\n",
    "    key = (str(lab_id), int(video_id), int(agent_id), int(target_id))\n",
    "    if key in _feature_cache:\n",
    "        return _feature_cache[key]\n",
    "\n",
    "    fps, pix_per_cm = get_video_params(video_id, meta)\n",
    "    tracking = load_tracking(lab_id, video_id)\n",
    "\n",
    "    fe = FeatureExtractor(\n",
    "        fps=fps,\n",
    "        pix_per_cm=pix_per_cm,\n",
    "        smooth_sigma=1.0,\n",
    "        use_pairwise=True,\n",
    "    )\n",
    "\n",
    "    frames, mouse_ids, pos, per_mouse_df = fe.build_pose_tensor(tracking)\n",
    "\n",
    "    # agent/target có thể là cùng chuột (self) hoặc khác chuột (pair)\n",
    "    features_df: pd.DataFrame = fe.extract_agent_target(\n",
    "        frames=frames,\n",
    "        mouse_ids=mouse_ids,\n",
    "        pos=pos,\n",
    "        agent_id=agent_id,\n",
    "        target_id=target_id,\n",
    "        per_mouse_df=per_mouse_df,\n",
    "    )\n",
    "    # index chính là frame\n",
    "    features_df.index = frames\n",
    "\n",
    "    _feature_cache[key] = (frames, features_df)\n",
    "    return frames, features_df\n",
    "\n",
    "_feature_cache: Dict[Tuple[str, int, Any, Any], Tuple[np.ndarray, pd.DataFrame]] = {}\n",
    "\n",
    "def get_frame_features_for_pair_test(\n",
    "    lab_id: str,\n",
    "    video_id: int,\n",
    "    agent_id: Any,\n",
    "    target_id: Any,\n",
    "    test_meta: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Feature per-frame cho test (video_id, agent, target).\n",
    "    Trả về: frames [F], features_df [F, D]\n",
    "    \"\"\"\n",
    "    key = (f\"test_{lab_id}\", int(video_id), agent_id, target_id)\n",
    "    if key in _feature_cache:\n",
    "        return _feature_cache[key]\n",
    "\n",
    "    # Lấy fps, pix_per_cm_approx từ test.csv\n",
    "    row = test_meta[test_meta[\"video_id\"] == video_id].iloc[0]\n",
    "    fps = float(row[\"frames_per_second\"])\n",
    "    pix_per_cm = float(row[\"pix_per_cm_approx\"])\n",
    "    if not np.isfinite(pix_per_cm) or pix_per_cm <= 0:\n",
    "        pix_per_cm = 1.0\n",
    "\n",
    "    tracking = load_tracking_test(lab_id, video_id)\n",
    "\n",
    "    fe = FeatureExtractor(\n",
    "        fps=fps,\n",
    "        pix_per_cm=pix_per_cm,\n",
    "        smooth_sigma=1.0,\n",
    "        use_pairwise=True,\n",
    "    )\n",
    "\n",
    "    frames, mouse_ids, pos, per_mouse_df = fe.build_pose_tensor(tracking)\n",
    "\n",
    "    features_df = fe.extract_agent_target(\n",
    "        frames=frames,\n",
    "        mouse_ids=mouse_ids,\n",
    "        pos=pos,\n",
    "        agent_id=agent_id,\n",
    "        target_id=target_id,\n",
    "        per_mouse_df=per_mouse_df,\n",
    "    )\n",
    "    features_df.index = frames\n",
    "\n",
    "    _feature_cache[key] = (frames, features_df)\n",
    "    return frames, features_df\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. BUILD FRAME-LEVEL DATASET CHO 1 (lab_id, behavior)\n",
    "# =========================================================\n",
    "\n",
    "def build_frame_dataset_for_lab_behavior(\n",
    "    lab_id: str,\n",
    "    behavior: str,\n",
    "    train_meta: pd.DataFrame,\n",
    "    mode: str = \"self\",\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Xây tập frame-level (indices, features, labels) cho 1 (lab, behavior).\n",
    "\n",
    "    indices: DataFrame với cột INDEX_COLS\n",
    "    features: DataFrame per-frame features\n",
    "    labels: np.ndarray nhị phân (0/1)\n",
    "    \"\"\"\n",
    "\n",
    "    videos = (\n",
    "        train_meta[train_meta[\"lab_id\"] == lab_id][\"video_id\"]\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    index_list = []\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for video_id in videos:\n",
    "        ann = load_annotation(lab_id, video_id)\n",
    "        if ann.empty:\n",
    "            continue\n",
    "\n",
    "        # chỉ lấy annotation của behavior này\n",
    "        ann_bhv = ann[ann[\"action\"] == behavior]\n",
    "        if ann_bhv.empty:\n",
    "            continue\n",
    "\n",
    "        # các (agent, target) cần xem\n",
    "        pairs = ann_bhv[[\"agent_id\", \"target_id\"]].drop_duplicates().values.tolist()\n",
    "        for (agent_id, target_id) in pairs:\n",
    "            if mode == \"self\":\n",
    "                target_id_use = agent_id\n",
    "            else:\n",
    "                target_id_use = target_id\n",
    "\n",
    "            frames, feat_df = get_frame_features_for_pair(\n",
    "                lab_id=lab_id,\n",
    "                video_id=video_id,\n",
    "                agent_id=agent_id,\n",
    "                target_id=target_id_use,\n",
    "                meta=train_meta,\n",
    "            )\n",
    "\n",
    "            # label per-frame: frame ∈ bất kỳ [start, stop) của (agent,target,behavior)\n",
    "            ann_pair = ann_bhv[\n",
    "                (ann_bhv[\"agent_id\"] == agent_id)\n",
    "                & (ann_bhv[\"target_id\"] == target_id)\n",
    "            ]\n",
    "            if ann_pair.empty and mode == \"self\":\n",
    "                ann_pair = ann_bhv[ann_bhv[\"agent_id\"] == agent_id]\n",
    "\n",
    "            pos_frames = set()\n",
    "            for _, r in ann_pair.iterrows():\n",
    "                pos_frames.update(range(int(r[\"start_frame\"]), int(r[\"stop_frame\"])))\n",
    "\n",
    "            if len(pos_frames) == 0:\n",
    "                continue\n",
    "\n",
    "            label = np.isin(frames, list(pos_frames)).astype(\"int8\")\n",
    "            if label.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            idx_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"video_id\": video_id,\n",
    "                    \"agent_id\": agent_id,\n",
    "                    \"target_id\": target_id,\n",
    "                    \"video_frame\": frames,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            index_list.append(idx_df)\n",
    "            feature_list.append(feat_df.reset_index(drop=True))\n",
    "            label_list.append(label)\n",
    "\n",
    "    if not index_list:\n",
    "        return (\n",
    "            pd.DataFrame(columns=INDEX_COLS),\n",
    "            pd.DataFrame(),\n",
    "            np.zeros(0, dtype=\"int8\"),\n",
    "        )\n",
    "\n",
    "    indices = pd.concat(index_list, ignore_index=True)\n",
    "    features = pd.concat(feature_list, ignore_index=True)\n",
    "    labels = np.concatenate(label_list).astype(\"int8\")\n",
    "\n",
    "    assert len(indices) == len(features) == len(labels)\n",
    "\n",
    "    return indices, features, labels\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. TRAIN + OOF CHO 1 (lab_id, behavior)\n",
    "# =========================================================\n",
    "\n",
    "def tune_threshold(oof_pred: np.ndarray, y: np.ndarray) -> float:\n",
    "    ths = np.arange(0.0, 1.005, 0.005)\n",
    "    scores = [f1_score(y, (oof_pred >= th), zero_division=0) for th in ths]\n",
    "    return float(ths[int(np.argmax(scores))])\n",
    "\n",
    "#\n",
    "def train_validate_one(\n",
    "    lab_id: str,\n",
    "    behavior: str,\n",
    "    indices: pd.DataFrame,\n",
    "    features: pd.DataFrame,\n",
    "    labels: np.ndarray,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Train XGBoost binary cho 1 (lab, behavior) + lưu OOF prediction.\n",
    "    Trả về: F1 trên toàn bộ OOF (frame-level).\n",
    "    \"\"\"\n",
    "    result_dir = RESULTS_DIR / lab_id / behavior\n",
    "    result_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    n = len(labels)\n",
    "\n",
    "    if n == 0 or labels.sum() == 0:\n",
    "        oof_df = indices.copy()\n",
    "        oof_df[\"fold\"] = -1\n",
    "        oof_df[\"prediction\"] = 0.0\n",
    "        oof_df[\"predicted_label\"] = 0\n",
    "        oof_df.to_parquet(result_dir / \"oof_predictions.parquet\", index=False)\n",
    "        (result_dir / \"f1.txt\").write_text(\"0.0\\n\")\n",
    "        return 0.0\n",
    "\n",
    "    X = features.values.astype(\"float32\")\n",
    "    y = labels.astype(\"int8\")\n",
    "    groups = indices[\"video_id\"].values\n",
    "\n",
    "    folds = np.ones(n, dtype=\"int8\") * -1\n",
    "    oof_pred = np.zeros(n, dtype=\"float32\")\n",
    "    oof_label = np.zeros(n, dtype=\"int8\")\n",
    "\n",
    "    cv = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups=groups)):\n",
    "        fold_dir = result_dir / f\"fold_{fold}\"\n",
    "        fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "        X_va, y_va = X[va_idx], y[va_idx]\n",
    "\n",
    "        # scale_pos_weight\n",
    "        pos = y_tr.sum()\n",
    "        neg = len(y_tr) - pos\n",
    "        scale_pos_weight = float(neg / pos) if pos > 0 else 1.0\n",
    "\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"logloss\",\n",
    "            \"device\": \"cuda\",\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"max_depth\": 6,\n",
    "            \"min_child_weight\": 5,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"scale_pos_weight\": scale_pos_weight,\n",
    "            \"max_bin\": 64,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "\n",
    "        dtrain = xgb.QuantileDMatrix(\n",
    "            X_tr,\n",
    "            label=y_tr,\n",
    "            feature_names=features.columns.tolist(),\n",
    "            max_bin=64,\n",
    "        )\n",
    "        dvalid = xgb.DMatrix(\n",
    "            X_va,\n",
    "            label=y_va,\n",
    "            feature_names=features.columns.tolist(),\n",
    "        )\n",
    "\n",
    "        evals_result: Dict[str, Dict[str, List[float]]] = {}\n",
    "\n",
    "        early_stop = xgb.callback.EarlyStopping(\n",
    "            rounds=10, metric_name=\"logloss\", data_name=\"valid\", maximize=False\n",
    "        )\n",
    "\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=250,\n",
    "            evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "            callbacks=[early_stop],\n",
    "            evals_result=evals_result,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "\n",
    "        pred_va = model.predict(dvalid)\n",
    "        th = tune_threshold(pred_va, y_va)\n",
    "\n",
    "        folds[va_idx] = fold\n",
    "        oof_pred[va_idx] = pred_va\n",
    "        oof_label[va_idx] = (pred_va >= th).astype(\"int8\")\n",
    "\n",
    "        model.save_model(fold_dir / \"model.json\")\n",
    "        with open(fold_dir / \"threshold.txt\", \"w\") as f:\n",
    "            f.write(f\"{th}\\n\")\n",
    "\n",
    "    # lưu OOF\n",
    "    oof_df = indices.copy()\n",
    "    oof_df[\"fold\"] = folds\n",
    "    oof_df[\"prediction\"] = oof_pred\n",
    "    oof_df[\"predicted_label\"] = oof_label\n",
    "    oof_df.to_parquet(result_dir / \"oof_predictions.parquet\", index=False)\n",
    "\n",
    "    f1 = f1_score(y, oof_label, zero_division=0)\n",
    "    (result_dir / \"f1.txt\").write_text(f\"{f1:.6f}\\n\")\n",
    "    return float(f1)\n",
    "\n",
    "def load_models_for_behavior_infer(lab_id: str, behavior: str):\n",
    "    \"\"\"\n",
    "    Đọc các fold model + threshold cho (lab, behavior) từ RESULTS_DIR.\n",
    "    Dùng cho inference (test).\n",
    "    \"\"\"\n",
    "    base_dir = RESULTS_DIR / lab_id / behavior\n",
    "    if not base_dir.exists():\n",
    "        return []\n",
    "\n",
    "    models = []\n",
    "    for fold_dir in sorted(base_dir.glob(\"fold_*\")):\n",
    "        model_file = fold_dir / \"model.json\"\n",
    "        thr_file = fold_dir / \"threshold.txt\"\n",
    "        if not model_file.exists():\n",
    "            continue\n",
    "\n",
    "        booster = xgb.Booster()\n",
    "        booster.load_model(str(model_file))\n",
    "\n",
    "        if thr_file.exists():\n",
    "            thr = float(thr_file.read_text().strip())\n",
    "        else:\n",
    "            thr = 0.5\n",
    "\n",
    "        models.append((booster, thr))\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. LOOP QUA TẤT CẢ BEHAVIORS TRONG 1 LAB\n",
    "#    (train_all_labs_behaviors vẫn giữ nguyên, nhưng main\n",
    "#     sẽ filter train_meta chỉ còn 1 lab)\n",
    "# =========================================================\n",
    "\n",
    "def train_all_labs_behaviors(train_meta: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Loop qua từng lab trong train_meta (ở đây main đã filter chỉ còn 1 lab):\n",
    "      - đọc annotation của tất cả video\n",
    "      - lấy unique action xuất hiện trong lab đó\n",
    "      - train 1 model/frame-level cho từng (lab, action)\n",
    "    \"\"\"\n",
    "    labs = train_meta[\"lab_id\"].unique().tolist()\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    for lab_id in labs:\n",
    "        # tập video của lab này\n",
    "        videos = train_meta[train_meta[\"lab_id\"] == lab_id][\"video_id\"].unique().tolist()\n",
    "\n",
    "        # gom toàn bộ action thực sự có trong annotation của lab này\n",
    "        behaviors_set = set()\n",
    "        for vid in videos:\n",
    "            ann = load_annotation(lab_id, vid)\n",
    "            if ann.empty:\n",
    "                continue\n",
    "            behaviors_set.update(ann[\"action\"].unique().tolist())\n",
    "\n",
    "        behaviors = sorted(behaviors_set)\n",
    "        print(f\"\\n===== LAB {lab_id}: {len(behaviors)} behaviors =====\")\n",
    "\n",
    "        for behavior in behaviors:\n",
    "            # if behavior != \"submit\": continue\n",
    "\n",
    "            mode = \"self\" if behavior in SELF_BEHAVIORS else \"pair\"\n",
    "\n",
    "            print(f\"\\n=== LAB={lab_id} | behavior={behavior} | mode={mode} ===\")\n",
    "            indices, features, labels = build_frame_dataset_for_lab_behavior(\n",
    "                lab_id=str(lab_id),\n",
    "                behavior=behavior,\n",
    "                train_meta=train_meta,\n",
    "                mode=mode,\n",
    "            )\n",
    "            print(\n",
    "                f\"frames: {len(labels):,}, positives: {labels.sum():,}, features: \"\n",
    "                f\"{features.shape[1] if not features.empty else 0}\"\n",
    "            )\n",
    "\n",
    "            if len(labels) == 0:\n",
    "                print(\" -> skip (no samples)\")\n",
    "                continue\n",
    "\n",
    "            f1 = train_validate_one(str(lab_id), behavior, indices, features, labels)\n",
    "            elapsed = time.perf_counter() - start_time\n",
    "            print(f\" -> OOF F1 (frame-level): {f1:.3f} | elapsed={elapsed/60:.1f} min\")\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. GOM OOF PREDICTION → SEGMENT & TÍNH SCORE()\n",
    "# =========================================================\n",
    "\n",
    "def build_oof_submission_from_parquet(\n",
    "    target_lab_id: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Đọc tất cả oof_predictions.parquet trong RESULTS_DIR,\n",
    "    gom thành frame-level table rồi nối thành segment-level prediction\n",
    "    giống inference notebook (simplified).\n",
    "\n",
    "    Nếu target_lab_id != None thì chỉ lấy OOF của lab đó\n",
    "    (vd \"AdaptableSnail\").\n",
    "    \"\"\"\n",
    "    oof_files = list(RESULTS_DIR.glob(\"*/**/oof_predictions.parquet\"))\n",
    "    if not oof_files:\n",
    "        raise RuntimeError(\"Không tìm thấy OOF parquet, hãy train trước.\")\n",
    "\n",
    "    frame_preds = []\n",
    "\n",
    "    for path in oof_files:\n",
    "        # path: results_xgb_fe/lab/behavior/oof_predictions.parquet\n",
    "        parts = path.parts\n",
    "        behavior = parts[-2]\n",
    "        lab_id = parts[-3]\n",
    "\n",
    "        # chỉ lấy file thuộc lab mong muốn (nếu có)\n",
    "        if target_lab_id is not None and lab_id != target_lab_id:\n",
    "            continue\n",
    "\n",
    "        df = pd.read_parquet(path)\n",
    "        df = df[INDEX_COLS + [\"prediction\"]].copy()\n",
    "        df[\"lab_id\"] = lab_id\n",
    "        df[\"action\"] = behavior\n",
    "        frame_preds.append(df)\n",
    "\n",
    "    if not frame_preds:\n",
    "        raise RuntimeError(\n",
    "            f\"Không có OOF predictions nào cho lab_id={target_lab_id}\"\n",
    "        )\n",
    "\n",
    "    frame_df = pd.concat(frame_preds, ignore_index=True)\n",
    "\n",
    "    # sắp xếp\n",
    "    frame_df = frame_df.sort_values(\n",
    "        [\"lab_id\", \"video_id\", \"agent_id\", \"target_id\", \"action\", \"video_frame\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Convert frame-level prob -> hard label + segments\n",
    "    segments = []\n",
    "    for (lab_id, video_id, agent_id, target_id, action), group in frame_df.groupby(\n",
    "        [\"lab_id\", \"video_id\", \"agent_id\", \"target_id\", \"action\"], sort=False\n",
    "    ):\n",
    "        frames = group[\"video_frame\"].values\n",
    "        scores = group[\"prediction\"].values\n",
    "\n",
    "        # dùng một threshold fix (vd 0.5) cho demo\n",
    "        # (hoặc bạn có thể lưu threshold per (lab,behavior) và apply)\n",
    "        hard = scores >= 0.5\n",
    "\n",
    "        in_seg = False\n",
    "        start = None\n",
    "        prev_f = None\n",
    "\n",
    "        for f, h in zip(frames, hard):\n",
    "            if h and not in_seg:\n",
    "                in_seg = True\n",
    "                start = int(f)\n",
    "            elif (not h) and in_seg:\n",
    "                stop = int(prev_f + 1)  # [start, stop)\n",
    "                segments.append(\n",
    "                    {\n",
    "                        \"lab_id\": lab_id,\n",
    "                        \"video_id\": int(video_id),\n",
    "                        \"agent_id\": int(agent_id),\n",
    "                        \"target_id\": int(target_id),\n",
    "                        \"action\": action,\n",
    "                        \"start_frame\": start,\n",
    "                        \"stop_frame\": stop,\n",
    "                    }\n",
    "                )\n",
    "                in_seg = False\n",
    "            prev_f = f\n",
    "\n",
    "        if in_seg:\n",
    "            stop = int(frames[-1] + 1)\n",
    "            segments.append(\n",
    "                {\n",
    "                    \"lab_id\": lab_id,\n",
    "                    \"video_id\": int(video_id),\n",
    "                    \"agent_id\": int(agent_id),\n",
    "                    \"target_id\": int(target_id),\n",
    "                    \"action\": action,\n",
    "                    \"start_frame\": start,\n",
    "                    \"stop_frame\": stop,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not segments:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"lab_id\",\n",
    "                \"video_id\",\n",
    "                \"agent_id\",\n",
    "                \"target_id\",\n",
    "                \"action\",\n",
    "                \"start_frame\",\n",
    "                \"stop_frame\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    submission = pd.DataFrame(segments)\n",
    "    submission = submission.sort_values(\n",
    "        [\"lab_id\", \"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return submission\n",
    "\n",
    "BAD_VIDEOS = []\n",
    "\n",
    "def compute_validation_score(\n",
    "    submission: pd.DataFrame,\n",
    "    lab_id: Optional[str] = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Gọi metric `score()` chính thức trên train set.\n",
    "    Nếu lab_id != None → chỉ validate trên lab đó.\n",
    "    \"\"\"\n",
    "    # ===== THAY ĐỔI Ở ĐÂY =====\n",
    "    # Không dùng train.csv, mà phải đọc toàn bộ annotations\n",
    "    train_meta = pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "    \n",
    "    if lab_id is not None:\n",
    "        train_meta = train_meta[train_meta[\"lab_id\"] == lab_id].reset_index(drop=True)\n",
    "\n",
    "    if BAD_VIDEOS:\n",
    "        train_meta = train_meta[~train_meta[\"video_id\"].isin(BAD_VIDEOS)]\n",
    "    \n",
    "    # Đọc tất cả annotation files\n",
    "    all_annotations = []\n",
    "    for _, row in train_meta.iterrows():\n",
    "        lab = row[\"lab_id\"]\n",
    "        vid = row[\"video_id\"]\n",
    "        ann = load_annotation(lab, vid)\n",
    "        if not ann.empty:\n",
    "            ann[\"lab_id\"] = lab\n",
    "            ann[\"video_id\"] = vid\n",
    "            ann[\"behaviors_labeled\"] = row[\"behaviors_labeled\"]\n",
    "            all_annotations.append(ann)\n",
    "    \n",
    "    if not all_annotations:\n",
    "        print(\"Không có annotation nào để validate!\")\n",
    "        return 0.0\n",
    "    \n",
    "    dataset = pd.concat(all_annotations, ignore_index=True)\n",
    "    \n",
    "    # Filter submission theo lab nếu cần\n",
    "    if lab_id is not None:\n",
    "        submission = submission[submission[\"lab_id\"] == lab_id].reset_index(drop=True)\n",
    "    \n",
    "    # ===== GỌI METRIC =====\n",
    "    s = score(dataset, submission, row_id_column_name=\"row_id\")\n",
    "\n",
    "    print(\n",
    "        f\"Official validation score\"\n",
    "        f\"{' (lab=' + lab_id + ')' if lab_id is not None else ''}: {s:.6f}\"\n",
    "    )\n",
    "    return float(s)\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. MAIN\n",
    "# =========================================================\n",
    "def str_to_mouse_id(s: str) -> int:\n",
    "    if s == \"self\":\n",
    "        return -1\n",
    "    return int(str(s).replace(\"mouse\", \"\"))\n",
    "\n",
    "\n",
    "def predict_behaviors_for_pair(\n",
    "    lab_id: str,\n",
    "    video_id: int,\n",
    "    agent_internal_id: Any,\n",
    "    target_internal_id: Any,\n",
    "    behaviors: List[str],\n",
    "    test_meta: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Chạy inference cho 1 cặp (video, agent_internal_id, target_internal_id)\n",
    "    với list behaviors (cùng mode: all self hoặc all pair).\n",
    "    Trả về segment-level DataFrame: video_id, action, start_frame, stop_frame.\n",
    "    \"\"\"\n",
    "    if lab_id != \"GroovyShrew\": return None\n",
    "    frames, feat_df = get_frame_features_for_pair_test(\n",
    "        lab_id=lab_id,\n",
    "        video_id=video_id,\n",
    "        agent_id=agent_internal_id,\n",
    "        target_id=target_internal_id,\n",
    "        test_meta=test_meta,\n",
    "    )\n",
    "    if feat_df.empty:\n",
    "        return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "\n",
    "    feat_df = feat_df.astype(\"float32\")\n",
    "    n_frames = len(feat_df)\n",
    "\n",
    "    scores_per_behavior = {}\n",
    "    for behavior in behaviors:\n",
    "        models = load_models_for_behavior_infer(lab_id, behavior)\n",
    "        if not models:\n",
    "            continue\n",
    "\n",
    "        req_feats = models[0][0].feature_names\n",
    "        # Build X_test với đúng bộ feature của model\n",
    "        X_test = pd.DataFrame(\n",
    "            0.0,\n",
    "            index=feat_df.index,\n",
    "            columns=req_feats,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        common = list(set(req_feats) & set(feat_df.columns))\n",
    "        if common:\n",
    "            X_test[common] = feat_df[common]\n",
    "\n",
    "        dtest = xgb.DMatrix(X_test, feature_names=req_feats)\n",
    "\n",
    "        agg_scores = np.zeros(n_frames, dtype=np.float32)\n",
    "        for booster, thr in models:\n",
    "            probs = booster.predict(dtest)\n",
    "            labels = (probs >= thr).astype(np.int8)\n",
    "            agg_scores += probs * labels\n",
    "\n",
    "        agg_scores /= max(len(models), 1)\n",
    "        scores_per_behavior[behavior] = agg_scores\n",
    "\n",
    "        del dtest, X_test\n",
    "        gc.collect()\n",
    "\n",
    "    if not scores_per_behavior:\n",
    "        return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "\n",
    "    beh_list = list(scores_per_behavior.keys())\n",
    "    score_mat = np.vstack([scores_per_behavior[b] for b in beh_list]).T  # [F, B]\n",
    "\n",
    "    max_idx = score_mat.argmax(axis=1)\n",
    "    max_scores = score_mat.max(axis=1)\n",
    "    labels = np.where(max_scores == 0.0, \"none\", np.array(beh_list)[max_idx])\n",
    "\n",
    "    # frame-level → segment\n",
    "    segments = []\n",
    "    prev_lab = \"none\"\n",
    "    prev_start = None\n",
    "    prev_f = None\n",
    "\n",
    "    for f, lab in zip(frames, labels):\n",
    "        if lab != prev_lab:\n",
    "            if prev_lab != \"none\":\n",
    "                segments.append(\n",
    "                    {\n",
    "                        \"video_id\": int(video_id),\n",
    "                        \"action\": prev_lab,\n",
    "                        \"start_frame\": int(prev_start),\n",
    "                        \"stop_frame\": int(prev_f + 1),\n",
    "                    }\n",
    "                )\n",
    "            prev_lab = lab\n",
    "            prev_start = f\n",
    "        prev_f = f\n",
    "\n",
    "    if prev_lab != \"none\":\n",
    "        segments.append(\n",
    "            {\n",
    "                \"video_id\": int(video_id),\n",
    "                \"action\": prev_lab,\n",
    "                \"start_frame\": int(prev_start),\n",
    "                \"stop_frame\": int(prev_f + 1),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not segments:\n",
    "        return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "\n",
    "    return pd.DataFrame(segments)\n",
    "\n",
    "\n",
    "\n",
    "target_lab = \"GroovyShrew\"\n",
    "print(f\"Đọc test.csv cho lab {target_lab} ...\")\n",
    "test_meta = pd.read_csv(INPUT_DIR / \"test.csv\")\n",
    "test_meta = test_meta[test_meta[\"lab_id\"] == target_lab].reset_index(drop=True)\n",
    "\n",
    "# Lấy danh sách behavior đã train (thư mục con trong RESULTS_DIR/AdaptableSnail)\n",
    "lab_result_dir = RESULTS_DIR / target_lab\n",
    "if lab_result_dir.exists():\n",
    "    trained_behaviors = sorted(\n",
    "        [p.name for p in lab_result_dir.iterdir() if p.is_dir()]\n",
    "    )\n",
    "else:\n",
    "    trained_behaviors = []\n",
    "\n",
    "self_behaviors_in_lab = [b for b in trained_behaviors if b in SELF_BEHAVIORS]\n",
    "pair_behaviors_in_lab = [b for b in trained_behaviors if b in PAIR_BEHAVIORS]\n",
    "\n",
    "print(\"Behaviors (self) dùng để predict:\", self_behaviors_in_lab)\n",
    "print(\"Behaviors (pair) dùng để predict:\", pair_behaviors_in_lab)\n",
    "\n",
    "all_segments = []\n",
    "\n",
    "# Loop từng video test của lab\n",
    "for video_id in sorted(test_meta[\"video_id\"].unique()):\n",
    "    print(f\"Predict video_id={video_id} ...\")\n",
    "\n",
    "    tracking = load_tracking_test(target_lab, video_id)\n",
    "    mouse_ids_internal = sorted(tracking[\"mouse_id\"].unique().tolist())\n",
    "\n",
    "    # Map internal mouse_id -> string để đưa vào submission\n",
    "    def to_submit_id(mid):\n",
    "        s = str(mid)\n",
    "        return s if s.startswith(\"mouse\") else f\"mouse{s}\"\n",
    "\n",
    "    # SELF behaviors: agent == target (self)\n",
    "    if self_behaviors_in_lab:\n",
    "        for mid in mouse_ids_internal:\n",
    "            seg_df = predict_behaviors_for_pair(\n",
    "                lab_id=target_lab,\n",
    "                video_id=video_id,\n",
    "                agent_internal_id=mid,\n",
    "                target_internal_id=mid,  # self\n",
    "                behaviors=self_behaviors_in_lab,\n",
    "                test_meta=test_meta,\n",
    "            )\n",
    "            if not seg_df.empty:\n",
    "                seg_df[\"agent_id\"] = to_submit_id(mid)\n",
    "                seg_df[\"target_id\"] = \"self\"\n",
    "                all_segments.append(seg_df)\n",
    "\n",
    "    # PAIR behaviors: mọi cặp agent != target\n",
    "    if pair_behaviors_in_lab and len(mouse_ids_internal) > 1:\n",
    "        for agent_internal, target_internal in itertools.permutations(\n",
    "            mouse_ids_internal, 2\n",
    "        ):\n",
    "            seg_df = predict_behaviors_for_pair(\n",
    "                lab_id=target_lab,\n",
    "                video_id=video_id,\n",
    "                agent_internal_id=agent_internal,\n",
    "                target_internal_id=target_internal,\n",
    "                behaviors=pair_behaviors_in_lab,\n",
    "                test_meta=test_meta,\n",
    "            )\n",
    "            if not seg_df.empty:\n",
    "                seg_df[\"agent_id\"] = to_submit_id(agent_internal)\n",
    "                seg_df[\"target_id\"] = to_submit_id(target_internal)\n",
    "                all_segments.append(seg_df)\n",
    "\n",
    "# Gộp tất cả segments → submission.csv\n",
    "# Gộp tất cả segments → submission2.csv\n",
    "if all_segments:\n",
    "    submission4 = pd.concat(all_segments, ignore_index=True)\n",
    "    submission4 = submission4[\n",
    "        [\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "    ]\n",
    "    submission4 = submission4.sort_values(\n",
    "        [\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\"]\n",
    "    ).reset_index(drop=True)\n",
    "else:\n",
    "    # DataFrame rỗng, KHÔNG dummy row\n",
    "    submission4 = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"video_id\",\n",
    "            \"agent_id\",\n",
    "            \"target_id\",\n",
    "            \"action\",\n",
    "            \"start_frame\",\n",
    "            \"stop_frame\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Thêm row_id (kể cả khi rỗng)\n",
    "submission4.insert(0, \"row_id\", np.arange(len(submission4), dtype=np.int64))\n",
    "\n",
    "sub_path = WORKING_DIR / \"submission4.csv\"\n",
    "submission4.to_csv(sub_path, index=False)\n",
    "print(f\"Saved GroovyShrew submission to {sub_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bef108",
   "metadata": {
    "papermill": {
     "duration": 0.018708,
     "end_time": "2025-12-13T17:39:17.229915",
     "exception": false,
     "start_time": "2025-12-13T17:39:17.211207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# JovialSwallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54aab07a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:17.267605Z",
     "iopub.status.busy": "2025-12-13T17:39:17.267078Z",
     "iopub.status.idle": "2025-12-13T17:39:17.381383Z",
     "shell.execute_reply": "2025-12-13T17:39:17.380708Z"
    },
    "papermill": {
     "duration": 0.134372,
     "end_time": "2025-12-13T17:39:17.382633",
     "exception": false,
     "start_time": "2025-12-13T17:39:17.248261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "\n",
    "# 1) Xóa mọi thứ trong /kaggle/working trừ .csv\n",
    "for path in WORKING_DIR.iterdir():\n",
    "    # giữ lại file .csv\n",
    "    if path.is_file() and path.suffix == \".csv\":\n",
    "        continue\n",
    "\n",
    "    if path.is_file():\n",
    "        try:\n",
    "            path.unlink()\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove file {path}: {e}\")\n",
    "    elif path.is_dir():\n",
    "        try:\n",
    "            shutil.rmtree(path, ignore_errors=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove dir {path}: {e}\")\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c404896",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:17.422126Z",
     "iopub.status.busy": "2025-12-13T17:39:17.421912Z",
     "iopub.status.idle": "2025-12-13T17:39:17.575408Z",
     "shell.execute_reply": "2025-12-13T17:39:17.574585Z"
    },
    "papermill": {
     "duration": 0.175216,
     "end_time": "2025-12-13T17:39:17.576670",
     "exception": false,
     "start_time": "2025-12-13T17:39:17.401454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đọc test.csv cho lab JovialSwallow ...\n",
      "Behaviors (self) dùng để predict: []\n",
      "Behaviors (pair) dùng để predict: ['attack', 'chase', 'sniff']\n",
      "Saved JovialSwallow submission to /kaggle/working/submission5.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "np.seterr(invalid=\"ignore\", divide=\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    \"\"\"\n",
    "    Chứa cấu hình tham số (Hyperparameters).\n",
    "    \"\"\"\n",
    "    fps: float = 30.0\n",
    "    pix_per_cm: float = 1.0\n",
    "    smooth_sigma: float = 1.0\n",
    "    use_pairwise: bool = True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. AGENT CONTEXT\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class AgentContext:\n",
    "    \"\"\"\n",
    "    Container chứa dữ liệu đã tiền xử lý của một con chuột.\n",
    "    Giúp tránh việc tính toán lại vận tốc/gia tốc nhiều lần.\n",
    "    \"\"\"\n",
    "    idx: pd.Index          # Index frame\n",
    "    pos: np.ndarray        # [F, 2] cm\n",
    "    vel: np.ndarray        # [F, 2] cm/s\n",
    "    speed: np.ndarray      # [F, 1] cm/s\n",
    "    acc: np.ndarray        # [F, 2] cm/s^2\n",
    "    \n",
    "    cx: pd.Series          # Series tọa độ X (để dùng rolling)\n",
    "    cy: pd.Series          # Series tọa độ Y\n",
    "    speed_series: pd.Series # Series tốc độ\n",
    "    \n",
    "    raw_df: Optional[pd.DataFrame] = None # Dữ liệu gốc các bộ phận \n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. FEATURE EXTRACTOR\n",
    "# =============================================================================\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Class trích xuất đặc trưng hành vi từ dữ liệu tracking.\n",
    "    \"\"\"\n",
    "    def __init__(self, fps: float, pix_per_cm: float, smooth_sigma: float = 1.0, use_pairwise: bool = True):\n",
    "        # Map tham số từ init vào Config\n",
    "        self.cfg = FeatureConfig(\n",
    "            fps=float(fps), \n",
    "            pix_per_cm=float(pix_per_cm), \n",
    "            smooth_sigma=smooth_sigma,\n",
    "            use_pairwise=use_pairwise\n",
    "        )\n",
    "        \n",
    "        # Đăng ký các hàm feature sẽ chạy\n",
    "        self.feature_registry = {\n",
    "            \"kinematics\": self._feat_basic_kinematics,\n",
    "            \"multiscale\": self._feat_multiscale,\n",
    "            \"long_range\": self._feat_long_range,\n",
    "            \"cumulative\": self._feat_cumulative,\n",
    "            \"curvature\": self._feat_curvature,\n",
    "            \"speed_asym\": self._feat_speed_asym,\n",
    "            \"gauss_shift\": self._feat_gauss_shift,\n",
    "            \"avoid\": self._feat_attack_sniff,\n",
    "            # \"pose\": self._feat_pose_shape,\n",
    "            \"a\": self._feat_follow_pattern,\n",
    "            \"b\": self._feat_shortburst_social,\n",
    "            # \"pairwise\": self._feat_pairwise\n",
    "        }\n",
    "\n",
    "    # --- Helpers ---\n",
    "    def _scale(self, n_frames_30fps: int) -> int:\n",
    "        \"\"\"Quy đổi số frame từ chuẩn 30fps sang fps thực tế của video.\"\"\"\n",
    "        return max(1, int(round(n_frames_30fps * self.cfg.fps / 30.0)))\n",
    "\n",
    "    def _to_cm(self, arr):\n",
    "        \"\"\"Chuyển pixel -> cm.\"\"\"\n",
    "        return arr / self.cfg.pix_per_cm\n",
    "\n",
    "    def _smooth(self, x):\n",
    "        \"\"\"Làm mượt dữ liệu bằng Gaussian filter.\"\"\"\n",
    "        if self.cfg.smooth_sigma is None or x.shape[0] < 3: return x\n",
    "        if np.all(np.isnan(x)): return x\n",
    "        return gaussian_filter1d(x, sigma=self.cfg.smooth_sigma, axis=0, mode=\"nearest\")\n",
    "\n",
    "    def _forward_fill_nan(self, pos):\n",
    "        \"\"\"\n",
    "        Điền dữ liệu thiếu (NaN) bằng giá trị hợp lệ trước đó (Forward Fill).\n",
    "        \"\"\"\n",
    "        if np.all(np.isnan(pos)):\n",
    "            return np.zeros_like(pos)\n",
    "\n",
    "        pos_ffill = pos.copy()\n",
    "        mask = np.any(~np.isnan(pos_ffill), axis=1)\n",
    "        if not mask.any():\n",
    "            return np.zeros_like(pos_ffill)\n",
    "\n",
    "        valid_idx = np.where(mask)[0]\n",
    "        first, last = valid_idx[0], valid_idx[-1]\n",
    "        pos_ffill[:first] = pos_ffill[first]\n",
    "        pos_ffill[last + 1:] = pos_ffill[last]\n",
    "        df_temp = pd.DataFrame(pos_ffill)\n",
    "        df_temp = df_temp.ffill()\n",
    "        return df_temp.to_numpy()\n",
    "    \n",
    "    def _speed_series(self, cx: pd.Series, cy: pd.Series) -> pd.Series:\n",
    "        dx = cx.diff()\n",
    "        dy = cy.diff()\n",
    "        v = np.hypot(dx, dy).fillna(0.0) * self.cfg.fps\n",
    "        return v.astype(\"float32\")\n",
    "    \n",
    "    def _roll_future_mean(self, s: pd.Series, w: int, min_p: int = 1) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).mean().iloc[::-1]\n",
    "\n",
    "    def _roll_future_var(self, s: pd.Series, w: int, min_p: int = 2) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).var().iloc[::-1]\n",
    "\n",
    "    # --- Core Logic ---\n",
    "    def _compute_kinematics(self, pos_px: np.ndarray):\n",
    "        \"\"\"\n",
    "        Tính toán vật lý cơ bản: Pos(cm), Vel, Speed, Acc.\n",
    "        Input: Array [Frames, 2] (pixel).\n",
    "        Output: Tuple (pos_cm, vel, speed, acc).\n",
    "        \"\"\"\n",
    "        pos_ffill = self._forward_fill_nan(pos_px)\n",
    "        pos_cm = self._to_cm(pos_ffill.astype(np.float32))\n",
    "        pos_cm = self._smooth(pos_cm)                                               # [F, 2]\n",
    "\n",
    "        dt = 1.0 / self.cfg.fps\n",
    "        vel = np.zeros_like(pos_cm, dtype=np.float32)\n",
    "        vel[1:] = (pos_cm[1:] - pos_cm[:-1]) / dt                                   # [F, 2: (vx, vy)]\n",
    "        speed = np.linalg.norm(vel, axis=1, keepdims=True).astype(np.float32)       # [F, 1]\n",
    "\n",
    "        acc = np.zeros_like(pos_cm, dtype=np.float32)                          \n",
    "        acc[1:] = (vel[1:] - vel[:-1]) / dt                                         # [F, 2:(ax, ay)]\n",
    "        return pos_cm.astype(np.float32), vel, speed, acc\n",
    "\n",
    "    def _build_context(self, frames, pos_px, mouse_df=None) -> AgentContext:\n",
    "        \"\"\"\n",
    "        Tạo AgentContext chứa đầy đủ thông tin vật lý của 1 con chuột.\n",
    "        \"\"\"\n",
    "        p, v, s, a = self._compute_kinematics(pos_px)\n",
    "        idx = pd.Index(frames, name=\"frame\")\n",
    "        \n",
    "        return AgentContext(\n",
    "            idx=idx, pos=p, vel=v, speed=s, acc=a, \n",
    "            cx=pd.Series(p[:, 0], index=idx), \n",
    "            cy=pd.Series(p[:, 1], index=idx), \n",
    "            speed_series=pd.Series(s[:, 0], index=idx), \n",
    "            raw_df=mouse_df\n",
    "        )\n",
    "\n",
    "    # --- Feature Modules ---\n",
    "    def _feat_basic_kinematics(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Lấy các giá trị thô: tọa độ x, y, vận tốc vx, vy, tốc độ, gia tốc ax, ay.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"a_x\": ctx.pos[:, 0], \"a_y\": ctx.pos[:, 1],\n",
    "            \"a_vx\": ctx.vel[:, 0], \"a_vy\": ctx.vel[:, 1],\n",
    "            \"a_speed\": ctx.speed[:, 0],\n",
    "            \"a_ax\": ctx.acc[:, 0], \"a_ay\": ctx.acc[:, 1]\n",
    "        }\n",
    "\n",
    "    def _feat_multiscale(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tính tốc độ trung bình (Mean) và độ lệch chuẩn (Std) ở đa mức thời gian.\n",
    "        Feature 'sp_ratio' đo độ bùng nổ (Burstiness).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        speed = ctx.speed_series\n",
    "        frame_scales = [10, 40, 160]\n",
    "        for scale in frame_scales:\n",
    "            ws = self._scale(scale)\n",
    "            if len(speed) >= ws:\n",
    "                roller = speed.rolling(ws, min_periods=max(1, ws//4), center=True)\n",
    "                feats[f\"sp_m{scale}\"] = roller.mean().astype(\"float32\")\n",
    "                feats[f\"sp_s{scale}\"] = roller.std().astype(\"float32\")\n",
    "        feats[f\"sp_ratio\"] = feats[\"sp_m10\"] / (feats[\"sp_m160\"] + 1e-6)\n",
    "        return feats \n",
    "\n",
    "    \n",
    "        \n",
    "    def _feat_long_range(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng ngữ cảnh dài hạn:\n",
    "        - x_ml, y_ml: Vị trí trung bình trong quá khứ.\n",
    "        - sp_pct: Xếp hạng (percentile) của tốc độ hiện tại so với quá khứ.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        speed = ctx.speed_series\n",
    "\n",
    "        for window in [120, 240]:\n",
    "            ws = self._scale(window)\n",
    "            if len(ctx.cx) >= ws:\n",
    "                feats[f\"x_ml{window}\"] = ctx.cx.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "                feats[f\"y_ml{window}\"] = ctx.cy.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "\n",
    "        for span in [60, 120]:\n",
    "            s = self._scale(span)\n",
    "            feats[f\"x_e{span}\"] = ctx.cx.ewm(span=s, min_periods=1).mean()\n",
    "            feats[f\"y_e{span}\"] = ctx.cy.ewm(span=s, min_periods=1).mean()\n",
    "\n",
    "        for window in [60, 120]:\n",
    "            ws = self._scale(window)\n",
    "            if len(speed) >= ws:\n",
    "                feats[f\"sp_pct{window}\"] = speed.rolling(\n",
    "                    ws, min_periods=max(5, ws // 6), center=True\n",
    "                ).rank(pct=True)\n",
    "        return feats\n",
    "    \n",
    "\n",
    "    def _feat_curvature(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        feats = {}\n",
    "\n",
    "        vel_x, vel_y = ctx.vel[:, 0], ctx.vel[:, 1]\n",
    "        acc_x, acc_y = ctx.acc[:, 0], ctx.acc[:, 1]\n",
    "        cross_prod = vel_x * acc_y - vel_y * acc_x\n",
    "        vel_mag = np.sqrt(vel_x**2 + vel_y**2)\n",
    "        moving_mask = vel_mag > 2.0\n",
    "        vel_mag_safe = np.maximum(vel_mag, 0.1 / self.cfg.fps)\n",
    "        raw_curv = cross_prod / (vel_mag_safe**3)\n",
    "        raw_curv = np.where(moving_mask, raw_curv, 0.0)\n",
    "        min_turn_radius_cm = 0.5\n",
    "        max_k = 1.0 / min_turn_radius_cm\n",
    "        raw_curv = np.clip(raw_curv, -max_k, max_k)\n",
    "        abs_curv = np.abs(raw_curv)\n",
    "        abs_curv_series = pd.Series(abs_curv, index=ctx.idx)\n",
    "\n",
    "        for w in [30, 60]:\n",
    "            ws = self._scale(w)\n",
    "            min_p = max(ws // 3, 1)\n",
    "            feats[f\"curv_mean_{w}\"] = abs_curv_series.rolling(ws, min_periods=min_p).mean()\n",
    "\n",
    "        angle = np.arctan2(vel_y, vel_x)\n",
    "        angle_series = pd.Series(angle, index=ctx.idx)\n",
    "        angle_change = np.abs(angle_series.diff().fillna(0.0))\n",
    "        angle_change = np.where(angle_change > np.pi, 2 * np.pi - angle_change, angle_change)\n",
    "        angle_change_series = pd.Series(angle_change, index=ctx.idx)\n",
    "        angle_change_series = pd.Series(np.where(moving_mask, angle_change_series, 0.0), index=ctx.idx)\n",
    "\n",
    "        ws = self._scale(30)\n",
    "        feats[\"turn_rate_30\"] = angle_change_series.rolling(ws, min_periods=max(ws // 3, 1)).sum()\n",
    "\n",
    "        return feats\n",
    "    \n",
    "    def _feat_cumulative(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tổng quãng đường di chuyển trong một khoảng thời gian dài xung quanh frame hiện tại.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        L = max(1, self._scale(180))\n",
    "        step = np.hypot(ctx.cx.diff(), ctx.cy.diff()).fillna(0.0)\n",
    "        path = step.rolling(2 * L + 1, min_periods=max(5, L // 6), center=True).sum()\n",
    "        feats[\"path_cum180\"] =  path.fillna(0.0).astype(\"float32\")\n",
    "        return feats\n",
    "\n",
    "    def _feat_speed_asym(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Bất đối xứng tốc độ (Tương lai - Quá khứ).\n",
    "        \"\"\"\n",
    "        w = max(3, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        v_past = v.rolling(w, min_periods=1).mean()\n",
    "        v_fut = self._roll_future_mean(v, w, min_p=1)\n",
    "        return {\"spd_asym_1s\": (v_fut - v_past).fillna(0.0)}\n",
    "    \n",
    "    def _feat_gauss_shift(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Độ lệch Gaussian (KL Divergence) giữa quá khứ và tương lai.\n",
    "        Đo lường sự thay đổi trạng thái thống kê.\n",
    "        \"\"\"\n",
    "        w = max(5, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        mu_p = v.rolling(w, min_periods=1).mean()\n",
    "        va_p = v.rolling(w, min_periods=1).var().clip(lower=1e-6)\n",
    "        mu_f = self._roll_future_mean(v, w, min_p=1)\n",
    "        va_f = self._roll_future_var(v, w, min_p=1).clip(lower=1e-6)\n",
    "\n",
    "        kl_pf = 0.5 * (\n",
    "            (va_p / va_f) + ((mu_f - mu_p) ** 2) / va_f - 1.0 + np.log(va_f / va_p)\n",
    "        )\n",
    "        kl_fp = 0.5 * (\n",
    "            (va_f / va_p) + ((mu_p - mu_f) ** 2) / va_p - 1.0 + np.log(va_p / va_f)\n",
    "        )\n",
    "        return {\n",
    "            \"spd_symkl_1s\": (kl_pf + kl_fp).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        }\n",
    "\n",
    "    def _feat_avoidance_trajectory(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Tính toán quỹ đạo né tránh:\n",
    "        1. Relative Heading: Góc di chuyển so với hướng tới đối thủ.\n",
    "        2. Future Distance Gain: Dự báo xem hành động này có giúp chuột ra xa đối thủ trong tương lai không.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        if target_ctx is None: \n",
    "            return feats\n",
    "\n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        # Góc hướng tới địch (Angle to Target)\n",
    "        angle_to_target = np.arctan2(rel_vec[:, 1], rel_vec[:, 0])\n",
    "        \n",
    "        # Góc di chuyển của Tôi (My Heading)\n",
    "        my_heading = np.arctan2(ctx.vel[:, 1], ctx.vel[:, 0])\n",
    "        \n",
    "        # Độ lệch góc (Absolute Difference)\n",
    "        # Cần xử lý wrap góc (ví dụ: lệch giữa 179 độ và -179 độ là 2 độ chứ ko phải 358)\n",
    "        diff = np.abs(angle_to_target - my_heading)\n",
    "        diff = np.minimum(diff, 2*np.pi - diff) # Chuẩn hóa về [0, pi]\n",
    "        \n",
    "        # Feature: Cosine của góc lệch\n",
    "        # 1.0 (0 độ) -> Lao vào\n",
    "        # 0.0 (90 độ) -> AVOID (Lách ngang)\n",
    "        # -1.0 (180 độ) -> Escape\n",
    "        feats[\"heading_rel_cos\"] = pd.Series(np.cos(diff), index=idx, dtype=\"float32\")\n",
    "        \n",
    "        # Feature: Góc lệch tuyệt đối (đổi ra độ cho dễ hình dung nếu cần, ở đây để rad)\n",
    "        feats[\"heading_rel_abs\"] = pd.Series(diff, index=idx, dtype=\"float32\")\n",
    "\n",
    "\n",
    "        # --- 2. FUTURE DISTANCE GAIN (Hiệu quả tránh né) ---\n",
    "        # \"Sau 15 frame (0.5s) hoặc 30 frame (1s), mình có xa nó ra không?\"\n",
    "        \n",
    "        dist_now = np.linalg.norm(rel_vec, axis=1)\n",
    "        s_dist = pd.Series(dist_now, index=idx)\n",
    "        \n",
    "        scales = [15, 30] # 0.5s và 1s\n",
    "        for w in scales:\n",
    "            ws = self._scale(w)\n",
    "            \n",
    "            # Lấy khoảng cách ở tương lai (shift ngược lên)\n",
    "            # s.shift(-ws) là giá trị của t + ws\n",
    "            dist_future = s_dist.shift(-ws)\n",
    "            gain = dist_future - s_dist\n",
    "            \n",
    "            feats[f\"dist_gain_{w}f\"] = gain.fillna(0.0).astype(\"float32\")\n",
    "\n",
    "        return feats\n",
    "    \n",
    "    def _extract_part(self, ctx: AgentContext, part: str) -> Optional[np.ndarray]:\n",
    "        if ctx.raw_df is None: return None\n",
    "        if part not in ctx.raw_df.columns.get_level_values(0): return None\n",
    "        try:\n",
    "            sub_df = ctx.raw_df.xs(part, axis=1, level=0)[[\"x\", \"y\"]].reindex(ctx.idx)\n",
    "        except KeyError: return None\n",
    "        raw = sub_df.to_numpy()\n",
    "        raw = self._forward_fill_nan(raw)\n",
    "        cm = self._to_cm(raw.astype(np.float32))\n",
    "        return self._smooth(cm)\n",
    "    \n",
    "    def _extract_parts_dict(self, ctx: AgentContext, parts: List[str] = None) -> Dict[str, Optional[np.ndarray]]:\n",
    "        out = {}\n",
    "        for p in parts:\n",
    "            out[p] = self._extract_part(ctx, p)\n",
    "        return out\n",
    "        \n",
    "    def _feat_pose_shape(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Placeholder cho các đặc trưng hình dáng (Elongation, Body Angle...).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "\n",
    "        def zero(): return pd.Series(0.0, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        def dist(k1, k2):\n",
    "            p1, p2 = parts.get(k1), parts.get(k2)\n",
    "            if p1 is None or p2 is None: return zero()\n",
    "            d = np.linalg.norm(p1 - p2, axis=1)\n",
    "            return pd.Series(d, index=ctx.idx, dtype=\"float32\")\n",
    "        \n",
    "        # def body_angle():\n",
    "        #     if parts.get(\"nose\") is None: return zero()\n",
    "        #     if parts.get(\"neck\") is None: return zero()\n",
    "        #     if parts.get(\"tail_base\") is None: return zero()\n",
    "\n",
    "        #     v1 = parts.get(\"nose\") - parts.get(\"neck\")\n",
    "        #     v2 = parts.get(\"tail_base\") - parts.get(\"neck\")\n",
    "        #     dot_product = np.sum(v1 * v2, axis=1)\n",
    "        #     mag = np.linalg.norm(v1, axis=1) * np.linalg.norm(v2, axis=1)\n",
    "        #     cos_angle = np.clip(dot_product / (mag + 1e-6), -1.0, 1.0).astype(\"float32\")\n",
    "        #     return cos_angle\n",
    "        \n",
    "        # def elongation():\n",
    "        #     if parts.get(\"nose\")          is None: return zero()\n",
    "        #     if parts.get(\"tail_base\")     is None: return zero()\n",
    "        #     if parts.get(\"lateral_left\")  is None: return zero()\n",
    "        #     if parts.get(\"lateral_right\") is None: return zero()\n",
    "\n",
    "        #     d1 = dist(\"nose\", \"tail_base\")\n",
    "        #     d2 = dist(\"lateral_left\", \"lateral_right\")\n",
    "        #     elongation = d1 / (d2 + 1e-6).astype(\"float32\")\n",
    "        #     return elongation\n",
    "\n",
    "        \n",
    "        \n",
    "        def vel(part: str, n_frames_30fps: int) -> Dict:\n",
    "            part_pos = self._extract_part(ctx, part)\n",
    "            if part_pos is None: return zero()\n",
    "            \n",
    "            s_x = pd.Series(part_pos[:, 0], index=ctx.idx)\n",
    "            s_y = pd.Series(part_pos[:, 1], index=ctx.idx)\n",
    "            raw_speed = self._speed_series(s_x, s_y)\n",
    "\n",
    "            ws = self._scale(n_frames_30fps)\n",
    "            val = raw_speed.rolling(ws, min_periods=1, center=True).mean()\n",
    "            return val.astype(\"float32\")\n",
    "\n",
    "\n",
    "        target_parts = [\"nose\", \"tail_base\", \n",
    "                        \"ear_left\", \"ear_right\", \"neck\", \"hip_left\", \"hip_right\"]\n",
    "        \n",
    "        parts = self._extract_parts_dict(ctx, target_parts)\n",
    "\n",
    "        feats[\"a_body_width\"]                = dist(\"hip_left\", \"hip_right\")\n",
    "        # feats[\"aa_nose_bodycenter_dist\"]     = dist(\"nose\", \"body_center\")\n",
    "        # feats[\"aa_nose_tailbase_dist\"]       = dist(\"nose\", \"tail_base\")\n",
    "        # feats[\"aa_bodycenter_tailbase_dist\"] = dist(\"body_center\", \"tail_base\")\n",
    "        \n",
    "        # feats[\"aa_bodycenter_ear_l_dist\"]    = dist(\"body_center\", \"ear_left\")\n",
    "        # feats[\"aa_bodycenter_ear_r_dist\"]    = dist(\"body_center\", \"ear_right\")\n",
    "        # feats[\"aa_bodycenter_lateral_l_dist\"]= dist(\"body_center\", \"lateral_left\")\n",
    "        # feats[\"aa_bodycenter_lateral_r_dist\"]= dist(\"body_center\", \"lateral_right\")\n",
    "        \n",
    "        # feats[\"a_body_angle\"]                = body_angle()\n",
    "        # feats[\"a_elongation\"]                = elongation()\n",
    "        # feats[\"a_tail_base_vel_500ms\"]       = vel(\"tail_base\", 15)\n",
    "        # feats[\"a_tail_base_vel_1000ms\"]      = vel(\"tail_base\", 30)\n",
    "        # feats[\"a_tail_base_vel_2000ms\"]      = vel(\"tail_base\", 60)\n",
    "        # feats[\"a_tail_base_vel_3000ms\"]      = vel(\"tail_base\", 90)\n",
    "        feats[\"a_nose_vel_500ms\"]            = vel(\"nose\", 15)\n",
    "        feats[\"a_nose_vel_1000ms\"]           = vel(\"nose\", 30)\n",
    "        feats[\"a_nose_vel_2000ms\"]           = vel(\"nose\", 60)\n",
    "        feats[\"a_nose_vel_3000ms\"]           = vel(\"nose\", 90)\n",
    "        # feats[\"a_ear_right_vel_500ms\"]       = vel(\"ear_right\", 15)\n",
    "        # feats[\"a_ear_right_vel_1000ms\"]      = vel(\"ear_right\", 30)\n",
    "        # feats[\"a_ear_right_vel_2000ms\"]      = vel(\"ear_right\", 60)\n",
    "        # feats[\"a_ear_right_vel_3000ms\"]      = vel(\"ear_right\", 90)\n",
    "\n",
    "        return feats\n",
    "\n",
    "    def _feat_shortburst_social(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Short-burst social features (10–30 frames) đặc biệt cho attack / chase / escape.\n",
    "        Chỉ dùng được khi có target_ctx.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- Lấy lại vài quantity cơ bản từ pairwise/avoidance ---\n",
    "        # vector Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # unit vector\n",
    "        rel_dist_safe = np.where(rel_dist == 0, 1e-6, rel_dist)\n",
    "        u_vec = rel_vec / rel_dist_safe[:, None]\n",
    "    \n",
    "        # velocity dọc trục nối (approach speed)\n",
    "        a_vel = ctx.vel\n",
    "        t_vel = target_ctx.vel\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)                # +: lao vào target\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1)             # +: target lao vào agent\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)    # +: lại gần nhau\n",
    "    \n",
    "        a_along_s = pd.Series(a_along, index=idx, dtype=\"float32\")\n",
    "        t_along_s = pd.Series(t_along, index=idx, dtype=\"float32\")\n",
    "        rel_along_s = pd.Series(rel_along, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # speed agent / target\n",
    "        a_speed = ctx.speed_series\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\"\n",
    "        )\n",
    "    \n",
    "        # heading_rel_cos ~ escape / approach\n",
    "        # vector body của agent\n",
    "        # (reuse idea từ _feat_pairwise)\n",
    "        # head ~ nose, tail ~ tail_base/body_center\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\", \"body_center\"])\n",
    "        head_a = parts_a.get(\"nose\")\n",
    "        tail_a = parts_a.get(\"tail_base\") if parts_a.get(\"tail_base\") is not None else parts_a.get(\"body_center\")\n",
    "    \n",
    "        if head_a is not None and tail_a is not None:\n",
    "            body_vec_a = head_a - tail_a\n",
    "            dot = np.sum(body_vec_a * rel_vec, axis=1)\n",
    "            mag = np.linalg.norm(body_vec_a, axis=1) * rel_dist_safe\n",
    "            heading_cos = np.clip(dot / (mag + 1e-6), -1.0, 1.0)\n",
    "            heading_cos_s = pd.Series(heading_cos, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            heading_cos_s = zero()\n",
    "    \n",
    "        # --- Rolling window 10, 20, 30 frames (ở fps gốc) ---\n",
    "        for w30 in [10, 20, 30]:\n",
    "            ws = self._scale(w30)\n",
    "            min_p = max(1, ws // 3)\n",
    "    \n",
    "            # Attack-like: approach mạnh, khoảng cách giảm nhanh\n",
    "            feats[f\"sb_att_approach_mean_{w30}\"] = a_along_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_att_rel_along_mean_{w30}\"] = rel_along_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_att_dist_delta_{w30}\"] = (rel_dist_s - rel_dist_s.shift(ws)).fillna(0.0)\n",
    "    \n",
    "            # Chase-like: agent & target đều nhanh, dist tương đối nhỏ\n",
    "            feats[f\"sb_chase_speed_agent_mean_{w30}\"] = a_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_chase_speed_target_mean_{w30}\"] = t_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_chase_dist_mean_{w30}\"] = rel_dist_s.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            # Escape-like: heading ngược, dist tăng nhanh\n",
    "            feats[f\"sb_esc_heading_cos_mean_{w30}\"] = heading_cos_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_esc_dist_gain_{w30}\"] = (rel_dist_s.shift(-ws) - rel_dist_s).fillna(0.0)\n",
    "    \n",
    "        # clip & fillna\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = v.replace([np.inf, -np.inf], np.nan).fillna(0.0).astype(\"float32\")\n",
    "    \n",
    "        return feats\n",
    "\n",
    "\n",
    "    def _feat_follow_pattern(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng hành vi FOLLOW:\n",
    "          - Agent ở gần target\n",
    "          - Cùng hướng (body + velocity)\n",
    "          - Tốc độ vừa phải\n",
    "          - Khoảng cách tương đối ổn định trong 0.5–1s\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 1. CÁC ĐẠI LƯỢNG CƠ BẢN ---\n",
    "        # Vector Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # Speed agent/target\n",
    "        a_speed = ctx.speed_series.astype(\"float32\")\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "    \n",
    "        # Body vector: nose - tail/body_center\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\", \"neck\"])\n",
    "        parts_t = self._extract_parts_dict(target_ctx, [\"nose\", \"tail_base\", \"neck\"])\n",
    "    \n",
    "        def body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"nose\")\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if tail is None:\n",
    "                tail = parts_dict.get(\"neck\")\n",
    "            if head is None or tail is None:\n",
    "                return None\n",
    "            return head - tail\n",
    "    \n",
    "        a_body = body_vec(parts_a)\n",
    "        t_body = body_vec(parts_t)\n",
    "    \n",
    "        if a_body is not None and t_body is not None:\n",
    "            dot_bt = np.sum(a_body * t_body, axis=1)\n",
    "            mag_bt = np.linalg.norm(a_body, axis=1) * np.linalg.norm(t_body, axis=1)\n",
    "            cos_body = np.clip(dot_bt / (mag_bt + 1e-6), -1.0, 1.0)\n",
    "            cos_body_s = pd.Series(cos_body, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            cos_body_s = zero()\n",
    "    \n",
    "        # Velocity hướng\n",
    "        a_vel = ctx.vel\n",
    "        t_vel = target_ctx.vel\n",
    "        a_speed_np = np.linalg.norm(a_vel, axis=1)\n",
    "        t_speed_np = np.linalg.norm(t_vel, axis=1)\n",
    "        moving_mask = (a_speed_np > 1e-3) & (t_speed_np > 1e-3)\n",
    "    \n",
    "        # cos giữa hướng velocity 2 con\n",
    "        dot_v = np.sum(a_vel * t_vel, axis=1)\n",
    "        mag_v = a_speed_np * t_speed_np + 1e-6\n",
    "        cos_vel = np.zeros_like(dot_v, dtype=\"float32\")\n",
    "        cos_vel[moving_mask] = np.clip(dot_v[moving_mask] / mag_v[moving_mask], -1.0, 1.0)\n",
    "        cos_vel_s = pd.Series(cos_vel, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 2. WINDOW NGẮN (FOLLOW LÀ PATTERN DÀI HƠN ATTACK) ---\n",
    "        for w30 in [15, 30, 60]:   # ~0.5s, 1s, 2s\n",
    "            ws = self._scale(w30)\n",
    "            min_p = max(ws // 3, 1)\n",
    "    \n",
    "            # Khoảng cách trung bình & độ dao động\n",
    "            m_dist = rel_dist_s.rolling(ws, min_periods=min_p).mean()\n",
    "            s_dist = rel_dist_s.rolling(ws, min_periods=min_p).std()\n",
    "    \n",
    "            # Cùng hướng (body + velocity)\n",
    "            m_cos_body = cos_body_s.rolling(ws, min_periods=min_p).mean()\n",
    "            m_cos_vel  = cos_vel_s.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            # Tốc độ vừa phải\n",
    "            m_sp_a = a_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            m_sp_t = t_speed.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            feats[f\"follow_dist_mean_{w30}\"] = m_dist\n",
    "            feats[f\"follow_dist_std_{w30}\"]  = s_dist\n",
    "            feats[f\"follow_cos_body_mean_{w30}\"] = m_cos_body\n",
    "            feats[f\"follow_cos_vel_mean_{w30}\"]  = m_cos_vel\n",
    "            feats[f\"follow_speed_agent_mean_{w30}\"] = m_sp_a\n",
    "            feats[f\"follow_speed_target_mean_{w30}\"] = m_sp_t\n",
    "    \n",
    "        # Clean\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "\n",
    "\n",
    "    def _feat_attack_sniff(\n",
    "        self,\n",
    "        ctx: AgentContext,\n",
    "        target_ctx: AgentContext = None,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng phân biệt attack vs sniff cho lab 2-mouse (agent=1, target=2).\n",
    "    \n",
    "        Ý tưởng:\n",
    "          - attack: speed 2 con biến động mạnh, đổi hướng nhiều, body overlap cao.\n",
    "          - sniff : mũi gần cổ/thân, overlap thấp hơn, motion nhẹ/ổn định hơn.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "    \n",
    "        def zero():\n",
    "            return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 1) TRÍCH XUẤT CÁC BỘ PHẬN CẦN THIẾT\n",
    "        # ---------------------------------------------------------\n",
    "        parts_a = self._extract_parts_dict(\n",
    "            ctx,\n",
    "            [\"nose\", \"neck\", \"ear_left\", \"ear_right\", \"hip_left\", \"hip_right\", \"tail_base\"],\n",
    "        )\n",
    "        parts_t = self._extract_parts_dict(\n",
    "            target_ctx,\n",
    "            [\"nose\", \"neck\", \"ear_left\", \"ear_right\", \"hip_left\", \"hip_right\", \"tail_base\"],\n",
    "        )\n",
    "\n",
    "        # helper khoảng cách\n",
    "        def dist(p1, p2):\n",
    "            if p1 is None or p2 is None:\n",
    "                return zero()\n",
    "            d = np.linalg.norm(p1 - p2, axis=1)\n",
    "            return pd.Series(d, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 2) ĐIỂM ĐẠI DIỆN THÂN (BODY CENTER) CHO MỖI CON\n",
    "        #    dùng trung bình neck – hips – tail_base\n",
    "        # ---------------------------------------------------------\n",
    "        def body_center(parts: Dict[str, Optional[np.ndarray]]):\n",
    "            arrs = []\n",
    "            for key in [\"neck\", \"hip_left\", \"hip_right\", \"tail_base\"]:\n",
    "                if parts.get(key) is not None:\n",
    "                    arrs.append(parts[key])\n",
    "            if not arrs:\n",
    "                return None\n",
    "            stack = np.stack(arrs, axis=1)  # [F, K, 2]\n",
    "            # trung bình theo bộ phận\n",
    "            bc = np.nanmean(stack, axis=1)\n",
    "            return bc.astype(\"float32\")\n",
    "    \n",
    "        a_center = body_center(parts_a)\n",
    "        t_center = body_center(parts_t)\n",
    "    \n",
    "        if a_center is not None and t_center is not None:\n",
    "            rel_vec = t_center - a_center\n",
    "            rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "            feats[\"as_rel_body_dist\"] = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            feats[\"as_rel_body_dist\"] = zero()\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 3) KHOẢNG CÁCH MŨI → PHẦN THÂN TARGET (CHO SNIFF)\n",
    "        # ---------------------------------------------------------\n",
    "        # mũi agent tới cổ/hips/tail_base target\n",
    "        a_nose = parts_a.get(\"nose\")\n",
    "        t_neck = parts_t.get(\"neck\")\n",
    "        t_hip_l = parts_t.get(\"hip_left\")\n",
    "        t_hip_r = parts_t.get(\"hip_right\")\n",
    "        t_tail  = parts_t.get(\"tail_base\")\n",
    "    \n",
    "        feats[\"as_dist_nose_neck\"]   = dist(a_nose, t_neck)\n",
    "        feats[\"as_dist_nose_hip_l\"]  = dist(a_nose, t_hip_l)\n",
    "        feats[\"as_dist_nose_hip_r\"]  = dist(a_nose, t_hip_r)\n",
    "        feats[\"as_dist_nose_tail\"]   = dist(a_nose, t_tail)\n",
    "    \n",
    "        # khoảng cách mũi → \"trung tâm thân\" target\n",
    "        t_body_c = body_center(parts_t)\n",
    "        if a_nose is not None and t_body_c is not None:\n",
    "            feats[\"as_dist_nose_bodycenter\"] = dist(a_nose, t_body_c)\n",
    "        else:\n",
    "            feats[\"as_dist_nose_bodycenter\"] = zero()\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 4) MỨC ĐỘ “BẠO LỰC”: DAO ĐỘNG TỐC ĐỘ & ĐỔI HƯỚNG\n",
    "        # ---------------------------------------------------------\n",
    "        # speed 2 con từ velocity\n",
    "        a_speed = pd.Series(\n",
    "            np.linalg.norm(ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "\n",
    "        ws_05 = self._scale(15)  # ~0.5s\n",
    "        mp_05 = max(ws_05 // 3, 1)\n",
    "    \n",
    "        feats[\"as_a_speed_std_05\"] = (\n",
    "            a_speed.rolling(ws_05, min_periods=mp_05).std().fillna(0.0).astype(\"float32\")\n",
    "        )\n",
    "        feats[\"as_t_speed_std_05\"] = (\n",
    "            t_speed.rolling(ws_05, min_periods=mp_05).std().fillna(0.0).astype(\"float32\")\n",
    "        )\n",
    "        feats[\"as_speed_std_sum_05\"] = (\n",
    "            feats[\"as_a_speed_std_05\"] + feats[\"as_t_speed_std_05\"]\n",
    "        )\n",
    "    \n",
    "        # Đổi hướng (jerk góc) của agent\n",
    "        a_angle = np.arctan2(ctx.vel[:, 1], ctx.vel[:, 0])\n",
    "        a_angle_diff = np.abs(np.diff(a_angle))\n",
    "        a_angle_diff = np.where(\n",
    "            a_angle_diff > np.pi, 2 * np.pi - a_angle_diff, a_angle_diff\n",
    "        )\n",
    "        a_angle_diff = np.concatenate([[0.0], a_angle_diff])\n",
    "        a_angle_diff_s = pd.Series(a_angle_diff, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        feats[\"as_a_turn_jerk_05\"] = (\n",
    "            a_angle_diff_s.rolling(ws_05, min_periods=mp_05)\n",
    "            .sum()\n",
    "            .fillna(0.0)\n",
    "            .astype(\"float32\")\n",
    "        )\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 5) XẤP XỈ OVERLAP CƠ THỂ (BODY OVERLAP)\n",
    "        #    dùng bbox từ các bộ phận thân\n",
    "        # ---------------------------------------------------------\n",
    "        def build_bbox(parts: Dict[str, Optional[np.ndarray]]):\n",
    "            # dùng neck, hips, tail_base; nếu thiếu sẽ bỏ qua\n",
    "            arrs = []\n",
    "            for k in [\"neck\", \"hip_left\", \"hip_right\", \"tail_base\"]:\n",
    "                if parts.get(k) is not None:\n",
    "                    arrs.append(parts[k])\n",
    "            if not arrs:\n",
    "                return None\n",
    "            stack = np.stack(arrs, axis=1)  # [F, K, 2]\n",
    "            xs = stack[:, :, 0]\n",
    "            ys = stack[:, :, 1]\n",
    "            xmin = np.nanmin(xs, axis=1)\n",
    "            xmax = np.nanmax(xs, axis=1)\n",
    "            ymin = np.nanmin(ys, axis=1)\n",
    "            ymax = np.nanmax(ys, axis=1)\n",
    "            return np.stack([xmin, ymin, xmax, ymax], axis=1).astype(\"float32\")\n",
    "    \n",
    "        def iou_box(box1: np.ndarray, box2: np.ndarray):\n",
    "            # box: [F, 4] = (xmin, ymin, xmax, ymax)\n",
    "            x1 = np.maximum(box1[:, 0], box2[:, 0])\n",
    "            y1 = np.maximum(box1[:, 1], box2[:, 1])\n",
    "            x2 = np.minimum(box1[:, 2], box2[:, 2])\n",
    "            y2 = np.minimum(box1[:, 3], box2[:, 3])\n",
    "    \n",
    "            inter_w = np.clip(x2 - x1, 0.0, None)\n",
    "            inter_h = np.clip(y2 - y1, 0.0, None)\n",
    "            inter = inter_w * inter_h\n",
    "    \n",
    "            area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "            area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "            union = area1 + area2 - inter + 1e-6\n",
    "            iou = inter / union\n",
    "            return iou.astype(\"float32\")\n",
    "\n",
    "        bbox_a = build_bbox(parts_a)\n",
    "        bbox_t = build_bbox(parts_t)\n",
    "        if bbox_a is not None and bbox_t is not None:\n",
    "            iou = iou_box(bbox_a, bbox_t)\n",
    "            iou_s = pd.Series(iou, index=idx, dtype=\"float32\")\n",
    "    \n",
    "            feats[\"as_body_iou\"] = iou_s\n",
    "    \n",
    "            ws_1s = self._scale(30)\n",
    "            mp_1s = max(ws_1s // 3, 1)\n",
    "            feats[\"as_body_iou_mean_1s\"] = (\n",
    "                iou_s.rolling(ws_1s, min_periods=mp_1s).mean().fillna(0.0).astype(\"float32\")\n",
    "            )\n",
    "        else:\n",
    "            feats[\"as_body_iou\"] = zero()\n",
    "            feats[\"as_body_iou_mean_1s\"] = zero()\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 6) DỌN NẠN NaN / Inf\n",
    "        # ---------------------------------------------------------\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "        \n",
    "\n",
    "    def _feat_pairwise(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng tương tác cặp đôi (Pairwise): Khoảng cách, Tốc độ tiếp cận.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None: \n",
    "            return feats\n",
    "\n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "\n",
    "        # --- 1. KHOẢNG CÁCH CƠ BẢN (DISTANCES) ---\n",
    "        # Vector nối Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        feats[\"rel_dist\"] = pd.Series(dist, index=idx, dtype=\"float32\")\n",
    "\n",
    "        # --- 2. KHOẢNG CÁCH CHI TIẾT (NOSE-TO-PART) ---\n",
    "        # Lấy các bộ phận quan trọng\n",
    "        my_parts = self._extract_parts_dict(ctx, [\"nose\"])\n",
    "        target_parts = self._extract_parts_dict(target_ctx, \n",
    "            [\"nose\", \"tail_base\", \"ear_left\", \"ear_right\", \"neck\", \"hip_left\", \"hip_right\"])\n",
    "\n",
    "        def dist_ab(pt_a, pt_b):\n",
    "            if pt_a is None or pt_b is None: return zero()\n",
    "            d = np.linalg.norm(pt_a - pt_b, axis=1)\n",
    "            return pd.Series(d, index=idx, dtype=\"float32\")\n",
    "\n",
    "        an, tn = my_parts[\"nose\"], target_parts[\"nose\"]\n",
    "        feats[\"dist_nose_nose\"] = dist_ab(an, tn)\n",
    "        feats[\"dist_nose_tail\"] = dist_ab(an, target_parts[\"tail_base\"])\n",
    "        # feats[\"dist_nose_body\"] = dist_ab(an, target_parts[\"body_center\"])\n",
    "        feats[\"dist_nose_el\"]   = dist_ab(an, target_parts[\"ear_left\"])\n",
    "        feats[\"dist_nose_er\"]   = dist_ab(an, target_parts[\"ear_right\"])\n",
    "\n",
    "        # feats[\"dist_nose_hl\"]   = dist_ab(an, target_parts[\"hip_left\"])\n",
    "        # feats[\"dist_nose_hr\"]   = dist_ab(an, target_parts[\"hip_right\"])\n",
    "        # feats[\"dist_nose_ne\"]   = dist_ab(an, target_parts[\"neck\"])\n",
    "        \n",
    "        # feats[\"dist_nose_tll\"]  = dist_ab(an, target_parts[\"lateral_left\"])\n",
    "        # feats[\"dist_nose_tlr\"]  = dist_ab(an, target_parts[\"lateral_right\"])\n",
    "        # feats[\"dist_nose_tt\"]  = dist_ab(an, target_parts[\"tail_tip\"])\n",
    "\n",
    "        # --- 3. ĐỊNH HƯỚNG & GÓC NHÌN (ORIENTATION & GAZE) ---\n",
    "        # Helper lấy vector cơ thể (Mũi - Đuôi/Thân)\n",
    "        def get_body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"nose\")\n",
    "            # Ưu tiên đuôi, nếu ko có thì dùng thân\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if tail is None: tail = parts_dict.get(\"neck\") # Fallback\n",
    "            \n",
    "            if head is not None and tail is not None:\n",
    "                return head - tail\n",
    "            return None\n",
    "\n",
    "        a_vec = get_body_vec(my_parts)\n",
    "        t_vec = get_body_vec(target_parts)\n",
    "\n",
    "        # A. Body Cosine: Hai con cùng chiều hay ngược chiều?\n",
    "        if a_vec is not None and t_vec is not None:\n",
    "            dot = np.sum(a_vec * t_vec, axis=1)\n",
    "            mags = np.linalg.norm(a_vec, axis=1) * np.linalg.norm(t_vec, axis=1)\n",
    "            feats[\"body_cosine\"] = pd.Series(\n",
    "                np.clip(dot / (mags + 1e-6), -1.0, 1.0), index=idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"body_cosine\"] = zero()\n",
    "\n",
    "        # B. Gaze Cosine: Tôi có đang nhìn về phía Target không?\n",
    "        # Vector ánh nhìn = Target_Pos - My_Pos = rel_vec\n",
    "        if a_vec is not None:\n",
    "            dot_gaze = np.sum(a_vec * rel_vec, axis=1)\n",
    "            mag_a = np.linalg.norm(a_vec, axis=1)\n",
    "            # dist đã tính ở bước 1\n",
    "            feats[\"gaze_cosine\"] = pd.Series(\n",
    "                np.clip(dot_gaze / (mag_a * dist + 1e-6), -1.0, 1.0),\n",
    "                index=idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"gaze_cosine\"] = zero()\n",
    "\n",
    "        # --- 4. PHÂN RÃ VẬN TỐC (VELOCITY DECOMPOSITION) - CHÌA KHÓA CHO AVOID/ESCAPE ---\n",
    "        # Vector đơn vị hướng về địch (u)\n",
    "        dist_safe = dist.copy()\n",
    "        dist_safe[dist_safe == 0] = 1e-6\n",
    "        u_vec = rel_vec / dist_safe[:, None]\n",
    "\n",
    "        # a_vel và t_vel lấy từ Context\n",
    "        a_vel, t_vel = ctx.vel, target_ctx.vel\n",
    "\n",
    "        # A. Approach Speed (Vận tốc dọc trục nối 2 con)\n",
    "        # Dương: Lao vào nhau | Âm: Chạy ra xa nhau\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1) # Target hướng ngược lại\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)\n",
    "\n",
    "        # B. Lateral Speed (Vận tốc ngang - Vuông góc trục nối)\n",
    "        # Vector chiếu: v_proj = (v . u) * u\n",
    "        a_proj = a_along[:, None] * u_vec\n",
    "        a_lat_vec = a_vel - a_proj\n",
    "        a_lat_speed = np.linalg.norm(a_lat_vec, axis=1)\n",
    "\n",
    "        feats[\"approach_speed_agent\"]  = pd.Series(a_along, index=idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_target\"] = pd.Series(t_along, index=idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_rel\"]    = pd.Series(rel_along, index=idx, dtype=\"float32\")\n",
    "        feats[\"lateral_speed_agent\"]   = pd.Series(a_lat_speed, index=idx, dtype=\"float32\")\n",
    "        return feats\n",
    "        \n",
    "\n",
    "    # --- Methods tương thích ---\n",
    "    \n",
    "    def build_pose_tensor(self, tracking: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Chuyển dữ liệu tracking (DataFrame) sang Tensor [Frames, Mice, 2] và Dict chi tiết.\n",
    "        \"\"\"\n",
    "        tracking = tracking.sort_values(\"video_frame\")\n",
    "        frames = np.sort(tracking[\"video_frame\"].unique())\n",
    "        \n",
    "        pvid = tracking.pivot(\n",
    "            index=\"video_frame\", \n",
    "            columns=[\"mouse_id\", \"bodypart\"], \n",
    "            values=[\"x\", \"y\"]\n",
    "        )\n",
    "        pvid = pvid.reorder_levels([1, 2, 0], axis=1).sort_index(axis=1).astype(\"float32\")\n",
    "        mouse_ids = list(pvid.columns.get_level_values(0).unique())\n",
    "        pos = np.full((len(frames), len(mouse_ids), 2), np.nan, dtype=np.float32)\n",
    "        per_mouse_df = {}\n",
    "        \n",
    "        for i, mid in enumerate(mouse_ids):\n",
    "            single = pvid[mid]\n",
    "            per_mouse_df[mid] = single\n",
    "            \n",
    "            if \"body_center\" in single.columns.get_level_values(0):\n",
    "                cx = single[\"body_center\"][\"x\"]\n",
    "                cy = single[\"body_center\"][\"y\"]\n",
    "            else:\n",
    "                cx = single.xs(\"x\", level=1, axis=1).mean(axis=1)\n",
    "                cy = single.xs(\"y\", level=1, axis=1).mean(axis=1)\n",
    "            \n",
    "            pos[:, i, 0] = cx.reindex(frames).values\n",
    "            pos[:, i, 1] = cy.reindex(frames).values\n",
    "            \n",
    "        return frames, mouse_ids, pos, per_mouse_df\n",
    "\n",
    "    def extract_agent_target(\n",
    "        self, \n",
    "        frames: np.ndarray, \n",
    "        mouse_ids: List[Any], \n",
    "        pos: np.ndarray, \n",
    "        agent_id: Any, \n",
    "        target_id: Any, \n",
    "        per_mouse_df: Dict = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Trích xuất đặc trưng cho cặp (Agent, Target).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            aid_idx = mouse_ids.index(agent_id)\n",
    "        except ValueError:\n",
    "            return pd.DataFrame() \n",
    "\n",
    "        # 1. Build Agent Context\n",
    "        ctx_agent = self._build_context(\n",
    "            frames, \n",
    "            pos[:, aid_idx, :], \n",
    "            per_mouse_df.get(agent_id) if per_mouse_df else None\n",
    "        )\n",
    "\n",
    "        # 2. Build Target Context\n",
    "        ctx_target = None\n",
    "        if self.cfg.use_pairwise and target_id is not None and target_id in mouse_ids:\n",
    "             tid_idx = mouse_ids.index(target_id)\n",
    "             ctx_target = self._build_context(\n",
    "                 frames, \n",
    "                 pos[:, tid_idx, :], \n",
    "                 per_mouse_df.get(target_id) if per_mouse_df else None\n",
    "             )\n",
    "\n",
    "        # 3. Run all features\n",
    "        all_data = {}\n",
    "        for func_name, func in self.feature_registry.items():\n",
    "            out_dict = func(ctx_agent, target_ctx=ctx_target)\n",
    "            all_data.update(out_dict)\n",
    "\n",
    "        df_out = pd.DataFrame(all_data, index=ctx_agent.idx)\n",
    "        df_out = df_out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        \n",
    "        return df_out.reindex(sorted(df_out.columns), axis=1)\n",
    "\n",
    "#======================================================================================\n",
    "#======================================================================================\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# (Trên Kaggle) dùng metric chính thức\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/usr/lib/mabe-f-beta\")\n",
    "from metric import score   # hàm score(submission_df, dataset_df)\n",
    "\n",
    "# =========================================================\n",
    "# 1. ĐƯỜNG DẪN & CẤU HÌNH\n",
    "# =========================================================\n",
    "\n",
    "INPUT_DIR = Path(\"/kaggle/input/MABe-mouse-behavior-detection\")\n",
    "TRAIN_TRACKING_DIR = INPUT_DIR / \"train_tracking\"\n",
    "TRAIN_ANNOTATION_DIR = INPUT_DIR / \"train_annotation\"\n",
    "TEST_TRACKING_DIR = INPUT_DIR / \"test_tracking\"\n",
    "\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "RESULTS_DIR = Path(r\"/kaggle/input/results-xgb-fe\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "INDEX_COLS = [\"video_id\", \"agent_id\", \"target_id\", \"video_frame\"]\n",
    "\n",
    "# hành vi “self” vs “pair” giống notebook (có thể chỉnh nếu muốn)\n",
    "SELF_BEHAVIORS = [\n",
    "    \"biteobject\", \"climb\", \"dig\", \"exploreobject\", \"freeze\",\n",
    "    \"genitalgroom\", \"huddle\", \"rear\", \"rest\", \"run\", \"selfgroom\",\n",
    "]\n",
    "PAIR_BEHAVIORS = [\n",
    "    \"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\",\n",
    "    \"chase\", \"chaseattack\", \"defend\", \"disengage\", \"dominance\",\n",
    "    \"dominancegroom\", \"dominancemount\", \"ejaculate\", \"escape\",\n",
    "    \"flinch\", \"follow\", \"intromit\", \"mount\", \"reciprocalsniff\",\n",
    "    \"shepherd\", \"sniff\", \"sniffbody\", \"sniffface\", \"sniffgenital\",\n",
    "    \"submit\", \"tussle\",\n",
    "]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. ĐỌC METADATA & HELPER\n",
    "# =========================================================\n",
    "\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    train_meta = pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "    return train_meta\n",
    "\n",
    "\n",
    "def get_video_params(video_id: Any, meta: pd.DataFrame) -> Tuple[float, float]:\n",
    "    \"\"\"Lấy fps, pix_per_cm cho video từ train.csv.\"\"\"\n",
    "    row = meta.loc[meta[\"video_id\"] == video_id]\n",
    "    if row.empty:\n",
    "        raise KeyError(f\"video_id={video_id} không có trong train.csv\")\n",
    "    row = row.iloc[0]\n",
    "\n",
    "    # giống notebook: cột \"frames per second\" & \"pix per cm (approx)\"\n",
    "    fps = float(row[\"frames_per_second\"])\n",
    "    pix_per_cm = float(row[\"pix_per_cm_approx\"])\n",
    "    if not np.isfinite(pix_per_cm) or pix_per_cm <= 0:\n",
    "        pix_per_cm = 1.0\n",
    "    return fps, pix_per_cm\n",
    "\n",
    "\n",
    "def load_tracking(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    \"\"\"Đọc tracking parquet → pandas (schema: video_frame, mouse_id, bodypart, x, y).\"\"\"\n",
    "    path = TRAIN_TRACKING_DIR / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    df = pd.read_parquet(path)\n",
    "    return df\n",
    "\n",
    "def load_tracking_test(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    \"\"\"Đọc tracking parquet của test → pandas.\"\"\"\n",
    "    path = INPUT_DIR / \"test_tracking\" / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "def load_annotation(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    \"\"\"Đọc annotation (agent_id, target_id, action, start_frame, stop_frame).\"\"\"\n",
    "    path = TRAIN_ANNOTATION_DIR / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists():\n",
    "        # không có label cho video này\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "        )\n",
    "    ann = pd.read_parquet(path)\n",
    "    return ann[[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. TÍNH FEATURE PER-FRAME BẰNG FEATUREEXTRACTOR\n",
    "# =========================================================\n",
    "\n",
    "# Cache: (lab, video, agent, target) -> (frames, feature_df)\n",
    "_feature_cache: Dict[Tuple[str, int, int, int], Tuple[np.ndarray, pd.DataFrame]] = {}\n",
    "\n",
    "\n",
    "def get_frame_features_for_pair(\n",
    "    lab_id: str,\n",
    "    video_id: int,\n",
    "    agent_id: int,\n",
    "    target_id: int,\n",
    "    meta: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Tính (hoặc lấy cache) feature per-frame cho 1 video + (agent, target).\n",
    "    Trả về: frames [F], features_df [F, D]\n",
    "    \"\"\"\n",
    "    key = (str(lab_id), int(video_id), int(agent_id), int(target_id))\n",
    "    if key in _feature_cache:\n",
    "        return _feature_cache[key]\n",
    "\n",
    "    fps, pix_per_cm = get_video_params(video_id, meta)\n",
    "    tracking = load_tracking(lab_id, video_id)\n",
    "\n",
    "    fe = FeatureExtractor(\n",
    "        fps=fps,\n",
    "        pix_per_cm=pix_per_cm,\n",
    "        smooth_sigma=1.0,\n",
    "        use_pairwise=True,\n",
    "    )\n",
    "\n",
    "    frames, mouse_ids, pos, per_mouse_df = fe.build_pose_tensor(tracking)\n",
    "\n",
    "    # agent/target có thể là cùng chuột (self) hoặc khác chuột (pair)\n",
    "    features_df: pd.DataFrame = fe.extract_agent_target(\n",
    "        frames=frames,\n",
    "        mouse_ids=mouse_ids,\n",
    "        pos=pos,\n",
    "        agent_id=agent_id,\n",
    "        target_id=target_id,\n",
    "        per_mouse_df=per_mouse_df,\n",
    "    )\n",
    "    # index chính là frame\n",
    "    features_df.index = frames\n",
    "\n",
    "    _feature_cache[key] = (frames, features_df)\n",
    "    return frames, features_df\n",
    "\n",
    "_feature_cache: Dict[Tuple[str, int, Any, Any], Tuple[np.ndarray, pd.DataFrame]] = {}\n",
    "\n",
    "def get_frame_features_for_pair_test(\n",
    "    lab_id: str,\n",
    "    video_id: int,\n",
    "    agent_id: Any,\n",
    "    target_id: Any,\n",
    "    test_meta: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Feature per-frame cho test (video_id, agent, target).\n",
    "    Trả về: frames [F], features_df [F, D]\n",
    "    \"\"\"\n",
    "    key = (f\"test_{lab_id}\", int(video_id), agent_id, target_id)\n",
    "    if key in _feature_cache:\n",
    "        return _feature_cache[key]\n",
    "\n",
    "    # Lấy fps, pix_per_cm_approx từ test.csv\n",
    "    row = test_meta[test_meta[\"video_id\"] == video_id].iloc[0]\n",
    "    fps = float(row[\"frames_per_second\"])\n",
    "    pix_per_cm = float(row[\"pix_per_cm_approx\"])\n",
    "    if not np.isfinite(pix_per_cm) or pix_per_cm <= 0:\n",
    "        pix_per_cm = 1.0\n",
    "\n",
    "    tracking = load_tracking_test(lab_id, video_id)\n",
    "\n",
    "    fe = FeatureExtractor(\n",
    "        fps=fps,\n",
    "        pix_per_cm=pix_per_cm,\n",
    "        smooth_sigma=1.0,\n",
    "        use_pairwise=True,\n",
    "    )\n",
    "\n",
    "    frames, mouse_ids, pos, per_mouse_df = fe.build_pose_tensor(tracking)\n",
    "\n",
    "    features_df = fe.extract_agent_target(\n",
    "        frames=frames,\n",
    "        mouse_ids=mouse_ids,\n",
    "        pos=pos,\n",
    "        agent_id=agent_id,\n",
    "        target_id=target_id,\n",
    "        per_mouse_df=per_mouse_df,\n",
    "    )\n",
    "    features_df.index = frames\n",
    "\n",
    "    _feature_cache[key] = (frames, features_df)\n",
    "    return frames, features_df\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. BUILD FRAME-LEVEL DATASET CHO 1 (lab_id, behavior)\n",
    "# =========================================================\n",
    "\n",
    "def build_frame_dataset_for_lab_behavior(\n",
    "    lab_id: str,\n",
    "    behavior: str,\n",
    "    train_meta: pd.DataFrame,\n",
    "    mode: str = \"self\",\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Xây tập frame-level (indices, features, labels) cho 1 (lab, behavior).\n",
    "\n",
    "    indices: DataFrame với cột INDEX_COLS\n",
    "    features: DataFrame per-frame features\n",
    "    labels: np.ndarray nhị phân (0/1)\n",
    "    \"\"\"\n",
    "\n",
    "    videos = (\n",
    "        train_meta[train_meta[\"lab_id\"] == lab_id][\"video_id\"]\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    index_list = []\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for video_id in videos:\n",
    "        ann = load_annotation(lab_id, video_id)\n",
    "        if ann.empty:\n",
    "            continue\n",
    "\n",
    "        # chỉ lấy annotation của behavior này\n",
    "        ann_bhv = ann[ann[\"action\"] == behavior]\n",
    "        if ann_bhv.empty:\n",
    "            continue\n",
    "\n",
    "        # các (agent, target) cần xem\n",
    "        pairs = ann_bhv[[\"agent_id\", \"target_id\"]].drop_duplicates().values.tolist()\n",
    "        for (agent_id, target_id) in pairs:\n",
    "            if mode == \"self\":\n",
    "                target_id_use = agent_id\n",
    "            else:\n",
    "                target_id_use = target_id\n",
    "\n",
    "            frames, feat_df = get_frame_features_for_pair(\n",
    "                lab_id=lab_id,\n",
    "                video_id=video_id,\n",
    "                agent_id=agent_id,\n",
    "                target_id=target_id_use,\n",
    "                meta=train_meta,\n",
    "            )\n",
    "\n",
    "            # label per-frame: frame ∈ bất kỳ [start, stop) của (agent,target,behavior)\n",
    "            ann_pair = ann_bhv[\n",
    "                (ann_bhv[\"agent_id\"] == agent_id)\n",
    "                & (ann_bhv[\"target_id\"] == target_id)\n",
    "            ]\n",
    "            if ann_pair.empty and mode == \"self\":\n",
    "                ann_pair = ann_bhv[ann_bhv[\"agent_id\"] == agent_id]\n",
    "\n",
    "            pos_frames = set()\n",
    "            for _, r in ann_pair.iterrows():\n",
    "                pos_frames.update(range(int(r[\"start_frame\"]), int(r[\"stop_frame\"])))\n",
    "\n",
    "            if len(pos_frames) == 0:\n",
    "                continue\n",
    "\n",
    "            label = np.isin(frames, list(pos_frames)).astype(\"int8\")\n",
    "            if label.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            idx_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"video_id\": video_id,\n",
    "                    \"agent_id\": agent_id,\n",
    "                    \"target_id\": target_id,\n",
    "                    \"video_frame\": frames,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            index_list.append(idx_df)\n",
    "            feature_list.append(feat_df.reset_index(drop=True))\n",
    "            label_list.append(label)\n",
    "\n",
    "    if not index_list:\n",
    "        return (\n",
    "            pd.DataFrame(columns=INDEX_COLS),\n",
    "            pd.DataFrame(),\n",
    "            np.zeros(0, dtype=\"int8\"),\n",
    "        )\n",
    "\n",
    "    indices = pd.concat(index_list, ignore_index=True)\n",
    "    features = pd.concat(feature_list, ignore_index=True)\n",
    "    labels = np.concatenate(label_list).astype(\"int8\")\n",
    "\n",
    "    assert len(indices) == len(features) == len(labels)\n",
    "\n",
    "    return indices, features, labels\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. TRAIN + OOF CHO 1 (lab_id, behavior)\n",
    "# =========================================================\n",
    "\n",
    "def tune_threshold(oof_pred: np.ndarray, y: np.ndarray) -> float:\n",
    "    ths = np.arange(0.0, 1.005, 0.005)\n",
    "    scores = [f1_score(y, (oof_pred >= th), zero_division=0) for th in ths]\n",
    "    return float(ths[int(np.argmax(scores))])\n",
    "\n",
    "#\n",
    "def train_validate_one(\n",
    "    lab_id: str,\n",
    "    behavior: str,\n",
    "    indices: pd.DataFrame,\n",
    "    features: pd.DataFrame,\n",
    "    labels: np.ndarray,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Train XGBoost binary cho 1 (lab, behavior) + lưu OOF prediction.\n",
    "    Trả về: F1 trên toàn bộ OOF (frame-level).\n",
    "    \"\"\"\n",
    "    result_dir = RESULTS_DIR / lab_id / behavior\n",
    "    result_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    n = len(labels)\n",
    "\n",
    "    if n == 0 or labels.sum() == 0:\n",
    "        oof_df = indices.copy()\n",
    "        oof_df[\"fold\"] = -1\n",
    "        oof_df[\"prediction\"] = 0.0\n",
    "        oof_df[\"predicted_label\"] = 0\n",
    "        oof_df.to_parquet(result_dir / \"oof_predictions.parquet\", index=False)\n",
    "        (result_dir / \"f1.txt\").write_text(\"0.0\\n\")\n",
    "        return 0.0\n",
    "\n",
    "    X = features.values.astype(\"float32\")\n",
    "    y = labels.astype(\"int8\")\n",
    "    groups = indices[\"video_id\"].values\n",
    "\n",
    "    folds = np.ones(n, dtype=\"int8\") * -1\n",
    "    oof_pred = np.zeros(n, dtype=\"float32\")\n",
    "    oof_label = np.zeros(n, dtype=\"int8\")\n",
    "\n",
    "    cv = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups=groups)):\n",
    "        fold_dir = result_dir / f\"fold_{fold}\"\n",
    "        fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "        X_va, y_va = X[va_idx], y[va_idx]\n",
    "\n",
    "        # scale_pos_weight\n",
    "        pos = y_tr.sum()\n",
    "        neg = len(y_tr) - pos\n",
    "        scale_pos_weight = float(neg / pos) if pos > 0 else 1.0\n",
    "\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"logloss\",\n",
    "            \"device\": \"cuda\",\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"max_depth\": 6,\n",
    "            \"min_child_weight\": 5,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"scale_pos_weight\": scale_pos_weight,\n",
    "            \"max_bin\": 64,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "\n",
    "        dtrain = xgb.QuantileDMatrix(\n",
    "            X_tr,\n",
    "            label=y_tr,\n",
    "            feature_names=features.columns.tolist(),\n",
    "            max_bin=64,\n",
    "        )\n",
    "        dvalid = xgb.DMatrix(\n",
    "            X_va,\n",
    "            label=y_va,\n",
    "            feature_names=features.columns.tolist(),\n",
    "        )\n",
    "\n",
    "        evals_result: Dict[str, Dict[str, List[float]]] = {}\n",
    "\n",
    "        early_stop = xgb.callback.EarlyStopping(\n",
    "            rounds=10, metric_name=\"logloss\", data_name=\"valid\", maximize=False\n",
    "        )\n",
    "\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=250,\n",
    "            evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "            callbacks=[early_stop],\n",
    "            evals_result=evals_result,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "\n",
    "        pred_va = model.predict(dvalid)\n",
    "        th = tune_threshold(pred_va, y_va)\n",
    "\n",
    "        folds[va_idx] = fold\n",
    "        oof_pred[va_idx] = pred_va\n",
    "        oof_label[va_idx] = (pred_va >= th).astype(\"int8\")\n",
    "\n",
    "        model.save_model(fold_dir / \"model.json\")\n",
    "        with open(fold_dir / \"threshold.txt\", \"w\") as f:\n",
    "            f.write(f\"{th}\\n\")\n",
    "\n",
    "    # lưu OOF\n",
    "    oof_df = indices.copy()\n",
    "    oof_df[\"fold\"] = folds\n",
    "    oof_df[\"prediction\"] = oof_pred\n",
    "    oof_df[\"predicted_label\"] = oof_label\n",
    "    oof_df.to_parquet(result_dir / \"oof_predictions.parquet\", index=False)\n",
    "\n",
    "    f1 = f1_score(y, oof_label, zero_division=0)\n",
    "    (result_dir / \"f1.txt\").write_text(f\"{f1:.6f}\\n\")\n",
    "    return float(f1)\n",
    "\n",
    "def load_models_for_behavior_infer(lab_id: str, behavior: str):\n",
    "    \"\"\"\n",
    "    Đọc các fold model + threshold cho (lab, behavior) từ RESULTS_DIR.\n",
    "    Dùng cho inference (test).\n",
    "    \"\"\"\n",
    "    base_dir = RESULTS_DIR / lab_id / behavior\n",
    "    if not base_dir.exists():\n",
    "        return []\n",
    "\n",
    "    models = []\n",
    "    for fold_dir in sorted(base_dir.glob(\"fold_*\")):\n",
    "        model_file = fold_dir / \"model.json\"\n",
    "        thr_file = fold_dir / \"threshold.txt\"\n",
    "        if not model_file.exists():\n",
    "            continue\n",
    "\n",
    "        booster = xgb.Booster()\n",
    "        booster.load_model(str(model_file))\n",
    "\n",
    "        if thr_file.exists():\n",
    "            thr = float(thr_file.read_text().strip())\n",
    "        else:\n",
    "            thr = 0.5\n",
    "\n",
    "        models.append((booster, thr))\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. LOOP QUA TẤT CẢ BEHAVIORS TRONG 1 LAB\n",
    "#    (train_all_labs_behaviors vẫn giữ nguyên, nhưng main\n",
    "#     sẽ filter train_meta chỉ còn 1 lab)\n",
    "# =========================================================\n",
    "\n",
    "def train_all_labs_behaviors(train_meta: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Loop qua từng lab trong train_meta (ở đây main đã filter chỉ còn 1 lab):\n",
    "      - đọc annotation của tất cả video\n",
    "      - lấy unique action xuất hiện trong lab đó\n",
    "      - train 1 model/frame-level cho từng (lab, action)\n",
    "    \"\"\"\n",
    "    labs = train_meta[\"lab_id\"].unique().tolist()\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    for lab_id in labs:\n",
    "        # tập video của lab này\n",
    "        videos = train_meta[train_meta[\"lab_id\"] == lab_id][\"video_id\"].unique().tolist()\n",
    "\n",
    "        # gom toàn bộ action thực sự có trong annotation của lab này\n",
    "        behaviors_set = set()\n",
    "        for vid in videos:\n",
    "            ann = load_annotation(lab_id, vid)\n",
    "            if ann.empty:\n",
    "                continue\n",
    "            behaviors_set.update(ann[\"action\"].unique().tolist())\n",
    "\n",
    "        behaviors = sorted(behaviors_set)\n",
    "        print(f\"\\n===== LAB {lab_id}: {len(behaviors)} behaviors =====\")\n",
    "\n",
    "        for behavior in behaviors:\n",
    "            # if behavior != \"submit\": continue\n",
    "\n",
    "            mode = \"self\" if behavior in SELF_BEHAVIORS else \"pair\"\n",
    "\n",
    "            print(f\"\\n=== LAB={lab_id} | behavior={behavior} | mode={mode} ===\")\n",
    "            indices, features, labels = build_frame_dataset_for_lab_behavior(\n",
    "                lab_id=str(lab_id),\n",
    "                behavior=behavior,\n",
    "                train_meta=train_meta,\n",
    "                mode=mode,\n",
    "            )\n",
    "            print(\n",
    "                f\"frames: {len(labels):,}, positives: {labels.sum():,}, features: \"\n",
    "                f\"{features.shape[1] if not features.empty else 0}\"\n",
    "            )\n",
    "\n",
    "            if len(labels) == 0:\n",
    "                print(\" -> skip (no samples)\")\n",
    "                continue\n",
    "\n",
    "            f1 = train_validate_one(str(lab_id), behavior, indices, features, labels)\n",
    "            elapsed = time.perf_counter() - start_time\n",
    "            print(f\" -> OOF F1 (frame-level): {f1:.3f} | elapsed={elapsed/60:.1f} min\")\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. GOM OOF PREDICTION → SEGMENT & TÍNH SCORE()\n",
    "# =========================================================\n",
    "\n",
    "def build_oof_submission_from_parquet(\n",
    "    target_lab_id: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Đọc tất cả oof_predictions.parquet trong RESULTS_DIR,\n",
    "    gom thành frame-level table rồi nối thành segment-level prediction\n",
    "    giống inference notebook (simplified).\n",
    "\n",
    "    Nếu target_lab_id != None thì chỉ lấy OOF của lab đó\n",
    "    (vd \"AdaptableSnail\").\n",
    "    \"\"\"\n",
    "    oof_files = list(RESULTS_DIR.glob(\"*/**/oof_predictions.parquet\"))\n",
    "    if not oof_files:\n",
    "        raise RuntimeError(\"Không tìm thấy OOF parquet, hãy train trước.\")\n",
    "\n",
    "    frame_preds = []\n",
    "\n",
    "    for path in oof_files:\n",
    "        # path: results_xgb_fe/lab/behavior/oof_predictions.parquet\n",
    "        parts = path.parts\n",
    "        behavior = parts[-2]\n",
    "        lab_id = parts[-3]\n",
    "\n",
    "        # chỉ lấy file thuộc lab mong muốn (nếu có)\n",
    "        if target_lab_id is not None and lab_id != target_lab_id:\n",
    "            continue\n",
    "\n",
    "        df = pd.read_parquet(path)\n",
    "        df = df[INDEX_COLS + [\"prediction\"]].copy()\n",
    "        df[\"lab_id\"] = lab_id\n",
    "        df[\"action\"] = behavior\n",
    "        frame_preds.append(df)\n",
    "\n",
    "    if not frame_preds:\n",
    "        raise RuntimeError(\n",
    "            f\"Không có OOF predictions nào cho lab_id={target_lab_id}\"\n",
    "        )\n",
    "\n",
    "    frame_df = pd.concat(frame_preds, ignore_index=True)\n",
    "\n",
    "    # sắp xếp\n",
    "    frame_df = frame_df.sort_values(\n",
    "        [\"lab_id\", \"video_id\", \"agent_id\", \"target_id\", \"action\", \"video_frame\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Convert frame-level prob -> hard label + segments\n",
    "    segments = []\n",
    "    for (lab_id, video_id, agent_id, target_id, action), group in frame_df.groupby(\n",
    "        [\"lab_id\", \"video_id\", \"agent_id\", \"target_id\", \"action\"], sort=False\n",
    "    ):\n",
    "        frames = group[\"video_frame\"].values\n",
    "        scores = group[\"prediction\"].values\n",
    "\n",
    "        # dùng một threshold fix (vd 0.5) cho demo\n",
    "        # (hoặc bạn có thể lưu threshold per (lab,behavior) và apply)\n",
    "        hard = scores >= 0.5\n",
    "\n",
    "        in_seg = False\n",
    "        start = None\n",
    "        prev_f = None\n",
    "\n",
    "        for f, h in zip(frames, hard):\n",
    "            if h and not in_seg:\n",
    "                in_seg = True\n",
    "                start = int(f)\n",
    "            elif (not h) and in_seg:\n",
    "                stop = int(prev_f + 1)  # [start, stop)\n",
    "                segments.append(\n",
    "                    {\n",
    "                        \"lab_id\": lab_id,\n",
    "                        \"video_id\": int(video_id),\n",
    "                        \"agent_id\": int(agent_id),\n",
    "                        \"target_id\": int(target_id),\n",
    "                        \"action\": action,\n",
    "                        \"start_frame\": start,\n",
    "                        \"stop_frame\": stop,\n",
    "                    }\n",
    "                )\n",
    "                in_seg = False\n",
    "            prev_f = f\n",
    "\n",
    "        if in_seg:\n",
    "            stop = int(frames[-1] + 1)\n",
    "            segments.append(\n",
    "                {\n",
    "                    \"lab_id\": lab_id,\n",
    "                    \"video_id\": int(video_id),\n",
    "                    \"agent_id\": int(agent_id),\n",
    "                    \"target_id\": int(target_id),\n",
    "                    \"action\": action,\n",
    "                    \"start_frame\": start,\n",
    "                    \"stop_frame\": stop,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not segments:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"lab_id\",\n",
    "                \"video_id\",\n",
    "                \"agent_id\",\n",
    "                \"target_id\",\n",
    "                \"action\",\n",
    "                \"start_frame\",\n",
    "                \"stop_frame\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    submission = pd.DataFrame(segments)\n",
    "    submission = submission.sort_values(\n",
    "        [\"lab_id\", \"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return submission\n",
    "\n",
    "BAD_VIDEOS = []\n",
    "\n",
    "def compute_validation_score(\n",
    "    submission: pd.DataFrame,\n",
    "    lab_id: Optional[str] = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Gọi metric `score()` chính thức trên train set.\n",
    "    Nếu lab_id != None → chỉ validate trên lab đó.\n",
    "    \"\"\"\n",
    "    # ===== THAY ĐỔI Ở ĐÂY =====\n",
    "    # Không dùng train.csv, mà phải đọc toàn bộ annotations\n",
    "    train_meta = pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "    \n",
    "    if lab_id is not None:\n",
    "        train_meta = train_meta[train_meta[\"lab_id\"] == lab_id].reset_index(drop=True)\n",
    "\n",
    "    if BAD_VIDEOS:\n",
    "        train_meta = train_meta[~train_meta[\"video_id\"].isin(BAD_VIDEOS)]\n",
    "    \n",
    "    # Đọc tất cả annotation files\n",
    "    all_annotations = []\n",
    "    for _, row in train_meta.iterrows():\n",
    "        lab = row[\"lab_id\"]\n",
    "        vid = row[\"video_id\"]\n",
    "        ann = load_annotation(lab, vid)\n",
    "        if not ann.empty:\n",
    "            ann[\"lab_id\"] = lab\n",
    "            ann[\"video_id\"] = vid\n",
    "            ann[\"behaviors_labeled\"] = row[\"behaviors_labeled\"]\n",
    "            all_annotations.append(ann)\n",
    "    \n",
    "    if not all_annotations:\n",
    "        print(\"Không có annotation nào để validate!\")\n",
    "        return 0.0\n",
    "    \n",
    "    dataset = pd.concat(all_annotations, ignore_index=True)\n",
    "    \n",
    "    # Filter submission theo lab nếu cần\n",
    "    if lab_id is not None:\n",
    "        submission = submission[submission[\"lab_id\"] == lab_id].reset_index(drop=True)\n",
    "    \n",
    "    # ===== GỌI METRIC =====\n",
    "    s = score(dataset, submission, row_id_column_name=\"row_id\")\n",
    "\n",
    "    print(\n",
    "        f\"Official validation score\"\n",
    "        f\"{' (lab=' + lab_id + ')' if lab_id is not None else ''}: {s:.6f}\"\n",
    "    )\n",
    "    return float(s)\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. MAIN\n",
    "# =========================================================\n",
    "def str_to_mouse_id(s: str) -> int:\n",
    "    if s == \"self\":\n",
    "        return -1\n",
    "    return int(str(s).replace(\"mouse\", \"\"))\n",
    "\n",
    "\n",
    "def predict_behaviors_for_pair(\n",
    "    lab_id: str,\n",
    "    video_id: int,\n",
    "    agent_internal_id: Any,\n",
    "    target_internal_id: Any,\n",
    "    behaviors: List[str],\n",
    "    test_meta: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Chạy inference cho 1 cặp (video, agent_internal_id, target_internal_id)\n",
    "    với list behaviors (cùng mode: all self hoặc all pair).\n",
    "    Trả về segment-level DataFrame: video_id, action, start_frame, stop_frame.\n",
    "    \"\"\"\n",
    "    if lab_id != \"JovialSwallow\": return None\n",
    "    frames, feat_df = get_frame_features_for_pair_test(\n",
    "        lab_id=lab_id,\n",
    "        video_id=video_id,\n",
    "        agent_id=agent_internal_id,\n",
    "        target_id=target_internal_id,\n",
    "        test_meta=test_meta,\n",
    "    )\n",
    "    if feat_df.empty:\n",
    "        return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "\n",
    "    feat_df = feat_df.astype(\"float32\")\n",
    "    n_frames = len(feat_df)\n",
    "\n",
    "    scores_per_behavior = {}\n",
    "    for behavior in behaviors:\n",
    "        models = load_models_for_behavior_infer(lab_id, behavior)\n",
    "        if not models:\n",
    "            continue\n",
    "\n",
    "        req_feats = models[0][0].feature_names\n",
    "        # Build X_test với đúng bộ feature của model\n",
    "        X_test = pd.DataFrame(\n",
    "            0.0,\n",
    "            index=feat_df.index,\n",
    "            columns=req_feats,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        common = list(set(req_feats) & set(feat_df.columns))\n",
    "        if common:\n",
    "            X_test[common] = feat_df[common]\n",
    "\n",
    "        dtest = xgb.DMatrix(X_test, feature_names=req_feats)\n",
    "\n",
    "        agg_scores = np.zeros(n_frames, dtype=np.float32)\n",
    "        for booster, thr in models:\n",
    "            probs = booster.predict(dtest)\n",
    "            labels = (probs >= thr).astype(np.int8)\n",
    "            agg_scores += probs * labels\n",
    "\n",
    "        agg_scores /= max(len(models), 1)\n",
    "        scores_per_behavior[behavior] = agg_scores\n",
    "\n",
    "        del dtest, X_test\n",
    "        gc.collect()\n",
    "\n",
    "    if not scores_per_behavior:\n",
    "        return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "\n",
    "    beh_list = list(scores_per_behavior.keys())\n",
    "    score_mat = np.vstack([scores_per_behavior[b] for b in beh_list]).T  # [F, B]\n",
    "\n",
    "    max_idx = score_mat.argmax(axis=1)\n",
    "    max_scores = score_mat.max(axis=1)\n",
    "    labels = np.where(max_scores == 0.0, \"none\", np.array(beh_list)[max_idx])\n",
    "\n",
    "    # frame-level → segment\n",
    "    segments = []\n",
    "    prev_lab = \"none\"\n",
    "    prev_start = None\n",
    "    prev_f = None\n",
    "\n",
    "    for f, lab in zip(frames, labels):\n",
    "        if lab != prev_lab:\n",
    "            if prev_lab != \"none\":\n",
    "                segments.append(\n",
    "                    {\n",
    "                        \"video_id\": int(video_id),\n",
    "                        \"action\": prev_lab,\n",
    "                        \"start_frame\": int(prev_start),\n",
    "                        \"stop_frame\": int(prev_f + 1),\n",
    "                    }\n",
    "                )\n",
    "            prev_lab = lab\n",
    "            prev_start = f\n",
    "        prev_f = f\n",
    "\n",
    "    if prev_lab != \"none\":\n",
    "        segments.append(\n",
    "            {\n",
    "                \"video_id\": int(video_id),\n",
    "                \"action\": prev_lab,\n",
    "                \"start_frame\": int(prev_start),\n",
    "                \"stop_frame\": int(prev_f + 1),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not segments:\n",
    "        return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "\n",
    "    return pd.DataFrame(segments)\n",
    "\n",
    "\n",
    "\n",
    "target_lab = \"JovialSwallow\"\n",
    "print(f\"Đọc test.csv cho lab {target_lab} ...\")\n",
    "test_meta = pd.read_csv(INPUT_DIR / \"test.csv\")\n",
    "test_meta = test_meta[test_meta[\"lab_id\"] == target_lab].reset_index(drop=True)\n",
    "\n",
    "# Lấy danh sách behavior đã train (thư mục con trong RESULTS_DIR/AdaptableSnail)\n",
    "lab_result_dir = RESULTS_DIR / target_lab\n",
    "if lab_result_dir.exists():\n",
    "    trained_behaviors = sorted(\n",
    "        [p.name for p in lab_result_dir.iterdir() if p.is_dir()]\n",
    "    )\n",
    "else:\n",
    "    trained_behaviors = []\n",
    "\n",
    "self_behaviors_in_lab = [b for b in trained_behaviors if b in SELF_BEHAVIORS]\n",
    "pair_behaviors_in_lab = [b for b in trained_behaviors if b in PAIR_BEHAVIORS]\n",
    "\n",
    "print(\"Behaviors (self) dùng để predict:\", self_behaviors_in_lab)\n",
    "print(\"Behaviors (pair) dùng để predict:\", pair_behaviors_in_lab)\n",
    "\n",
    "all_segments = []\n",
    "\n",
    "# Loop từng video test của lab\n",
    "for video_id in sorted(test_meta[\"video_id\"].unique()):\n",
    "    print(f\"Predict video_id={video_id} ...\")\n",
    "\n",
    "    tracking = load_tracking_test(target_lab, video_id)\n",
    "    mouse_ids_internal = sorted(tracking[\"mouse_id\"].unique().tolist())\n",
    "\n",
    "    # Map internal mouse_id -> string để đưa vào submission\n",
    "    def to_submit_id(mid):\n",
    "        s = str(mid)\n",
    "        return s if s.startswith(\"mouse\") else f\"mouse{s}\"\n",
    "\n",
    "    # SELF behaviors: agent == target (self)\n",
    "    if self_behaviors_in_lab:\n",
    "        for mid in mouse_ids_internal:\n",
    "            seg_df = predict_behaviors_for_pair(\n",
    "                lab_id=target_lab,\n",
    "                video_id=video_id,\n",
    "                agent_internal_id=mid,\n",
    "                target_internal_id=mid,  # self\n",
    "                behaviors=self_behaviors_in_lab,\n",
    "                test_meta=test_meta,\n",
    "            )\n",
    "            if not seg_df.empty:\n",
    "                seg_df[\"agent_id\"] = to_submit_id(mid)\n",
    "                seg_df[\"target_id\"] = \"self\"\n",
    "                all_segments.append(seg_df)\n",
    "\n",
    "    # PAIR behaviors: mọi cặp agent != target\n",
    "    if pair_behaviors_in_lab and len(mouse_ids_internal) > 1:\n",
    "        for agent_internal, target_internal in itertools.permutations(\n",
    "            mouse_ids_internal, 2\n",
    "        ):\n",
    "            seg_df = predict_behaviors_for_pair(\n",
    "                lab_id=target_lab,\n",
    "                video_id=video_id,\n",
    "                agent_internal_id=agent_internal,\n",
    "                target_internal_id=target_internal,\n",
    "                behaviors=pair_behaviors_in_lab,\n",
    "                test_meta=test_meta,\n",
    "            )\n",
    "            if not seg_df.empty:\n",
    "                seg_df[\"agent_id\"] = to_submit_id(agent_internal)\n",
    "                seg_df[\"target_id\"] = to_submit_id(target_internal)\n",
    "                all_segments.append(seg_df)\n",
    "\n",
    "# Gộp tất cả segments → submission.csv\n",
    "# Gộp tất cả segments → submission2.csv\n",
    "if all_segments:\n",
    "    submission5 = pd.concat(all_segments, ignore_index=True)\n",
    "    submission5 = submission5[\n",
    "        [\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "    ]\n",
    "    submission5 = submission5.sort_values(\n",
    "        [\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\"]\n",
    "    ).reset_index(drop=True)\n",
    "else:\n",
    "    # DataFrame rỗng, KHÔNG dummy row\n",
    "    submission5 = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"video_id\",\n",
    "            \"agent_id\",\n",
    "            \"target_id\",\n",
    "            \"action\",\n",
    "            \"start_frame\",\n",
    "            \"stop_frame\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Thêm row_id (kể cả khi rỗng)\n",
    "submission5.insert(0, \"row_id\", np.arange(len(submission5), dtype=np.int64))\n",
    "\n",
    "sub_path = WORKING_DIR / \"submission5.csv\"\n",
    "submission5.to_csv(sub_path, index=False)\n",
    "print(f\"Saved JovialSwallow submission to {sub_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a57fff",
   "metadata": {
    "papermill": {
     "duration": 0.019143,
     "end_time": "2025-12-13T17:39:17.614968",
     "exception": false,
     "start_time": "2025-12-13T17:39:17.595825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PleasantMeerkat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "818e1efc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:17.653025Z",
     "iopub.status.busy": "2025-12-13T17:39:17.652455Z",
     "iopub.status.idle": "2025-12-13T17:39:17.766532Z",
     "shell.execute_reply": "2025-12-13T17:39:17.765866Z"
    },
    "papermill": {
     "duration": 0.134384,
     "end_time": "2025-12-13T17:39:17.767677",
     "exception": false,
     "start_time": "2025-12-13T17:39:17.633293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "\n",
    "# 1) Xóa mọi thứ trong /kaggle/working trừ .csv\n",
    "for path in WORKING_DIR.iterdir():\n",
    "    # giữ lại file .csv\n",
    "    if path.is_file() and path.suffix == \".csv\":\n",
    "        continue\n",
    "\n",
    "    if path.is_file():\n",
    "        try:\n",
    "            path.unlink()\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove file {path}: {e}\")\n",
    "    elif path.is_dir():\n",
    "        try:\n",
    "            shutil.rmtree(path, ignore_errors=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove dir {path}: {e}\")\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14bc8b47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:17.808218Z",
     "iopub.status.busy": "2025-12-13T17:39:17.807956Z",
     "iopub.status.idle": "2025-12-13T17:39:17.956414Z",
     "shell.execute_reply": "2025-12-13T17:39:17.955457Z"
    },
    "papermill": {
     "duration": 0.170843,
     "end_time": "2025-12-13T17:39:17.957632",
     "exception": false,
     "start_time": "2025-12-13T17:39:17.786789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đọc test.csv cho lab PleasantMeerkat ...\n",
      "Behaviors (self) dùng để predict: []\n",
      "Behaviors (pair) dùng để predict: ['attack', 'chase', 'escape', 'follow']\n",
      "Saved PleasantMeerkat submission to /kaggle/working/submission6.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "np.seterr(invalid=\"ignore\", divide=\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    \"\"\"\n",
    "    Chứa cấu hình tham số (Hyperparameters).\n",
    "    \"\"\"\n",
    "    fps: float = 30.0\n",
    "    pix_per_cm: float = 1.0\n",
    "    smooth_sigma: float = 1.0\n",
    "    use_pairwise: bool = True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. AGENT CONTEXT\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class AgentContext:\n",
    "    \"\"\"\n",
    "    Container chứa dữ liệu đã tiền xử lý của một con chuột.\n",
    "    Giúp tránh việc tính toán lại vận tốc/gia tốc nhiều lần.\n",
    "    \"\"\"\n",
    "    idx: pd.Index          # Index frame\n",
    "    pos: np.ndarray        # [F, 2] cm\n",
    "    vel: np.ndarray        # [F, 2] cm/s\n",
    "    speed: np.ndarray      # [F, 1] cm/s\n",
    "    acc: np.ndarray        # [F, 2] cm/s^2\n",
    "    \n",
    "    cx: pd.Series          # Series tọa độ X (để dùng rolling)\n",
    "    cy: pd.Series          # Series tọa độ Y\n",
    "    speed_series: pd.Series # Series tốc độ\n",
    "    \n",
    "    raw_df: Optional[pd.DataFrame] = None # Dữ liệu gốc các bộ phận \n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. FEATURE EXTRACTOR\n",
    "# =============================================================================\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Class trích xuất đặc trưng hành vi từ dữ liệu tracking.\n",
    "    \"\"\"\n",
    "    def __init__(self, fps: float, pix_per_cm: float, smooth_sigma: float = 1.0, use_pairwise: bool = True):\n",
    "        # Map tham số từ init vào Config\n",
    "        self.cfg = FeatureConfig(\n",
    "            fps=float(fps), \n",
    "            pix_per_cm=float(pix_per_cm), \n",
    "            smooth_sigma=smooth_sigma,\n",
    "            use_pairwise=use_pairwise\n",
    "        )\n",
    "        \n",
    "        # Đăng ký các hàm feature sẽ chạy\n",
    "        self.feature_registry = {\n",
    "            \"kinematics\": self._feat_basic_kinematics,\n",
    "            \"multiscale\": self._feat_multiscale,\n",
    "            \"long_range\": self._feat_long_range,\n",
    "            \"cumulative\": self._feat_cumulative,\n",
    "            \"curvature\": self._feat_curvature,\n",
    "            \"speed_asym\": self._feat_speed_asym,\n",
    "            \"gauss_shift\": self._feat_gauss_shift,\n",
    "            \"avoid\": self._feat_avoidance_trajectory,\n",
    "            \"pose\": self._feat_pose_shape,\n",
    "            \"a\": self._feat_follow_pattern,\n",
    "            \"b\": self._feat_shortburst_social,\n",
    "            \"pairwise\": self._feat_pairwise\n",
    "        }\n",
    "\n",
    "    # --- Helpers ---\n",
    "    def _scale(self, n_frames_30fps: int) -> int:\n",
    "        \"\"\"Quy đổi số frame từ chuẩn 30fps sang fps thực tế của video.\"\"\"\n",
    "        return max(1, int(round(n_frames_30fps * self.cfg.fps / 30.0)))\n",
    "\n",
    "    def _to_cm(self, arr):\n",
    "        \"\"\"Chuyển pixel -> cm.\"\"\"\n",
    "        return arr / self.cfg.pix_per_cm\n",
    "\n",
    "    def _smooth(self, x):\n",
    "        \"\"\"Làm mượt dữ liệu bằng Gaussian filter.\"\"\"\n",
    "        if self.cfg.smooth_sigma is None or x.shape[0] < 3: return x\n",
    "        if np.all(np.isnan(x)): return x\n",
    "        return gaussian_filter1d(x, sigma=self.cfg.smooth_sigma, axis=0, mode=\"nearest\")\n",
    "\n",
    "    def _forward_fill_nan(self, pos):\n",
    "        \"\"\"\n",
    "        Điền dữ liệu thiếu (NaN) bằng giá trị hợp lệ trước đó (Forward Fill).\n",
    "        \"\"\"\n",
    "        if np.all(np.isnan(pos)):\n",
    "            return np.zeros_like(pos)\n",
    "\n",
    "        pos_ffill = pos.copy()\n",
    "        mask = np.any(~np.isnan(pos_ffill), axis=1)\n",
    "        if not mask.any():\n",
    "            return np.zeros_like(pos_ffill)\n",
    "\n",
    "        valid_idx = np.where(mask)[0]\n",
    "        first, last = valid_idx[0], valid_idx[-1]\n",
    "        pos_ffill[:first] = pos_ffill[first]\n",
    "        pos_ffill[last + 1:] = pos_ffill[last]\n",
    "        df_temp = pd.DataFrame(pos_ffill)\n",
    "        df_temp = df_temp.ffill()\n",
    "        return df_temp.to_numpy()\n",
    "    \n",
    "    def _speed_series(self, cx: pd.Series, cy: pd.Series) -> pd.Series:\n",
    "        dx = cx.diff()\n",
    "        dy = cy.diff()\n",
    "        v = np.hypot(dx, dy).fillna(0.0) * self.cfg.fps\n",
    "        return v.astype(\"float32\")\n",
    "    \n",
    "    def _roll_future_mean(self, s: pd.Series, w: int, min_p: int = 1) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).mean().iloc[::-1]\n",
    "\n",
    "    def _roll_future_var(self, s: pd.Series, w: int, min_p: int = 2) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).var().iloc[::-1]\n",
    "\n",
    "    # --- Core Logic ---\n",
    "    def _compute_kinematics(self, pos_px: np.ndarray):\n",
    "        \"\"\"\n",
    "        Tính toán vật lý cơ bản: Pos(cm), Vel, Speed, Acc.\n",
    "        Input: Array [Frames, 2] (pixel).\n",
    "        Output: Tuple (pos_cm, vel, speed, acc).\n",
    "        \"\"\"\n",
    "        pos_ffill = self._forward_fill_nan(pos_px)\n",
    "        pos_cm = self._to_cm(pos_ffill.astype(np.float32))\n",
    "        pos_cm = self._smooth(pos_cm)                                               # [F, 2]\n",
    "\n",
    "        dt = 1.0 / self.cfg.fps\n",
    "        vel = np.zeros_like(pos_cm, dtype=np.float32)\n",
    "        vel[1:] = (pos_cm[1:] - pos_cm[:-1]) / dt                                   # [F, 2: (vx, vy)]\n",
    "        speed = np.linalg.norm(vel, axis=1, keepdims=True).astype(np.float32)       # [F, 1]\n",
    "\n",
    "        acc = np.zeros_like(pos_cm, dtype=np.float32)                          \n",
    "        acc[1:] = (vel[1:] - vel[:-1]) / dt                                         # [F, 2:(ax, ay)]\n",
    "        return pos_cm.astype(np.float32), vel, speed, acc\n",
    "\n",
    "    def _build_context(self, frames, pos_px, mouse_df=None) -> AgentContext:\n",
    "        \"\"\"\n",
    "        Tạo AgentContext chứa đầy đủ thông tin vật lý của 1 con chuột.\n",
    "        \"\"\"\n",
    "        p, v, s, a = self._compute_kinematics(pos_px)\n",
    "        idx = pd.Index(frames, name=\"frame\")\n",
    "        \n",
    "        return AgentContext(\n",
    "            idx=idx, pos=p, vel=v, speed=s, acc=a, \n",
    "            cx=pd.Series(p[:, 0], index=idx), \n",
    "            cy=pd.Series(p[:, 1], index=idx), \n",
    "            speed_series=pd.Series(s[:, 0], index=idx), \n",
    "            raw_df=mouse_df\n",
    "        )\n",
    "\n",
    "    # --- Feature Modules ---\n",
    "    def _feat_basic_kinematics(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Lấy các giá trị thô: tọa độ x, y, vận tốc vx, vy, tốc độ, gia tốc ax, ay.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"a_x\": ctx.pos[:, 0], \"a_y\": ctx.pos[:, 1],\n",
    "            \"a_vx\": ctx.vel[:, 0], \"a_vy\": ctx.vel[:, 1],\n",
    "            \"a_speed\": ctx.speed[:, 0],\n",
    "            \"a_ax\": ctx.acc[:, 0], \"a_ay\": ctx.acc[:, 1]\n",
    "        }\n",
    "\n",
    "    def _feat_multiscale(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tính tốc độ trung bình (Mean) và độ lệch chuẩn (Std) ở đa mức thời gian.\n",
    "        Feature 'sp_ratio' đo độ bùng nổ (Burstiness).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        speed = ctx.speed_series\n",
    "        frame_scales = [10, 40, 160]\n",
    "        for scale in frame_scales:\n",
    "            ws = self._scale(scale)\n",
    "            if len(speed) >= ws:\n",
    "                roller = speed.rolling(ws, min_periods=max(1, ws//4), center=True)\n",
    "                feats[f\"sp_m{scale}\"] = roller.mean().astype(\"float32\")\n",
    "                feats[f\"sp_s{scale}\"] = roller.std().astype(\"float32\")\n",
    "        feats[f\"sp_ratio\"] = feats[\"sp_m10\"] / (feats[\"sp_m160\"] + 1e-6)\n",
    "        return feats \n",
    "\n",
    "    \n",
    "        \n",
    "    def _feat_long_range(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng ngữ cảnh dài hạn:\n",
    "        - x_ml, y_ml: Vị trí trung bình trong quá khứ.\n",
    "        - sp_pct: Xếp hạng (percentile) của tốc độ hiện tại so với quá khứ.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        speed = ctx.speed_series\n",
    "\n",
    "        for window in [120, 240]:\n",
    "            ws = self._scale(window)\n",
    "            if len(ctx.cx) >= ws:\n",
    "                feats[f\"x_ml{window}\"] = ctx.cx.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "                feats[f\"y_ml{window}\"] = ctx.cy.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "\n",
    "        for span in [60, 120]:\n",
    "            s = self._scale(span)\n",
    "            feats[f\"x_e{span}\"] = ctx.cx.ewm(span=s, min_periods=1).mean()\n",
    "            feats[f\"y_e{span}\"] = ctx.cy.ewm(span=s, min_periods=1).mean()\n",
    "\n",
    "        for window in [60, 120]:\n",
    "            ws = self._scale(window)\n",
    "            if len(speed) >= ws:\n",
    "                feats[f\"sp_pct{window}\"] = speed.rolling(\n",
    "                    ws, min_periods=max(5, ws // 6), center=True\n",
    "                ).rank(pct=True)\n",
    "        return feats\n",
    "    \n",
    "\n",
    "    def _feat_curvature(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        feats = {}\n",
    "\n",
    "        vel_x, vel_y = ctx.vel[:, 0], ctx.vel[:, 1]\n",
    "        acc_x, acc_y = ctx.acc[:, 0], ctx.acc[:, 1]\n",
    "        cross_prod = vel_x * acc_y - vel_y * acc_x\n",
    "        vel_mag = np.sqrt(vel_x**2 + vel_y**2)\n",
    "        moving_mask = vel_mag > 2.0\n",
    "        vel_mag_safe = np.maximum(vel_mag, 0.1 / self.cfg.fps)\n",
    "        raw_curv = cross_prod / (vel_mag_safe**3)\n",
    "        raw_curv = np.where(moving_mask, raw_curv, 0.0)\n",
    "        min_turn_radius_cm = 0.5\n",
    "        max_k = 1.0 / min_turn_radius_cm\n",
    "        raw_curv = np.clip(raw_curv, -max_k, max_k)\n",
    "        abs_curv = np.abs(raw_curv)\n",
    "        abs_curv_series = pd.Series(abs_curv, index=ctx.idx)\n",
    "\n",
    "        for w in [30, 60]:\n",
    "            ws = self._scale(w)\n",
    "            min_p = max(ws // 3, 1)\n",
    "            feats[f\"curv_mean_{w}\"] = abs_curv_series.rolling(ws, min_periods=min_p).mean()\n",
    "\n",
    "        angle = np.arctan2(vel_y, vel_x)\n",
    "        angle_series = pd.Series(angle, index=ctx.idx)\n",
    "        angle_change = np.abs(angle_series.diff().fillna(0.0))\n",
    "        angle_change = np.where(angle_change > np.pi, 2 * np.pi - angle_change, angle_change)\n",
    "        angle_change_series = pd.Series(angle_change, index=ctx.idx)\n",
    "        angle_change_series = pd.Series(np.where(moving_mask, angle_change_series, 0.0), index=ctx.idx)\n",
    "\n",
    "        ws = self._scale(30)\n",
    "        feats[\"turn_rate_30\"] = angle_change_series.rolling(ws, min_periods=max(ws // 3, 1)).sum()\n",
    "\n",
    "        return feats\n",
    "    \n",
    "    def _feat_cumulative(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tổng quãng đường di chuyển trong một khoảng thời gian dài xung quanh frame hiện tại.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        L = max(1, self._scale(180))\n",
    "        step = np.hypot(ctx.cx.diff(), ctx.cy.diff()).fillna(0.0)\n",
    "        path = step.rolling(2 * L + 1, min_periods=max(5, L // 6), center=True).sum()\n",
    "        feats[\"path_cum180\"] =  path.fillna(0.0).astype(\"float32\")\n",
    "        return feats\n",
    "\n",
    "    def _feat_speed_asym(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Bất đối xứng tốc độ (Tương lai - Quá khứ).\n",
    "        \"\"\"\n",
    "        w = max(3, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        v_past = v.rolling(w, min_periods=1).mean()\n",
    "        v_fut = self._roll_future_mean(v, w, min_p=1)\n",
    "        return {\"spd_asym_1s\": (v_fut - v_past).fillna(0.0)}\n",
    "    \n",
    "    def _feat_gauss_shift(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Độ lệch Gaussian (KL Divergence) giữa quá khứ và tương lai.\n",
    "        Đo lường sự thay đổi trạng thái thống kê.\n",
    "        \"\"\"\n",
    "        w = max(5, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        mu_p = v.rolling(w, min_periods=1).mean()\n",
    "        va_p = v.rolling(w, min_periods=1).var().clip(lower=1e-6)\n",
    "        mu_f = self._roll_future_mean(v, w, min_p=1)\n",
    "        va_f = self._roll_future_var(v, w, min_p=1).clip(lower=1e-6)\n",
    "\n",
    "        kl_pf = 0.5 * (\n",
    "            (va_p / va_f) + ((mu_f - mu_p) ** 2) / va_f - 1.0 + np.log(va_f / va_p)\n",
    "        )\n",
    "        kl_fp = 0.5 * (\n",
    "            (va_f / va_p) + ((mu_p - mu_f) ** 2) / va_p - 1.0 + np.log(va_p / va_f)\n",
    "        )\n",
    "        return {\n",
    "            \"spd_symkl_1s\": (kl_pf + kl_fp).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        }\n",
    "\n",
    "    def _feat_avoidance_trajectory(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Tính toán quỹ đạo né tránh:\n",
    "        1. Relative Heading: Góc di chuyển so với hướng tới đối thủ.\n",
    "        2. Future Distance Gain: Dự báo xem hành động này có giúp chuột ra xa đối thủ trong tương lai không.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        if target_ctx is None: \n",
    "            return feats\n",
    "\n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        # Góc hướng tới địch (Angle to Target)\n",
    "        angle_to_target = np.arctan2(rel_vec[:, 1], rel_vec[:, 0])\n",
    "        \n",
    "        # Góc di chuyển của Tôi (My Heading)\n",
    "        my_heading = np.arctan2(ctx.vel[:, 1], ctx.vel[:, 0])\n",
    "        \n",
    "        # Độ lệch góc (Absolute Difference)\n",
    "        # Cần xử lý wrap góc (ví dụ: lệch giữa 179 độ và -179 độ là 2 độ chứ ko phải 358)\n",
    "        diff = np.abs(angle_to_target - my_heading)\n",
    "        diff = np.minimum(diff, 2*np.pi - diff) # Chuẩn hóa về [0, pi]\n",
    "        \n",
    "        # Feature: Cosine của góc lệch\n",
    "        # 1.0 (0 độ) -> Lao vào\n",
    "        # 0.0 (90 độ) -> AVOID (Lách ngang)\n",
    "        # -1.0 (180 độ) -> Escape\n",
    "        feats[\"heading_rel_cos\"] = pd.Series(np.cos(diff), index=idx, dtype=\"float32\")\n",
    "        \n",
    "        # Feature: Góc lệch tuyệt đối (đổi ra độ cho dễ hình dung nếu cần, ở đây để rad)\n",
    "        feats[\"heading_rel_abs\"] = pd.Series(diff, index=idx, dtype=\"float32\")\n",
    "\n",
    "\n",
    "        # --- 2. FUTURE DISTANCE GAIN (Hiệu quả tránh né) ---\n",
    "        # \"Sau 15 frame (0.5s) hoặc 30 frame (1s), mình có xa nó ra không?\"\n",
    "        \n",
    "        dist_now = np.linalg.norm(rel_vec, axis=1)\n",
    "        s_dist = pd.Series(dist_now, index=idx)\n",
    "        \n",
    "        scales = [15, 30] # 0.5s và 1s\n",
    "        for w in scales:\n",
    "            ws = self._scale(w)\n",
    "            \n",
    "            # Lấy khoảng cách ở tương lai (shift ngược lên)\n",
    "            # s.shift(-ws) là giá trị của t + ws\n",
    "            dist_future = s_dist.shift(-ws)\n",
    "            gain = dist_future - s_dist\n",
    "            \n",
    "            feats[f\"dist_gain_{w}f\"] = gain.fillna(0.0).astype(\"float32\")\n",
    "\n",
    "        return feats\n",
    "    \n",
    "    def _extract_part(self, ctx: AgentContext, part: str) -> Optional[np.ndarray]:\n",
    "        if ctx.raw_df is None: return None\n",
    "        if part not in ctx.raw_df.columns.get_level_values(0): return None\n",
    "        try:\n",
    "            sub_df = ctx.raw_df.xs(part, axis=1, level=0)[[\"x\", \"y\"]].reindex(ctx.idx)\n",
    "        except KeyError: return None\n",
    "        raw = sub_df.to_numpy()\n",
    "        raw = self._forward_fill_nan(raw)\n",
    "        cm = self._to_cm(raw.astype(np.float32))\n",
    "        return self._smooth(cm)\n",
    "    \n",
    "    def _extract_parts_dict(self, ctx: AgentContext, parts: List[str] = None) -> Dict[str, Optional[np.ndarray]]:\n",
    "        out = {}\n",
    "        for p in parts:\n",
    "            out[p] = self._extract_part(ctx, p)\n",
    "        return out\n",
    "        \n",
    "    def _feat_pose_shape(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Placeholder cho các đặc trưng hình dáng (Elongation, Body Angle...).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "\n",
    "        def zero(): return pd.Series(0.0, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        def dist(k1, k2):\n",
    "            p1, p2 = parts.get(k1), parts.get(k2)\n",
    "            if p1 is None or p2 is None: return zero()\n",
    "            d = np.linalg.norm(p1 - p2, axis=1)\n",
    "            return pd.Series(d, index=ctx.idx, dtype=\"float32\")\n",
    "        \n",
    "        def body_angle():\n",
    "            if parts.get(\"nose\") is None: return zero()\n",
    "            if parts.get(\"body_center\") is None: return zero()\n",
    "            if parts.get(\"tail_base\") is None: return zero()\n",
    "\n",
    "            v1 = parts.get(\"nose\") - parts.get(\"body_center\")\n",
    "            v2 = parts.get(\"tail_base\") - parts.get(\"body_center\")\n",
    "            dot_product = np.sum(v1 * v2, axis=1)\n",
    "            mag = np.linalg.norm(v1, axis=1) * np.linalg.norm(v2, axis=1)\n",
    "            cos_angle = np.clip(dot_product / (mag + 1e-6), -1.0, 1.0).astype(\"float32\")\n",
    "            return cos_angle\n",
    "        \n",
    "        def elongation():\n",
    "            if parts.get(\"nose\")          is None: return zero()\n",
    "            if parts.get(\"tail_base\")     is None: return zero()\n",
    "            if parts.get(\"lateral_left\")  is None: return zero()\n",
    "            if parts.get(\"lateral_right\") is None: return zero()\n",
    "\n",
    "            d1 = dist(\"nose\", \"tail_base\")\n",
    "            d2 = dist(\"lateral_left\", \"lateral_right\")\n",
    "            elongation = d1 / (d2 + 1e-6).astype(\"float32\")\n",
    "            return elongation\n",
    "\n",
    "        \n",
    "        \n",
    "        def vel(part: str, n_frames_30fps: int) -> Dict:\n",
    "            part_pos = self._extract_part(ctx, part)\n",
    "            if part_pos is None: return zero()\n",
    "            \n",
    "            s_x = pd.Series(part_pos[:, 0], index=ctx.idx)\n",
    "            s_y = pd.Series(part_pos[:, 1], index=ctx.idx)\n",
    "            raw_speed = self._speed_series(s_x, s_y)\n",
    "\n",
    "            ws = self._scale(n_frames_30fps)\n",
    "            val = raw_speed.rolling(ws, min_periods=1, center=True).mean()\n",
    "            return val.astype(\"float32\")\n",
    "\n",
    "\n",
    "        target_parts = [\"nose\", \"body_center\", \"tail_base\", \n",
    "                        \"ear_left\", \"ear_right\", \n",
    "                        \"lateral_left\", \"lateral_right\"]\n",
    "        \n",
    "        parts = self._extract_parts_dict(ctx, target_parts)\n",
    "\n",
    "        # feats[\"a_body_width\"]                = dist(\"lateral_left\", \"lateral_right\")\n",
    "        # feats[\"aa_nose_bodycenter_dist\"]     = dist(\"nose\", \"body_center\")\n",
    "        # feats[\"aa_nose_tailbase_dist\"]       = dist(\"nose\", \"tail_base\")\n",
    "        # feats[\"aa_bodycenter_tailbase_dist\"] = dist(\"body_center\", \"tail_base\")\n",
    "        \n",
    "        # feats[\"aa_bodycenter_ear_l_dist\"]    = dist(\"body_center\", \"ear_left\")\n",
    "        # feats[\"aa_bodycenter_ear_r_dist\"]    = dist(\"body_center\", \"ear_right\")\n",
    "        # feats[\"aa_bodycenter_lateral_l_dist\"]= dist(\"body_center\", \"lateral_left\")\n",
    "        # feats[\"aa_bodycenter_lateral_r_dist\"]= dist(\"body_center\", \"lateral_right\")\n",
    "        \n",
    "        feats[\"a_body_angle\"]                = body_angle()\n",
    "        # feats[\"a_elongation\"]                = elongation()\n",
    "        feats[\"a_tail_base_vel_500ms\"]       = vel(\"tail_base\", 15)\n",
    "        feats[\"a_tail_base_vel_1000ms\"]      = vel(\"tail_base\", 30)\n",
    "        feats[\"a_tail_base_vel_2000ms\"]      = vel(\"tail_base\", 60)\n",
    "        feats[\"a_tail_base_vel_3000ms\"]      = vel(\"tail_base\", 90)\n",
    "        feats[\"a_nose_vel_500ms\"]            = vel(\"nose\", 15)\n",
    "        feats[\"a_nose_vel_1000ms\"]           = vel(\"nose\", 30)\n",
    "        feats[\"a_nose_vel_2000ms\"]           = vel(\"nose\", 60)\n",
    "        feats[\"a_nose_vel_3000ms\"]           = vel(\"nose\", 90)\n",
    "        feats[\"a_ear_right_vel_500ms\"]       = vel(\"ear_right\", 15)\n",
    "        feats[\"a_ear_right_vel_1000ms\"]      = vel(\"ear_right\", 30)\n",
    "        feats[\"a_ear_right_vel_2000ms\"]      = vel(\"ear_right\", 60)\n",
    "        feats[\"a_ear_right_vel_3000ms\"]      = vel(\"ear_right\", 90)\n",
    "        # len_1 = dist(\"tail_base\", \"tail_midpoint\")\n",
    "        # len_2 = dist(\"tail_midpoint\", \"tail_tip\")\n",
    "        # len_full = dist(\"tail_base\", \"tail_tip\")\n",
    "        # feats[\"tail_curl\"] = ((len_1 + len_2) / (len_full + 1e-6)).astype(\"float32\")\n",
    "        return feats\n",
    "\n",
    "    def _feat_shortburst_social(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Short-burst social features (10–30 frames) đặc biệt cho attack / chase / escape.\n",
    "        Chỉ dùng được khi có target_ctx.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- Lấy lại vài quantity cơ bản từ pairwise/avoidance ---\n",
    "        # vector Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # unit vector\n",
    "        rel_dist_safe = np.where(rel_dist == 0, 1e-6, rel_dist)\n",
    "        u_vec = rel_vec / rel_dist_safe[:, None]\n",
    "    \n",
    "        # velocity dọc trục nối (approach speed)\n",
    "        a_vel = ctx.vel\n",
    "        t_vel = target_ctx.vel\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)                # +: lao vào target\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1)             # +: target lao vào agent\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)    # +: lại gần nhau\n",
    "    \n",
    "        a_along_s = pd.Series(a_along, index=idx, dtype=\"float32\")\n",
    "        t_along_s = pd.Series(t_along, index=idx, dtype=\"float32\")\n",
    "        rel_along_s = pd.Series(rel_along, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # speed agent / target\n",
    "        a_speed = ctx.speed_series\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\"\n",
    "        )\n",
    "    \n",
    "        # heading_rel_cos ~ escape / approach\n",
    "        # vector body của agent\n",
    "        # (reuse idea từ _feat_pairwise)\n",
    "        # head ~ nose, tail ~ tail_base/body_center\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\", \"body_center\"])\n",
    "        head_a = parts_a.get(\"nose\")\n",
    "        tail_a = parts_a.get(\"tail_base\") if parts_a.get(\"tail_base\") is not None else parts_a.get(\"body_center\")\n",
    "    \n",
    "        if head_a is not None and tail_a is not None:\n",
    "            body_vec_a = head_a - tail_a\n",
    "            dot = np.sum(body_vec_a * rel_vec, axis=1)\n",
    "            mag = np.linalg.norm(body_vec_a, axis=1) * rel_dist_safe\n",
    "            heading_cos = np.clip(dot / (mag + 1e-6), -1.0, 1.0)\n",
    "            heading_cos_s = pd.Series(heading_cos, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            heading_cos_s = zero()\n",
    "    \n",
    "        # --- Rolling window 10, 20, 30 frames (ở fps gốc) ---\n",
    "        for w30 in [10, 20]:\n",
    "            ws = self._scale(w30)\n",
    "            min_p = max(1, ws // 3)\n",
    "    \n",
    "            # Attack-like: approach mạnh, khoảng cách giảm nhanh\n",
    "            feats[f\"sb_att_approach_mean_{w30}\"] = a_along_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_att_rel_along_mean_{w30}\"] = rel_along_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_att_dist_delta_{w30}\"] = (rel_dist_s - rel_dist_s.shift(ws)).fillna(0.0)\n",
    "    \n",
    "            # Chase-like: agent & target đều nhanh, dist tương đối nhỏ\n",
    "            feats[f\"sb_chase_speed_agent_mean_{w30}\"] = a_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_chase_speed_target_mean_{w30}\"] = t_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_chase_dist_mean_{w30}\"] = rel_dist_s.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            # Escape-like: heading ngược, dist tăng nhanh\n",
    "            feats[f\"sb_esc_heading_cos_mean_{w30}\"] = heading_cos_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_esc_dist_gain_{w30}\"] = (rel_dist_s.shift(-ws) - rel_dist_s).fillna(0.0)\n",
    "    \n",
    "        # clip & fillna\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = v.replace([np.inf, -np.inf], np.nan).fillna(0.0).astype(\"float32\")\n",
    "    \n",
    "        return feats\n",
    "\n",
    "\n",
    "    def _feat_follow_pattern(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng hành vi FOLLOW:\n",
    "          - Agent ở gần target\n",
    "          - Cùng hướng (body + velocity)\n",
    "          - Tốc độ vừa phải\n",
    "          - Khoảng cách tương đối ổn định trong 0.5–1s\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 1. CÁC ĐẠI LƯỢNG CƠ BẢN ---\n",
    "        # Vector Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # Speed agent/target\n",
    "        a_speed = ctx.speed_series.astype(\"float32\")\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "    \n",
    "        # Body vector: nose - tail/body_center\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\", \"body_center\"])\n",
    "        parts_t = self._extract_parts_dict(target_ctx, [\"nose\", \"tail_base\", \"body_center\"])\n",
    "    \n",
    "        def body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"nose\")\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if tail is None:\n",
    "                tail = parts_dict.get(\"body_center\")\n",
    "            if head is None or tail is None:\n",
    "                return None\n",
    "            return head - tail\n",
    "    \n",
    "        a_body = body_vec(parts_a)\n",
    "        t_body = body_vec(parts_t)\n",
    "    \n",
    "        if a_body is not None and t_body is not None:\n",
    "            dot_bt = np.sum(a_body * t_body, axis=1)\n",
    "            mag_bt = np.linalg.norm(a_body, axis=1) * np.linalg.norm(t_body, axis=1)\n",
    "            cos_body = np.clip(dot_bt / (mag_bt + 1e-6), -1.0, 1.0)\n",
    "            cos_body_s = pd.Series(cos_body, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            cos_body_s = zero()\n",
    "    \n",
    "        # Velocity hướng\n",
    "        a_vel = ctx.vel\n",
    "        t_vel = target_ctx.vel\n",
    "        a_speed_np = np.linalg.norm(a_vel, axis=1)\n",
    "        t_speed_np = np.linalg.norm(t_vel, axis=1)\n",
    "        moving_mask = (a_speed_np > 1e-3) & (t_speed_np > 1e-3)\n",
    "    \n",
    "        # cos giữa hướng velocity 2 con\n",
    "        dot_v = np.sum(a_vel * t_vel, axis=1)\n",
    "        mag_v = a_speed_np * t_speed_np + 1e-6\n",
    "        cos_vel = np.zeros_like(dot_v, dtype=\"float32\")\n",
    "        cos_vel[moving_mask] = np.clip(dot_v[moving_mask] / mag_v[moving_mask], -1.0, 1.0)\n",
    "        cos_vel_s = pd.Series(cos_vel, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 2. WINDOW NGẮN (FOLLOW LÀ PATTERN DÀI HƠN ATTACK) ---\n",
    "        for w30 in [15, 30, 60]:   # ~0.5s, 1s, 2s\n",
    "            ws = self._scale(w30)\n",
    "            min_p = max(ws // 3, 1)\n",
    "    \n",
    "            # Khoảng cách trung bình & độ dao động\n",
    "            m_dist = rel_dist_s.rolling(ws, min_periods=min_p).mean()\n",
    "            s_dist = rel_dist_s.rolling(ws, min_periods=min_p).std()\n",
    "    \n",
    "            # Cùng hướng (body + velocity)\n",
    "            m_cos_body = cos_body_s.rolling(ws, min_periods=min_p).mean()\n",
    "            m_cos_vel  = cos_vel_s.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            # Tốc độ vừa phải\n",
    "            m_sp_a = a_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            m_sp_t = t_speed.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            feats[f\"follow_dist_mean_{w30}\"] = m_dist\n",
    "            feats[f\"follow_dist_std_{w30}\"]  = s_dist\n",
    "            feats[f\"follow_cos_body_mean_{w30}\"] = m_cos_body\n",
    "            feats[f\"follow_cos_vel_mean_{w30}\"]  = m_cos_vel\n",
    "            feats[f\"follow_speed_agent_mean_{w30}\"] = m_sp_a\n",
    "            feats[f\"follow_speed_target_mean_{w30}\"] = m_sp_t\n",
    "    \n",
    "        # Clean\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "        \n",
    "\n",
    "    def _feat_pairwise(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng tương tác cặp đôi (Pairwise): Khoảng cách, Tốc độ tiếp cận.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None: \n",
    "            return feats\n",
    "\n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "\n",
    "        # --- 1. KHOẢNG CÁCH CƠ BẢN (DISTANCES) ---\n",
    "        # Vector nối Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        feats[\"rel_dist\"] = pd.Series(dist, index=idx, dtype=\"float32\")\n",
    "\n",
    "        # --- 2. KHOẢNG CÁCH CHI TIẾT (NOSE-TO-PART) ---\n",
    "        # Lấy các bộ phận quan trọng\n",
    "        my_parts = self._extract_parts_dict(ctx, [\"nose\", \"neck\"])\n",
    "        target_parts = self._extract_parts_dict(target_ctx, \n",
    "            [\"nose\", \"tail_base\", \"body_center\", \"ear_left\", \"ear_right\", \n",
    "             \"lateral_left\", \"lateral_right\"])\n",
    "\n",
    "        def dist_ab(pt_a, pt_b):\n",
    "            if pt_a is None or pt_b is None: return zero()\n",
    "            d = np.linalg.norm(pt_a - pt_b, axis=1)\n",
    "            return pd.Series(d, index=idx, dtype=\"float32\")\n",
    "\n",
    "        an, tn = my_parts[\"nose\"], target_parts[\"nose\"]\n",
    "        feats[\"dist_nose_nose\"] = dist_ab(an, tn)\n",
    "        feats[\"dist_nose_tail\"] = dist_ab(an, target_parts[\"tail_base\"])\n",
    "        feats[\"dist_nose_body\"] = dist_ab(an, target_parts[\"body_center\"])\n",
    "        feats[\"dist_nose_el\"]   = dist_ab(an, target_parts[\"ear_left\"])\n",
    "        feats[\"dist_nose_er\"]   = dist_ab(an, target_parts[\"ear_right\"])\n",
    "        feats[\"dist_nose_tll\"]  = dist_ab(an, target_parts[\"lateral_left\"])\n",
    "        feats[\"dist_nose_tlr\"]  = dist_ab(an, target_parts[\"lateral_right\"])\n",
    "        # feats[\"dist_nose_tt\"]  = dist_ab(an, target_parts[\"tail_tip\"])\n",
    "\n",
    "        # --- 3. ĐỊNH HƯỚNG & GÓC NHÌN (ORIENTATION & GAZE) ---\n",
    "        # Helper lấy vector cơ thể (Mũi - Đuôi/Thân)\n",
    "        def get_body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"nose\")\n",
    "            # Ưu tiên đuôi, nếu ko có thì dùng thân\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if tail is None: tail = parts_dict.get(\"body_center\") # Fallback\n",
    "            \n",
    "            if head is not None and tail is not None:\n",
    "                return head - tail\n",
    "            return None\n",
    "\n",
    "        a_vec = get_body_vec(my_parts)\n",
    "        t_vec = get_body_vec(target_parts)\n",
    "\n",
    "        # A. Body Cosine: Hai con cùng chiều hay ngược chiều?\n",
    "        if a_vec is not None and t_vec is not None:\n",
    "            dot = np.sum(a_vec * t_vec, axis=1)\n",
    "            mags = np.linalg.norm(a_vec, axis=1) * np.linalg.norm(t_vec, axis=1)\n",
    "            feats[\"body_cosine\"] = pd.Series(\n",
    "                np.clip(dot / (mags + 1e-6), -1.0, 1.0), index=idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"body_cosine\"] = zero()\n",
    "\n",
    "        # B. Gaze Cosine: Tôi có đang nhìn về phía Target không?\n",
    "        # Vector ánh nhìn = Target_Pos - My_Pos = rel_vec\n",
    "        if a_vec is not None:\n",
    "            dot_gaze = np.sum(a_vec * rel_vec, axis=1)\n",
    "            mag_a = np.linalg.norm(a_vec, axis=1)\n",
    "            # dist đã tính ở bước 1\n",
    "            feats[\"gaze_cosine\"] = pd.Series(\n",
    "                np.clip(dot_gaze / (mag_a * dist + 1e-6), -1.0, 1.0),\n",
    "                index=idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"gaze_cosine\"] = zero()\n",
    "\n",
    "        # --- 4. PHÂN RÃ VẬN TỐC (VELOCITY DECOMPOSITION) - CHÌA KHÓA CHO AVOID/ESCAPE ---\n",
    "        # Vector đơn vị hướng về địch (u)\n",
    "        dist_safe = dist.copy()\n",
    "        dist_safe[dist_safe == 0] = 1e-6\n",
    "        u_vec = rel_vec / dist_safe[:, None]\n",
    "\n",
    "        # a_vel và t_vel lấy từ Context\n",
    "        a_vel, t_vel = ctx.vel, target_ctx.vel\n",
    "\n",
    "        # A. Approach Speed (Vận tốc dọc trục nối 2 con)\n",
    "        # Dương: Lao vào nhau | Âm: Chạy ra xa nhau\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1) # Target hướng ngược lại\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)\n",
    "\n",
    "        # B. Lateral Speed (Vận tốc ngang - Vuông góc trục nối)\n",
    "        # Vector chiếu: v_proj = (v . u) * u\n",
    "        a_proj = a_along[:, None] * u_vec\n",
    "        a_lat_vec = a_vel - a_proj\n",
    "        a_lat_speed = np.linalg.norm(a_lat_vec, axis=1)\n",
    "\n",
    "        feats[\"approach_speed_agent\"]  = pd.Series(a_along, index=idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_target\"] = pd.Series(t_along, index=idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_rel\"]    = pd.Series(rel_along, index=idx, dtype=\"float32\")\n",
    "        feats[\"lateral_speed_agent\"]   = pd.Series(a_lat_speed, index=idx, dtype=\"float32\")\n",
    "        return feats\n",
    "\n",
    "\n",
    "    # --- Methods tương thích ---\n",
    "    \n",
    "    def build_pose_tensor(self, tracking: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Chuyển dữ liệu tracking (DataFrame) sang Tensor [Frames, Mice, 2] và Dict chi tiết.\n",
    "        \"\"\"\n",
    "        tracking = tracking.sort_values(\"video_frame\")\n",
    "        frames = np.sort(tracking[\"video_frame\"].unique())\n",
    "        \n",
    "        pvid = tracking.pivot(\n",
    "            index=\"video_frame\", \n",
    "            columns=[\"mouse_id\", \"bodypart\"], \n",
    "            values=[\"x\", \"y\"]\n",
    "        )\n",
    "        pvid = pvid.reorder_levels([1, 2, 0], axis=1).sort_index(axis=1).astype(\"float32\")\n",
    "        mouse_ids = list(pvid.columns.get_level_values(0).unique())\n",
    "        pos = np.full((len(frames), len(mouse_ids), 2), np.nan, dtype=np.float32)\n",
    "        per_mouse_df = {}\n",
    "        \n",
    "        for i, mid in enumerate(mouse_ids):\n",
    "            single = pvid[mid]\n",
    "            per_mouse_df[mid] = single\n",
    "            \n",
    "            if \"body_center\" in single.columns.get_level_values(0):\n",
    "                cx = single[\"body_center\"][\"x\"]\n",
    "                cy = single[\"body_center\"][\"y\"]\n",
    "            else:\n",
    "                cx = single.xs(\"x\", level=1, axis=1).mean(axis=1)\n",
    "                cy = single.xs(\"y\", level=1, axis=1).mean(axis=1)\n",
    "            \n",
    "            pos[:, i, 0] = cx.reindex(frames).values\n",
    "            pos[:, i, 1] = cy.reindex(frames).values\n",
    "            \n",
    "        return frames, mouse_ids, pos, per_mouse_df\n",
    "\n",
    "    def extract_agent_target(\n",
    "        self, \n",
    "        frames: np.ndarray, \n",
    "        mouse_ids: List[Any], \n",
    "        pos: np.ndarray, \n",
    "        agent_id: Any, \n",
    "        target_id: Any, \n",
    "        per_mouse_df: Dict = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Trích xuất đặc trưng cho cặp (Agent, Target).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            aid_idx = mouse_ids.index(agent_id)\n",
    "        except ValueError:\n",
    "            return pd.DataFrame() \n",
    "\n",
    "        # 1. Build Agent Context\n",
    "        ctx_agent = self._build_context(\n",
    "            frames, \n",
    "            pos[:, aid_idx, :], \n",
    "            per_mouse_df.get(agent_id) if per_mouse_df else None\n",
    "        )\n",
    "\n",
    "        # 2. Build Target Context\n",
    "        ctx_target = None\n",
    "        if self.cfg.use_pairwise and target_id is not None and target_id in mouse_ids:\n",
    "             tid_idx = mouse_ids.index(target_id)\n",
    "             ctx_target = self._build_context(\n",
    "                 frames, \n",
    "                 pos[:, tid_idx, :], \n",
    "                 per_mouse_df.get(target_id) if per_mouse_df else None\n",
    "             )\n",
    "\n",
    "        # 3. Run all features\n",
    "        all_data = {}\n",
    "        for func_name, func in self.feature_registry.items():\n",
    "            out_dict = func(ctx_agent, target_ctx=ctx_target)\n",
    "            all_data.update(out_dict)\n",
    "\n",
    "        df_out = pd.DataFrame(all_data, index=ctx_agent.idx)\n",
    "        df_out = df_out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        \n",
    "        return df_out.reindex(sorted(df_out.columns), axis=1)\n",
    "\n",
    "#================================================================================\n",
    "#================================================================================\n",
    "#================================================================================\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# (Trên Kaggle) dùng metric chính thức\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/usr/lib/mabe-f-beta\")\n",
    "from metric import score   # hàm score(submission_df, dataset_df)\n",
    "\n",
    "# =========================================================\n",
    "# 1. ĐƯỜNG DẪN & CẤU HÌNH\n",
    "# =========================================================\n",
    "\n",
    "INPUT_DIR = Path(\"/kaggle/input/MABe-mouse-behavior-detection\")\n",
    "TRAIN_TRACKING_DIR = INPUT_DIR / \"train_tracking\"\n",
    "TRAIN_ANNOTATION_DIR = INPUT_DIR / \"train_annotation\"\n",
    "TEST_TRACKING_DIR = INPUT_DIR / \"test_tracking\"\n",
    "\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "RESULTS_DIR = Path(r\"/kaggle/input/results-xgb-fe\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "INDEX_COLS = [\"video_id\", \"agent_id\", \"target_id\", \"video_frame\"]\n",
    "\n",
    "# hành vi “self” vs “pair” giống notebook (có thể chỉnh nếu muốn)\n",
    "SELF_BEHAVIORS = [\n",
    "    \"biteobject\", \"climb\", \"dig\", \"exploreobject\", \"freeze\",\n",
    "    \"genitalgroom\", \"huddle\", \"rear\", \"rest\", \"run\", \"selfgroom\",\n",
    "]\n",
    "PAIR_BEHAVIORS = [\n",
    "    \"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\",\n",
    "    \"chase\", \"chaseattack\", \"defend\", \"disengage\", \"dominance\",\n",
    "    \"dominancegroom\", \"dominancemount\", \"ejaculate\", \"escape\",\n",
    "    \"flinch\", \"follow\", \"intromit\", \"mount\", \"reciprocalsniff\",\n",
    "    \"shepherd\", \"sniff\", \"sniffbody\", \"sniffface\", \"sniffgenital\",\n",
    "    \"submit\", \"tussle\",\n",
    "]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. ĐỌC METADATA & HELPER\n",
    "# =========================================================\n",
    "\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    train_meta = pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "    return train_meta\n",
    "\n",
    "\n",
    "def get_video_params(video_id: Any, meta: pd.DataFrame) -> Tuple[float, float]:\n",
    "    \"\"\"Lấy fps, pix_per_cm cho video từ train.csv.\"\"\"\n",
    "    row = meta.loc[meta[\"video_id\"] == video_id]\n",
    "    if row.empty:\n",
    "        raise KeyError(f\"video_id={video_id} không có trong train.csv\")\n",
    "    row = row.iloc[0]\n",
    "\n",
    "    # giống notebook: cột \"frames per second\" & \"pix per cm (approx)\"\n",
    "    fps = float(row[\"frames_per_second\"])\n",
    "    pix_per_cm = float(row[\"pix_per_cm_approx\"])\n",
    "    if not np.isfinite(pix_per_cm) or pix_per_cm <= 0:\n",
    "        pix_per_cm = 1.0\n",
    "    return fps, pix_per_cm\n",
    "\n",
    "\n",
    "def load_tracking(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    \"\"\"Đọc tracking parquet → pandas (schema: video_frame, mouse_id, bodypart, x, y).\"\"\"\n",
    "    path = TRAIN_TRACKING_DIR / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    df = pd.read_parquet(path)\n",
    "    return df\n",
    "\n",
    "def load_tracking_test(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    \"\"\"Đọc tracking parquet của test → pandas.\"\"\"\n",
    "    path = INPUT_DIR / \"test_tracking\" / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "def load_annotation(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    \"\"\"Đọc annotation (agent_id, target_id, action, start_frame, stop_frame).\"\"\"\n",
    "    path = TRAIN_ANNOTATION_DIR / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists():\n",
    "        # không có label cho video này\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "        )\n",
    "    ann = pd.read_parquet(path)\n",
    "    return ann[[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. TÍNH FEATURE PER-FRAME BẰNG FEATUREEXTRACTOR\n",
    "# =========================================================\n",
    "\n",
    "# Cache: (lab, video, agent, target) -> (frames, feature_df)\n",
    "_feature_cache: Dict[Tuple[str, int, int, int], Tuple[np.ndarray, pd.DataFrame]] = {}\n",
    "\n",
    "\n",
    "def get_frame_features_for_pair(\n",
    "    lab_id: str,\n",
    "    video_id: int,\n",
    "    agent_id: int,\n",
    "    target_id: int,\n",
    "    meta: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Tính (hoặc lấy cache) feature per-frame cho 1 video + (agent, target).\n",
    "    Trả về: frames [F], features_df [F, D]\n",
    "    \"\"\"\n",
    "    key = (str(lab_id), int(video_id), int(agent_id), int(target_id))\n",
    "    if key in _feature_cache:\n",
    "        return _feature_cache[key]\n",
    "\n",
    "    fps, pix_per_cm = get_video_params(video_id, meta)\n",
    "    tracking = load_tracking(lab_id, video_id)\n",
    "\n",
    "    fe = FeatureExtractor(\n",
    "        fps=fps,\n",
    "        pix_per_cm=pix_per_cm,\n",
    "        smooth_sigma=1.0,\n",
    "        use_pairwise=True,\n",
    "    )\n",
    "\n",
    "    frames, mouse_ids, pos, per_mouse_df = fe.build_pose_tensor(tracking)\n",
    "\n",
    "    # agent/target có thể là cùng chuột (self) hoặc khác chuột (pair)\n",
    "    features_df: pd.DataFrame = fe.extract_agent_target(\n",
    "        frames=frames,\n",
    "        mouse_ids=mouse_ids,\n",
    "        pos=pos,\n",
    "        agent_id=agent_id,\n",
    "        target_id=target_id,\n",
    "        per_mouse_df=per_mouse_df,\n",
    "    )\n",
    "    # index chính là frame\n",
    "    features_df.index = frames\n",
    "\n",
    "    _feature_cache[key] = (frames, features_df)\n",
    "    return frames, features_df\n",
    "\n",
    "_feature_cache: Dict[Tuple[str, int, Any, Any], Tuple[np.ndarray, pd.DataFrame]] = {}\n",
    "\n",
    "def get_frame_features_for_pair_test(\n",
    "    lab_id: str,\n",
    "    video_id: int,\n",
    "    agent_id: Any,\n",
    "    target_id: Any,\n",
    "    test_meta: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Feature per-frame cho test (video_id, agent, target).\n",
    "    Trả về: frames [F], features_df [F, D]\n",
    "    \"\"\"\n",
    "    key = (f\"test_{lab_id}\", int(video_id), agent_id, target_id)\n",
    "    if key in _feature_cache:\n",
    "        return _feature_cache[key]\n",
    "\n",
    "    # Lấy fps, pix_per_cm_approx từ test.csv\n",
    "    row = test_meta[test_meta[\"video_id\"] == video_id].iloc[0]\n",
    "    fps = float(row[\"frames_per_second\"])\n",
    "    pix_per_cm = float(row[\"pix_per_cm_approx\"])\n",
    "    if not np.isfinite(pix_per_cm) or pix_per_cm <= 0:\n",
    "        pix_per_cm = 1.0\n",
    "\n",
    "    tracking = load_tracking_test(lab_id, video_id)\n",
    "\n",
    "    fe = FeatureExtractor(\n",
    "        fps=fps,\n",
    "        pix_per_cm=pix_per_cm,\n",
    "        smooth_sigma=1.0,\n",
    "        use_pairwise=True,\n",
    "    )\n",
    "\n",
    "    frames, mouse_ids, pos, per_mouse_df = fe.build_pose_tensor(tracking)\n",
    "\n",
    "    features_df = fe.extract_agent_target(\n",
    "        frames=frames,\n",
    "        mouse_ids=mouse_ids,\n",
    "        pos=pos,\n",
    "        agent_id=agent_id,\n",
    "        target_id=target_id,\n",
    "        per_mouse_df=per_mouse_df,\n",
    "    )\n",
    "    features_df.index = frames\n",
    "\n",
    "    _feature_cache[key] = (frames, features_df)\n",
    "    return frames, features_df\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. BUILD FRAME-LEVEL DATASET CHO 1 (lab_id, behavior)\n",
    "# =========================================================\n",
    "\n",
    "def build_frame_dataset_for_lab_behavior(\n",
    "    lab_id: str,\n",
    "    behavior: str,\n",
    "    train_meta: pd.DataFrame,\n",
    "    mode: str = \"self\",\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Xây tập frame-level (indices, features, labels) cho 1 (lab, behavior).\n",
    "\n",
    "    indices: DataFrame với cột INDEX_COLS\n",
    "    features: DataFrame per-frame features\n",
    "    labels: np.ndarray nhị phân (0/1)\n",
    "    \"\"\"\n",
    "\n",
    "    videos = (\n",
    "        train_meta[train_meta[\"lab_id\"] == lab_id][\"video_id\"]\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    index_list = []\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for video_id in videos:\n",
    "        ann = load_annotation(lab_id, video_id)\n",
    "        if ann.empty:\n",
    "            continue\n",
    "\n",
    "        # chỉ lấy annotation của behavior này\n",
    "        ann_bhv = ann[ann[\"action\"] == behavior]\n",
    "        if ann_bhv.empty:\n",
    "            continue\n",
    "\n",
    "        # các (agent, target) cần xem\n",
    "        pairs = ann_bhv[[\"agent_id\", \"target_id\"]].drop_duplicates().values.tolist()\n",
    "\n",
    "        for (agent_id, target_id) in pairs:\n",
    "            if mode == \"self\":\n",
    "                target_id_use = agent_id\n",
    "            else:\n",
    "                target_id_use = target_id\n",
    "\n",
    "            frames, feat_df = get_frame_features_for_pair(\n",
    "                lab_id=lab_id,\n",
    "                video_id=video_id,\n",
    "                agent_id=agent_id,\n",
    "                target_id=target_id_use,\n",
    "                meta=train_meta,\n",
    "            )\n",
    "\n",
    "            # label per-frame: frame ∈ bất kỳ [start, stop) của (agent,target,behavior)\n",
    "            ann_pair = ann_bhv[\n",
    "                (ann_bhv[\"agent_id\"] == agent_id)\n",
    "                & (ann_bhv[\"target_id\"] == target_id)\n",
    "            ]\n",
    "            if ann_pair.empty and mode == \"self\":\n",
    "                ann_pair = ann_bhv[ann_bhv[\"agent_id\"] == agent_id]\n",
    "\n",
    "            pos_frames = set()\n",
    "            for _, r in ann_pair.iterrows():\n",
    "                pos_frames.update(range(int(r[\"start_frame\"]), int(r[\"stop_frame\"])))\n",
    "\n",
    "            if len(pos_frames) == 0:\n",
    "                continue\n",
    "\n",
    "            label = np.isin(frames, list(pos_frames)).astype(\"int8\")\n",
    "            if label.sum() == 0:\n",
    "                continue\n",
    "\n",
    "\n",
    "            idx_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"video_id\": video_id,\n",
    "                    \"agent_id\": agent_id,\n",
    "                    \"target_id\": target_id,\n",
    "                    \"video_frame\": frames,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            index_list.append(idx_df)\n",
    "            feature_list.append(feat_df.reset_index(drop=True))\n",
    "            label_list.append(label)\n",
    "\n",
    "    if not index_list:\n",
    "        return (\n",
    "            pd.DataFrame(columns=INDEX_COLS),\n",
    "            pd.DataFrame(),\n",
    "            np.zeros(0, dtype=\"int8\"),\n",
    "        )\n",
    "\n",
    "    indices = pd.concat(index_list, ignore_index=True)\n",
    "    features = pd.concat(feature_list, ignore_index=True)\n",
    "    labels = np.concatenate(label_list).astype(\"int8\")\n",
    "\n",
    "    assert len(indices) == len(features) == len(labels)\n",
    "\n",
    "    return indices, features, labels\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. TRAIN + OOF CHO 1 (lab_id, behavior)\n",
    "# =========================================================\n",
    "\n",
    "def tune_threshold(oof_pred: np.ndarray, y: np.ndarray) -> float:\n",
    "    ths = np.arange(0.0, 1.005, 0.005)\n",
    "    scores = [f1_score(y, (oof_pred >= th), zero_division=0) for th in ths]\n",
    "    return float(ths[int(np.argmax(scores))])\n",
    "\n",
    "#\n",
    "def train_validate_one(\n",
    "    lab_id: str,\n",
    "    behavior: str,\n",
    "    indices: pd.DataFrame,\n",
    "    features: pd.DataFrame,\n",
    "    labels: np.ndarray,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Train XGBoost binary cho 1 (lab, behavior) + lưu OOF prediction.\n",
    "    Trả về: F1 trên toàn bộ OOF (frame-level).\n",
    "    \"\"\"\n",
    "    result_dir = RESULTS_DIR / lab_id / behavior\n",
    "    result_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    n = len(labels)\n",
    "\n",
    "    if n == 0 or labels.sum() == 0:\n",
    "        oof_df = indices.copy()\n",
    "        oof_df[\"fold\"] = -1\n",
    "        oof_df[\"prediction\"] = 0.0\n",
    "        oof_df[\"predicted_label\"] = 0\n",
    "        oof_df.to_parquet(result_dir / \"oof_predictions.parquet\", index=False)\n",
    "        (result_dir / \"f1.txt\").write_text(\"0.0\\n\")\n",
    "        return 0.0\n",
    "\n",
    "    X = features.values.astype(\"float32\")\n",
    "    y = labels.astype(\"int8\")\n",
    "    groups = indices[\"video_id\"].values\n",
    "\n",
    "    folds = np.ones(n, dtype=\"int8\") * -1\n",
    "    oof_pred = np.zeros(n, dtype=\"float32\")\n",
    "    oof_label = np.zeros(n, dtype=\"int8\")\n",
    "\n",
    "    cv = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups=groups)):\n",
    "        fold_dir = result_dir / f\"fold_{fold}\"\n",
    "        fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "        X_va, y_va = X[va_idx], y[va_idx]\n",
    "\n",
    "        # scale_pos_weight\n",
    "        pos = y_tr.sum()\n",
    "        neg = len(y_tr) - pos\n",
    "        scale_pos_weight = float(neg / pos) if pos > 0 else 1.0\n",
    "\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"logloss\",\n",
    "            \"device\": \"cuda\",\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"max_depth\": 6,\n",
    "            \"min_child_weight\": 5,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"scale_pos_weight\": scale_pos_weight,\n",
    "            \"max_bin\": 64,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "\n",
    "        dtrain = xgb.QuantileDMatrix(\n",
    "            X_tr,\n",
    "            label=y_tr,\n",
    "            feature_names=features.columns.tolist(),\n",
    "            max_bin=64,\n",
    "        )\n",
    "        dvalid = xgb.DMatrix(\n",
    "            X_va,\n",
    "            label=y_va,\n",
    "            feature_names=features.columns.tolist(),\n",
    "        )\n",
    "\n",
    "        evals_result: Dict[str, Dict[str, List[float]]] = {}\n",
    "\n",
    "        early_stop = xgb.callback.EarlyStopping(\n",
    "            rounds=10, metric_name=\"logloss\", data_name=\"valid\", maximize=False\n",
    "        )\n",
    "\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=250,\n",
    "            evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "            callbacks=[early_stop],\n",
    "            evals_result=evals_result,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "\n",
    "        pred_va = model.predict(dvalid)\n",
    "        th = tune_threshold(pred_va, y_va)\n",
    "\n",
    "        folds[va_idx] = fold\n",
    "        oof_pred[va_idx] = pred_va\n",
    "        oof_label[va_idx] = (pred_va >= th).astype(\"int8\")\n",
    "\n",
    "        model.save_model(fold_dir / \"model.json\")\n",
    "        with open(fold_dir / \"threshold.txt\", \"w\") as f:\n",
    "            f.write(f\"{th}\\n\")\n",
    "\n",
    "    # lưu OOF\n",
    "    oof_df = indices.copy()\n",
    "    oof_df[\"fold\"] = folds\n",
    "    oof_df[\"prediction\"] = oof_pred\n",
    "    oof_df[\"predicted_label\"] = oof_label\n",
    "    oof_df.to_parquet(result_dir / \"oof_predictions.parquet\", index=False)\n",
    "\n",
    "    f1 = f1_score(y, oof_label, zero_division=0)\n",
    "    (result_dir / \"f1.txt\").write_text(f\"{f1:.6f}\\n\")\n",
    "    return float(f1)\n",
    "\n",
    "def load_models_for_behavior_infer(lab_id: str, behavior: str):\n",
    "    \"\"\"\n",
    "    Đọc các fold model + threshold cho (lab, behavior) từ RESULTS_DIR.\n",
    "    Dùng cho inference (test).\n",
    "    \"\"\"\n",
    "    base_dir = RESULTS_DIR / lab_id / behavior\n",
    "    if not base_dir.exists():\n",
    "        return []\n",
    "\n",
    "    models = []\n",
    "    for fold_dir in sorted(base_dir.glob(\"fold_*\")):\n",
    "        model_file = fold_dir / \"model.json\"\n",
    "        thr_file = fold_dir / \"threshold.txt\"\n",
    "        if not model_file.exists():\n",
    "            continue\n",
    "\n",
    "        booster = xgb.Booster()\n",
    "        booster.load_model(str(model_file))\n",
    "\n",
    "        if thr_file.exists():\n",
    "            thr = float(thr_file.read_text().strip())\n",
    "        else:\n",
    "            thr = 0.5\n",
    "\n",
    "        models.append((booster, thr))\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. LOOP QUA TẤT CẢ BEHAVIORS TRONG 1 LAB\n",
    "#    (train_all_labs_behaviors vẫn giữ nguyên, nhưng main\n",
    "#     sẽ filter train_meta chỉ còn 1 lab)\n",
    "# =========================================================\n",
    "\n",
    "def train_all_labs_behaviors(train_meta: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Loop qua từng lab trong train_meta (ở đây main đã filter chỉ còn 1 lab):\n",
    "      - đọc annotation của tất cả video\n",
    "      - lấy unique action xuất hiện trong lab đó\n",
    "      - train 1 model/frame-level cho từng (lab, action)\n",
    "    \"\"\"\n",
    "    labs = train_meta[\"lab_id\"].unique().tolist()\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    for lab_id in labs:\n",
    "        # tập video của lab này\n",
    "        videos = train_meta[train_meta[\"lab_id\"] == lab_id][\"video_id\"].unique().tolist()\n",
    "\n",
    "        # gom toàn bộ action thực sự có trong annotation của lab này\n",
    "        behaviors_set = set()\n",
    "        for vid in videos:\n",
    "            ann = load_annotation(lab_id, vid)\n",
    "            if ann.empty:\n",
    "                continue\n",
    "            behaviors_set.update(ann[\"action\"].unique().tolist())\n",
    "\n",
    "        behaviors = sorted(behaviors_set)\n",
    "        print(f\"\\n===== LAB {lab_id}: {len(behaviors)} behaviors =====\")\n",
    "\n",
    "        for behavior in behaviors:\n",
    "            # if behavior != \"submit\": continue\n",
    "\n",
    "            mode = \"self\" if behavior in SELF_BEHAVIORS else \"pair\"\n",
    "\n",
    "            print(f\"\\n=== LAB={lab_id} | behavior={behavior} | mode={mode} ===\")\n",
    "            indices, features, labels = build_frame_dataset_for_lab_behavior(\n",
    "                lab_id=str(lab_id),\n",
    "                behavior=behavior,\n",
    "                train_meta=train_meta,\n",
    "                mode=mode,\n",
    "            )\n",
    "            print(\n",
    "                f\"frames: {len(labels):,}, positives: {labels.sum():,}, features: \"\n",
    "                f\"{features.shape[1] if not features.empty else 0}\"\n",
    "            )\n",
    "\n",
    "            if len(labels) == 0:\n",
    "                print(\" -> skip (no samples)\")\n",
    "                continue\n",
    "\n",
    "            f1 = train_validate_one(str(lab_id), behavior, indices, features, labels)\n",
    "            elapsed = time.perf_counter() - start_time\n",
    "            print(f\" -> OOF F1 (frame-level): {f1:.3f} | elapsed={elapsed/60:.1f} min\")\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. GOM OOF PREDICTION → SEGMENT & TÍNH SCORE()\n",
    "# =========================================================\n",
    "\n",
    "def build_oof_submission_from_parquet(\n",
    "    target_lab_id: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Đọc tất cả oof_predictions.parquet trong RESULTS_DIR,\n",
    "    gom thành frame-level table rồi nối thành segment-level prediction\n",
    "    giống inference notebook (simplified).\n",
    "\n",
    "    Nếu target_lab_id != None thì chỉ lấy OOF của lab đó\n",
    "    (vd \"AdaptableSnail\").\n",
    "    \"\"\"\n",
    "    oof_files = list(RESULTS_DIR.glob(\"*/**/oof_predictions.parquet\"))\n",
    "    if not oof_files:\n",
    "        raise RuntimeError(\"Không tìm thấy OOF parquet, hãy train trước.\")\n",
    "\n",
    "    frame_preds = []\n",
    "\n",
    "    for path in oof_files:\n",
    "        # path: results_xgb_fe/lab/behavior/oof_predictions.parquet\n",
    "        parts = path.parts\n",
    "        behavior = parts[-2]\n",
    "        lab_id = parts[-3]\n",
    "\n",
    "        # chỉ lấy file thuộc lab mong muốn (nếu có)\n",
    "        if target_lab_id is not None and lab_id != target_lab_id:\n",
    "            continue\n",
    "\n",
    "        df = pd.read_parquet(path)\n",
    "        df = df[INDEX_COLS + [\"prediction\"]].copy()\n",
    "        df[\"lab_id\"] = lab_id\n",
    "        df[\"action\"] = behavior\n",
    "        frame_preds.append(df)\n",
    "\n",
    "    if not frame_preds:\n",
    "        raise RuntimeError(\n",
    "            f\"Không có OOF predictions nào cho lab_id={target_lab_id}\"\n",
    "        )\n",
    "\n",
    "    frame_df = pd.concat(frame_preds, ignore_index=True)\n",
    "\n",
    "    # sắp xếp\n",
    "    frame_df = frame_df.sort_values(\n",
    "        [\"lab_id\", \"video_id\", \"agent_id\", \"target_id\", \"action\", \"video_frame\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Convert frame-level prob -> hard label + segments\n",
    "    segments = []\n",
    "    for (lab_id, video_id, agent_id, target_id, action), group in frame_df.groupby(\n",
    "        [\"lab_id\", \"video_id\", \"agent_id\", \"target_id\", \"action\"], sort=False\n",
    "    ):\n",
    "        frames = group[\"video_frame\"].values\n",
    "        scores = group[\"prediction\"].values\n",
    "\n",
    "        # dùng một threshold fix (vd 0.5) cho demo\n",
    "        # (hoặc bạn có thể lưu threshold per (lab,behavior) và apply)\n",
    "        hard = scores >= 0.5\n",
    "\n",
    "        in_seg = False\n",
    "        start = None\n",
    "        prev_f = None\n",
    "\n",
    "        for f, h in zip(frames, hard):\n",
    "            if h and not in_seg:\n",
    "                in_seg = True\n",
    "                start = int(f)\n",
    "            elif (not h) and in_seg:\n",
    "                stop = int(prev_f + 1)  # [start, stop)\n",
    "                segments.append(\n",
    "                    {\n",
    "                        \"lab_id\": lab_id,\n",
    "                        \"video_id\": int(video_id),\n",
    "                        \"agent_id\": int(agent_id),\n",
    "                        \"target_id\": int(target_id),\n",
    "                        \"action\": action,\n",
    "                        \"start_frame\": start,\n",
    "                        \"stop_frame\": stop,\n",
    "                    }\n",
    "                )\n",
    "                in_seg = False\n",
    "            prev_f = f\n",
    "\n",
    "        if in_seg:\n",
    "            stop = int(frames[-1] + 1)\n",
    "            segments.append(\n",
    "                {\n",
    "                    \"lab_id\": lab_id,\n",
    "                    \"video_id\": int(video_id),\n",
    "                    \"agent_id\": int(agent_id),\n",
    "                    \"target_id\": int(target_id),\n",
    "                    \"action\": action,\n",
    "                    \"start_frame\": start,\n",
    "                    \"stop_frame\": stop,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not segments:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"lab_id\",\n",
    "                \"video_id\",\n",
    "                \"agent_id\",\n",
    "                \"target_id\",\n",
    "                \"action\",\n",
    "                \"start_frame\",\n",
    "                \"stop_frame\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    submission = pd.DataFrame(segments)\n",
    "    submission = submission.sort_values(\n",
    "        [\"lab_id\", \"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return submission\n",
    "\n",
    "BAD_VIDEOS = [143861384, 1596473327, 1212811043, 878123481]\n",
    "\n",
    "def compute_validation_score(\n",
    "    submission: pd.DataFrame,\n",
    "    lab_id: Optional[str] = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Gọi metric `score()` chính thức trên train set.\n",
    "    Nếu lab_id != None → chỉ validate trên lab đó.\n",
    "    \"\"\"\n",
    "    # ===== THAY ĐỔI Ở ĐÂY =====\n",
    "    # Không dùng train.csv, mà phải đọc toàn bộ annotations\n",
    "    train_meta = pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "    \n",
    "    if lab_id is not None:\n",
    "        train_meta = train_meta[train_meta[\"lab_id\"] == lab_id].reset_index(drop=True)\n",
    "\n",
    "    if BAD_VIDEOS:\n",
    "        train_meta = train_meta[~train_meta[\"video_id\"].isin(BAD_VIDEOS)]\n",
    "    \n",
    "    # Đọc tất cả annotation files\n",
    "    all_annotations = []\n",
    "    for _, row in train_meta.iterrows():\n",
    "        lab = row[\"lab_id\"]\n",
    "        vid = row[\"video_id\"]\n",
    "        ann = load_annotation(lab, vid)\n",
    "        if not ann.empty:\n",
    "            ann[\"lab_id\"] = lab\n",
    "            ann[\"video_id\"] = vid\n",
    "            ann[\"behaviors_labeled\"] = row[\"behaviors_labeled\"]\n",
    "            all_annotations.append(ann)\n",
    "    \n",
    "    if not all_annotations:\n",
    "        print(\"Không có annotation nào để validate!\")\n",
    "        return 0.0\n",
    "    \n",
    "    dataset = pd.concat(all_annotations, ignore_index=True)\n",
    "    \n",
    "    # Filter submission theo lab nếu cần\n",
    "    if lab_id is not None:\n",
    "        submission = submission[submission[\"lab_id\"] == lab_id].reset_index(drop=True)\n",
    "    \n",
    "    # ===== GỌI METRIC =====\n",
    "    s = score(dataset, submission, row_id_column_name=\"row_id\")\n",
    "\n",
    "    print(\n",
    "        f\"Official validation score\"\n",
    "        f\"{' (lab=' + lab_id + ')' if lab_id is not None else ''}: {s:.6f}\"\n",
    "    )\n",
    "    return float(s)\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. MAIN\n",
    "# =========================================================\n",
    "def str_to_mouse_id(s: str) -> int:\n",
    "    if s == \"self\":\n",
    "        return -1\n",
    "    return int(str(s).replace(\"mouse\", \"\"))\n",
    "\n",
    "\n",
    "def predict_behaviors_for_pair(\n",
    "    lab_id: str,\n",
    "    video_id: int,\n",
    "    agent_internal_id: Any,\n",
    "    target_internal_id: Any,\n",
    "    behaviors: List[str],\n",
    "    test_meta: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Chạy inference cho 1 cặp (video, agent_internal_id, target_internal_id)\n",
    "    với list behaviors (cùng mode: all self hoặc all pair).\n",
    "    Trả về segment-level DataFrame: video_id, action, start_frame, stop_frame.\n",
    "    \"\"\"\n",
    "    if lab_id != \"PleasantMeerkat\": return None\n",
    "    frames, feat_df = get_frame_features_for_pair_test(\n",
    "        lab_id=lab_id,\n",
    "        video_id=video_id,\n",
    "        agent_id=agent_internal_id,\n",
    "        target_id=target_internal_id,\n",
    "        test_meta=test_meta,\n",
    "    )\n",
    "    if feat_df.empty:\n",
    "        return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "\n",
    "    feat_df = feat_df.astype(\"float32\")\n",
    "    n_frames = len(feat_df)\n",
    "\n",
    "    scores_per_behavior = {}\n",
    "    for behavior in behaviors:\n",
    "        models = load_models_for_behavior_infer(lab_id, behavior)\n",
    "        if not models:\n",
    "            continue\n",
    "\n",
    "        req_feats = models[0][0].feature_names\n",
    "        # Build X_test với đúng bộ feature của model\n",
    "        X_test = pd.DataFrame(\n",
    "            0.0,\n",
    "            index=feat_df.index,\n",
    "            columns=req_feats,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        common = list(set(req_feats) & set(feat_df.columns))\n",
    "        if common:\n",
    "            X_test[common] = feat_df[common]\n",
    "\n",
    "        dtest = xgb.DMatrix(X_test, feature_names=req_feats)\n",
    "\n",
    "        agg_scores = np.zeros(n_frames, dtype=np.float32)\n",
    "        for booster, thr in models:\n",
    "            probs = booster.predict(dtest)\n",
    "            labels = (probs >= thr).astype(np.int8)\n",
    "            agg_scores += probs * labels\n",
    "\n",
    "        agg_scores /= max(len(models), 1)\n",
    "        scores_per_behavior[behavior] = agg_scores\n",
    "\n",
    "        del dtest, X_test\n",
    "        gc.collect()\n",
    "\n",
    "    if not scores_per_behavior:\n",
    "        return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "\n",
    "    beh_list = list(scores_per_behavior.keys())\n",
    "    score_mat = np.vstack([scores_per_behavior[b] for b in beh_list]).T  # [F, B]\n",
    "\n",
    "    max_idx = score_mat.argmax(axis=1)\n",
    "    max_scores = score_mat.max(axis=1)\n",
    "    labels = np.where(max_scores == 0.0, \"none\", np.array(beh_list)[max_idx])\n",
    "\n",
    "    # frame-level → segment\n",
    "    segments = []\n",
    "    prev_lab = \"none\"\n",
    "    prev_start = None\n",
    "    prev_f = None\n",
    "\n",
    "    for f, lab in zip(frames, labels):\n",
    "        if lab != prev_lab:\n",
    "            if prev_lab != \"none\":\n",
    "                segments.append(\n",
    "                    {\n",
    "                        \"video_id\": int(video_id),\n",
    "                        \"action\": prev_lab,\n",
    "                        \"start_frame\": int(prev_start),\n",
    "                        \"stop_frame\": int(prev_f + 1),\n",
    "                    }\n",
    "                )\n",
    "            prev_lab = lab\n",
    "            prev_start = f\n",
    "        prev_f = f\n",
    "\n",
    "    if prev_lab != \"none\":\n",
    "        segments.append(\n",
    "            {\n",
    "                \"video_id\": int(video_id),\n",
    "                \"action\": prev_lab,\n",
    "                \"start_frame\": int(prev_start),\n",
    "                \"stop_frame\": int(prev_f + 1),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not segments:\n",
    "        return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "\n",
    "    return pd.DataFrame(segments)\n",
    "\n",
    "\n",
    "\n",
    "target_lab = \"PleasantMeerkat\"\n",
    "print(f\"Đọc test.csv cho lab {target_lab} ...\")\n",
    "test_meta = pd.read_csv(INPUT_DIR / \"test.csv\")\n",
    "test_meta = test_meta[test_meta[\"lab_id\"] == target_lab].reset_index(drop=True)\n",
    "\n",
    "# Lấy danh sách behavior đã train (thư mục con trong RESULTS_DIR/AdaptableSnail)\n",
    "lab_result_dir = RESULTS_DIR / target_lab\n",
    "if lab_result_dir.exists():\n",
    "    trained_behaviors = sorted(\n",
    "        [p.name for p in lab_result_dir.iterdir() if p.is_dir()]\n",
    "    )\n",
    "else:\n",
    "    trained_behaviors = []\n",
    "\n",
    "self_behaviors_in_lab = [b for b in trained_behaviors if b in SELF_BEHAVIORS]\n",
    "pair_behaviors_in_lab = [b for b in trained_behaviors if b in PAIR_BEHAVIORS]\n",
    "\n",
    "print(\"Behaviors (self) dùng để predict:\", self_behaviors_in_lab)\n",
    "print(\"Behaviors (pair) dùng để predict:\", pair_behaviors_in_lab)\n",
    "\n",
    "all_segments = []\n",
    "\n",
    "# Loop từng video test của lab\n",
    "for video_id in sorted(test_meta[\"video_id\"].unique()):\n",
    "    print(f\"Predict video_id={video_id} ...\")\n",
    "\n",
    "    tracking = load_tracking_test(target_lab, video_id)\n",
    "    mouse_ids_internal = sorted(tracking[\"mouse_id\"].unique().tolist())\n",
    "\n",
    "    # Map internal mouse_id -> string để đưa vào submission\n",
    "    def to_submit_id(mid):\n",
    "        s = str(mid)\n",
    "        return s if s.startswith(\"mouse\") else f\"mouse{s}\"\n",
    "\n",
    "    # SELF behaviors: agent == target (self)\n",
    "    if self_behaviors_in_lab:\n",
    "        for mid in mouse_ids_internal:\n",
    "            seg_df = predict_behaviors_for_pair(\n",
    "                lab_id=target_lab,\n",
    "                video_id=video_id,\n",
    "                agent_internal_id=mid,\n",
    "                target_internal_id=mid,  # self\n",
    "                behaviors=self_behaviors_in_lab,\n",
    "                test_meta=test_meta,\n",
    "            )\n",
    "            if not seg_df.empty:\n",
    "                seg_df[\"agent_id\"] = to_submit_id(mid)\n",
    "                seg_df[\"target_id\"] = \"self\"\n",
    "                all_segments.append(seg_df)\n",
    "\n",
    "    # PAIR behaviors: mọi cặp agent != target\n",
    "    if pair_behaviors_in_lab and len(mouse_ids_internal) > 1:\n",
    "        for agent_internal, target_internal in itertools.permutations(\n",
    "            mouse_ids_internal, 2\n",
    "        ):\n",
    "            seg_df = predict_behaviors_for_pair(\n",
    "                lab_id=target_lab,\n",
    "                video_id=video_id,\n",
    "                agent_internal_id=agent_internal,\n",
    "                target_internal_id=target_internal,\n",
    "                behaviors=pair_behaviors_in_lab,\n",
    "                test_meta=test_meta,\n",
    "            )\n",
    "            if not seg_df.empty:\n",
    "                seg_df[\"agent_id\"] = to_submit_id(agent_internal)\n",
    "                seg_df[\"target_id\"] = to_submit_id(target_internal)\n",
    "                all_segments.append(seg_df)\n",
    "\n",
    "# Gộp tất cả segments → submission.csv\n",
    "# Gộp tất cả segments → submission2.csv\n",
    "if all_segments:\n",
    "    submission6 = pd.concat(all_segments, ignore_index=True)\n",
    "    submission6 = submission6[\n",
    "        [\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "    ]\n",
    "    submission6 = submission6.sort_values(\n",
    "        [\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\"]\n",
    "    ).reset_index(drop=True)\n",
    "else:\n",
    "    # DataFrame rỗng, KHÔNG dummy row\n",
    "    submission6 = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"video_id\",\n",
    "            \"agent_id\",\n",
    "            \"target_id\",\n",
    "            \"action\",\n",
    "            \"start_frame\",\n",
    "            \"stop_frame\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Thêm row_id (kể cả khi rỗng)\n",
    "submission6.insert(0, \"row_id\", np.arange(len(submission6), dtype=np.int64))\n",
    "\n",
    "sub_path = WORKING_DIR / \"submission6.csv\"\n",
    "submission6.to_csv(sub_path, index=False)\n",
    "print(f\"Saved PleasantMeerkat submission to {sub_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5780d42",
   "metadata": {
    "papermill": {
     "duration": 0.019295,
     "end_time": "2025-12-13T17:39:17.997484",
     "exception": false,
     "start_time": "2025-12-13T17:39:17.978189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SparklingTapir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2166ece4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:18.037933Z",
     "iopub.status.busy": "2025-12-13T17:39:18.037352Z",
     "iopub.status.idle": "2025-12-13T17:39:18.154325Z",
     "shell.execute_reply": "2025-12-13T17:39:18.153663Z"
    },
    "papermill": {
     "duration": 0.138241,
     "end_time": "2025-12-13T17:39:18.155532",
     "exception": false,
     "start_time": "2025-12-13T17:39:18.017291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "\n",
    "# 1) Xóa mọi thứ trong /kaggle/working trừ .csv\n",
    "for path in WORKING_DIR.iterdir():\n",
    "    # giữ lại file .csv\n",
    "    if path.is_file() and path.suffix == \".csv\":\n",
    "        continue\n",
    "\n",
    "    if path.is_file():\n",
    "        try:\n",
    "            path.unlink()\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove file {path}: {e}\")\n",
    "    elif path.is_dir():\n",
    "        try:\n",
    "            shutil.rmtree(path, ignore_errors=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove dir {path}: {e}\")\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdd2471c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:18.195697Z",
     "iopub.status.busy": "2025-12-13T17:39:18.195471Z",
     "iopub.status.idle": "2025-12-13T17:39:18.361969Z",
     "shell.execute_reply": "2025-12-13T17:39:18.361042Z"
    },
    "papermill": {
     "duration": 0.189157,
     "end_time": "2025-12-13T17:39:18.363258",
     "exception": false,
     "start_time": "2025-12-13T17:39:18.174101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đọc test.csv cho lab SparklingTapir ...\n",
      "Behaviors (self) dùng để predict: []\n",
      "Behaviors (pair) dùng để predict: ['attack', 'defend', 'escape', 'mount']\n",
      "Saved SparklingTapir submission to /kaggle/working/submission7.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "np.seterr(invalid=\"ignore\", divide=\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    \"\"\"\n",
    "    Chứa cấu hình tham số (Hyperparameters).\n",
    "    \"\"\"\n",
    "    fps: float = 30.0\n",
    "    pix_per_cm: float = 1.0\n",
    "    smooth_sigma: float = 1.0\n",
    "    use_pairwise: bool = True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. AGENT CONTEXT\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class AgentContext:\n",
    "    \"\"\"\n",
    "    Container chứa dữ liệu đã tiền xử lý của một con chuột.\n",
    "    Giúp tránh việc tính toán lại vận tốc/gia tốc nhiều lần.\n",
    "    \"\"\"\n",
    "    idx: pd.Index          # Index frame\n",
    "    pos: np.ndarray        # [F, 2] cm\n",
    "    vel: np.ndarray        # [F, 2] cm/s\n",
    "    speed: np.ndarray      # [F, 1] cm/s\n",
    "    acc: np.ndarray        # [F, 2] cm/s^2\n",
    "    \n",
    "    cx: pd.Series          # Series tọa độ X (để dùng rolling)\n",
    "    cy: pd.Series          # Series tọa độ Y\n",
    "    speed_series: pd.Series # Series tốc độ\n",
    "    \n",
    "    raw_df: Optional[pd.DataFrame] = None # Dữ liệu gốc các bộ phận \n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. FEATURE EXTRACTOR\n",
    "# =============================================================================\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Class trích xuất đặc trưng hành vi từ dữ liệu tracking.\n",
    "    \"\"\"\n",
    "    def __init__(self, fps: float, pix_per_cm: float, smooth_sigma: float = 1.0, use_pairwise: bool = True):\n",
    "        # Map tham số từ init vào Config\n",
    "        self.cfg = FeatureConfig(\n",
    "            fps=float(fps), \n",
    "            pix_per_cm=float(pix_per_cm), \n",
    "            smooth_sigma=smooth_sigma,\n",
    "            use_pairwise=use_pairwise\n",
    "        )\n",
    "        \n",
    "        # Đăng ký các hàm feature sẽ chạy\n",
    "        self.feature_registry = {\n",
    "            \"kinematics\": self._feat_basic_kinematics,\n",
    "            \"multiscale\": self._feat_multiscale,\n",
    "            \"long_range\": self._feat_long_range,\n",
    "            \"cumulative\": self._feat_cumulative,\n",
    "            \"curvature\": self._feat_curvature,\n",
    "            \"speed_asym\": self._feat_speed_asym,\n",
    "            \"gauss_shift\": self._feat_gauss_shift,\n",
    "            \"pose\": self._feat_pose_shape,\n",
    "            \"a\": self._feat_attack_defend,\n",
    "            \"follow\": self._feat_follow_pattern,\n",
    "            \"short\": self._feat_shortburst_social,\n",
    "            \"pairwise\": self._feat_pairwise\n",
    "        }\n",
    "\n",
    "    # --- Helpers ---\n",
    "    def _scale(self, n_frames_30fps: int) -> int:\n",
    "        \"\"\"Quy đổi số frame từ chuẩn 30fps sang fps thực tế của video.\"\"\"\n",
    "        return max(1, int(round(n_frames_30fps * self.cfg.fps / 30.0)))\n",
    "\n",
    "    def _to_cm(self, arr):\n",
    "        \"\"\"Chuyển pixel -> cm.\"\"\"\n",
    "        return arr / self.cfg.pix_per_cm\n",
    "\n",
    "    def _smooth(self, x):\n",
    "        \"\"\"Làm mượt dữ liệu bằng Gaussian filter.\"\"\"\n",
    "        if self.cfg.smooth_sigma is None or x.shape[0] < 3: return x\n",
    "        if np.all(np.isnan(x)): return x\n",
    "        return gaussian_filter1d(x, sigma=self.cfg.smooth_sigma, axis=0, mode=\"nearest\")\n",
    "\n",
    "    def _forward_fill_nan(self, pos):\n",
    "        \"\"\"\n",
    "        Điền dữ liệu thiếu (NaN) bằng giá trị hợp lệ trước đó (Forward Fill).\n",
    "        \"\"\"\n",
    "        if np.all(np.isnan(pos)):\n",
    "            return np.zeros_like(pos)\n",
    "\n",
    "        pos_ffill = pos.copy()\n",
    "        mask = np.any(~np.isnan(pos_ffill), axis=1)\n",
    "        if not mask.any():\n",
    "            return np.zeros_like(pos_ffill)\n",
    "\n",
    "        valid_idx = np.where(mask)[0]\n",
    "        first, last = valid_idx[0], valid_idx[-1]\n",
    "        pos_ffill[:first] = pos_ffill[first]\n",
    "        pos_ffill[last + 1:] = pos_ffill[last]\n",
    "        df_temp = pd.DataFrame(pos_ffill)\n",
    "        df_temp = df_temp.ffill()\n",
    "        return df_temp.to_numpy()\n",
    "    \n",
    "    def _speed_series(self, cx: pd.Series, cy: pd.Series) -> pd.Series:\n",
    "        dx = cx.diff()\n",
    "        dy = cy.diff()\n",
    "        v = np.hypot(dx, dy).fillna(0.0) * self.cfg.fps\n",
    "        return v.astype(\"float32\")\n",
    "    \n",
    "    def _roll_future_mean(self, s: pd.Series, w: int, min_p: int = 1) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).mean().iloc[::-1]\n",
    "\n",
    "    def _roll_future_var(self, s: pd.Series, w: int, min_p: int = 2) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).var().iloc[::-1]\n",
    "\n",
    "    # --- Core Logic ---\n",
    "    def _compute_kinematics(self, pos_px: np.ndarray):\n",
    "        \"\"\"\n",
    "        Tính toán vật lý cơ bản: Pos(cm), Vel, Speed, Acc.\n",
    "        Input: Array [Frames, 2] (pixel).\n",
    "        Output: Tuple (pos_cm, vel, speed, acc).\n",
    "        \"\"\"\n",
    "        pos_ffill = self._forward_fill_nan(pos_px)\n",
    "        pos_cm = self._to_cm(pos_ffill.astype(np.float32))\n",
    "        pos_cm = self._smooth(pos_cm)                                               # [F, 2]\n",
    "\n",
    "        dt = 1.0 / self.cfg.fps\n",
    "        vel = np.zeros_like(pos_cm, dtype=np.float32)\n",
    "        vel[1:] = (pos_cm[1:] - pos_cm[:-1]) / dt                                   # [F, 2: (vx, vy)]\n",
    "        speed = np.linalg.norm(vel, axis=1, keepdims=True).astype(np.float32)       # [F, 1]\n",
    "\n",
    "        acc = np.zeros_like(pos_cm, dtype=np.float32)                          \n",
    "        acc[1:] = (vel[1:] - vel[:-1]) / dt                                         # [F, 2:(ax, ay)]\n",
    "        return pos_cm.astype(np.float32), vel, speed, acc\n",
    "\n",
    "    def _build_context(self, frames, pos_px, mouse_df=None) -> AgentContext:\n",
    "        \"\"\"\n",
    "        Tạo AgentContext chứa đầy đủ thông tin vật lý của 1 con chuột.\n",
    "        \"\"\"\n",
    "        p, v, s, a = self._compute_kinematics(pos_px)\n",
    "        idx = pd.Index(frames, name=\"frame\")\n",
    "        \n",
    "        return AgentContext(\n",
    "            idx=idx, pos=p, vel=v, speed=s, acc=a, \n",
    "            cx=pd.Series(p[:, 0], index=idx), \n",
    "            cy=pd.Series(p[:, 1], index=idx), \n",
    "            speed_series=pd.Series(s[:, 0], index=idx), \n",
    "            raw_df=mouse_df\n",
    "        )\n",
    "\n",
    "    # --- Feature Modules ---\n",
    "    def _feat_basic_kinematics(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Lấy các giá trị thô: tọa độ x, y, vận tốc vx, vy, tốc độ, gia tốc ax, ay.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"a_x\": ctx.pos[:, 0], \"a_y\": ctx.pos[:, 1],\n",
    "            \"a_vx\": ctx.vel[:, 0], \"a_vy\": ctx.vel[:, 1],\n",
    "            \"a_speed\": ctx.speed[:, 0],\n",
    "            \"a_ax\": ctx.acc[:, 0], \"a_ay\": ctx.acc[:, 1]\n",
    "        }\n",
    "\n",
    "    def _feat_multiscale(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tính tốc độ trung bình (Mean) và độ lệch chuẩn (Std) ở đa mức thời gian.\n",
    "        Feature 'sp_ratio' đo độ bùng nổ (Burstiness).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        speed = ctx.speed_series\n",
    "        frame_scales = [10, 40, 160]\n",
    "        for scale in frame_scales:\n",
    "            ws = self._scale(scale)\n",
    "            if len(speed) >= ws:\n",
    "                roller = speed.rolling(ws, min_periods=max(1, ws//4), center=True)\n",
    "                feats[f\"sp_m{scale}\"] = roller.mean().astype(\"float32\")\n",
    "                feats[f\"sp_s{scale}\"] = roller.std().astype(\"float32\")\n",
    "        feats[f\"sp_ratio\"] = feats[\"sp_m10\"] / (feats[\"sp_m160\"] + 1e-6)\n",
    "        return feats \n",
    "        \n",
    "    def _feat_long_range(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng ngữ cảnh dài hạn:\n",
    "        - x_ml, y_ml: Vị trí trung bình trong quá khứ.\n",
    "        - sp_pct: Xếp hạng (percentile) của tốc độ hiện tại so với quá khứ.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        speed = ctx.speed_series\n",
    "\n",
    "        for window in [120, 240]:\n",
    "            ws = self._scale(window)\n",
    "            if len(ctx.cx) >= ws:\n",
    "                feats[f\"x_ml{window}\"] = ctx.cx.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "                feats[f\"y_ml{window}\"] = ctx.cy.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "\n",
    "        for span in [60, 120]:\n",
    "            s = self._scale(span)\n",
    "            feats[f\"x_e{span}\"] = ctx.cx.ewm(span=s, min_periods=1).mean()\n",
    "            feats[f\"y_e{span}\"] = ctx.cy.ewm(span=s, min_periods=1).mean()\n",
    "\n",
    "        for window in [60, 120]:\n",
    "            ws = self._scale(window)\n",
    "            if len(speed) >= ws:\n",
    "                feats[f\"sp_pct{window}\"] = speed.rolling(\n",
    "                    ws, min_periods=max(5, ws // 6), center=True\n",
    "                ).rank(pct=True)\n",
    "        return feats\n",
    "    \n",
    "\n",
    "    def _feat_curvature(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        feats = {}\n",
    "\n",
    "        vel_x, vel_y = ctx.vel[:, 0], ctx.vel[:, 1]\n",
    "        acc_x, acc_y = ctx.acc[:, 0], ctx.acc[:, 1]\n",
    "        cross_prod = vel_x * acc_y - vel_y * acc_x\n",
    "        vel_mag = np.sqrt(vel_x**2 + vel_y**2)\n",
    "        moving_mask = vel_mag > 2.0\n",
    "        vel_mag_safe = np.maximum(vel_mag, 0.1 / self.cfg.fps)\n",
    "        raw_curv = cross_prod / (vel_mag_safe**3)\n",
    "        raw_curv = np.where(moving_mask, raw_curv, 0.0)\n",
    "        min_turn_radius_cm = 0.5\n",
    "        max_k = 1.0 / min_turn_radius_cm\n",
    "        raw_curv = np.clip(raw_curv, -max_k, max_k)\n",
    "        abs_curv = np.abs(raw_curv)\n",
    "        abs_curv_series = pd.Series(abs_curv, index=ctx.idx)\n",
    "\n",
    "        for w in [30, 60]:\n",
    "            ws = self._scale(w)\n",
    "            min_p = max(ws // 3, 1)\n",
    "            feats[f\"curv_mean_{w}\"] = abs_curv_series.rolling(ws, min_periods=min_p).mean()\n",
    "\n",
    "        angle = np.arctan2(vel_y, vel_x)\n",
    "        angle_series = pd.Series(angle, index=ctx.idx)\n",
    "        angle_change = np.abs(angle_series.diff().fillna(0.0))\n",
    "        angle_change = np.where(angle_change > np.pi, 2 * np.pi - angle_change, angle_change)\n",
    "        angle_change_series = pd.Series(angle_change, index=ctx.idx)\n",
    "        angle_change_series = pd.Series(np.where(moving_mask, angle_change_series, 0.0), index=ctx.idx)\n",
    "\n",
    "        ws = self._scale(30)\n",
    "        feats[\"turn_rate_30\"] = angle_change_series.rolling(ws, min_periods=max(ws // 3, 1)).sum()\n",
    "\n",
    "        return feats\n",
    "    \n",
    "    def _feat_cumulative(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tổng quãng đường di chuyển trong một khoảng thời gian dài xung quanh frame hiện tại.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        L = max(1, self._scale(180))\n",
    "        step = np.hypot(ctx.cx.diff(), ctx.cy.diff()).fillna(0.0)\n",
    "        path = step.rolling(2 * L + 1, min_periods=max(5, L // 6), center=True).sum()\n",
    "        feats[\"path_cum180\"] =  path.fillna(0.0).astype(\"float32\")\n",
    "        return feats\n",
    "\n",
    "    def _feat_speed_asym(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Bất đối xứng tốc độ (Tương lai - Quá khứ).\n",
    "        \"\"\"\n",
    "        w = max(3, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        v_past = v.rolling(w, min_periods=1).mean()\n",
    "        v_fut = self._roll_future_mean(v, w, min_p=1)\n",
    "        return {\"spd_asym_1s\": (v_fut - v_past).fillna(0.0)}\n",
    "    \n",
    "    def _feat_gauss_shift(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Độ lệch Gaussian (KL Divergence) giữa quá khứ và tương lai.\n",
    "        Đo lường sự thay đổi trạng thái thống kê.\n",
    "        \"\"\"\n",
    "        w = max(5, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        mu_p = v.rolling(w, min_periods=1).mean()\n",
    "        va_p = v.rolling(w, min_periods=1).var().clip(lower=1e-6)\n",
    "        mu_f = self._roll_future_mean(v, w, min_p=1)\n",
    "        va_f = self._roll_future_var(v, w, min_p=1).clip(lower=1e-6)\n",
    "\n",
    "        kl_pf = 0.5 * (\n",
    "            (va_p / va_f) + ((mu_f - mu_p) ** 2) / va_f - 1.0 + np.log(va_f / va_p)\n",
    "        )\n",
    "        kl_fp = 0.5 * (\n",
    "            (va_f / va_p) + ((mu_p - mu_f) ** 2) / va_p - 1.0 + np.log(va_p / va_f)\n",
    "        )\n",
    "        return {\n",
    "            \"spd_symkl_1s\": (kl_pf + kl_fp).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        }\n",
    "\n",
    "    def _feat_avoidance_trajectory(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Tính toán quỹ đạo né tránh:\n",
    "        1. Relative Heading: Góc di chuyển so với hướng tới đối thủ.\n",
    "        2. Future Distance Gain: Dự báo xem hành động này có giúp chuột ra xa đối thủ trong tương lai không.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        if target_ctx is None: \n",
    "            return feats\n",
    "\n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "\n",
    "        # --- 1. RELATIVE HEADING (Góc lệch hướng đi) ---\n",
    "        # Vector từ Tôi -> Địch\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        # Góc hướng tới địch (Angle to Target)\n",
    "        angle_to_target = np.arctan2(rel_vec[:, 1], rel_vec[:, 0])\n",
    "        \n",
    "        # Góc di chuyển của Tôi (My Heading)\n",
    "        my_heading = np.arctan2(ctx.vel[:, 1], ctx.vel[:, 0])\n",
    "        \n",
    "        # Độ lệch góc (Absolute Difference)\n",
    "        # Cần xử lý wrap góc (ví dụ: lệch giữa 179 độ và -179 độ là 2 độ chứ ko phải 358)\n",
    "        diff = np.abs(angle_to_target - my_heading)\n",
    "        diff = np.minimum(diff, 2*np.pi - diff) # Chuẩn hóa về [0, pi]\n",
    "        \n",
    "        # Feature: Cosine của góc lệch\n",
    "        # 1.0 (0 độ) -> Lao vào\n",
    "        # 0.0 (90 độ) -> AVOID (Lách ngang)\n",
    "        # -1.0 (180 độ) -> Escape\n",
    "        feats[\"heading_rel_cos\"] = pd.Series(np.cos(diff), index=idx, dtype=\"float32\")\n",
    "        \n",
    "        # Feature: Góc lệch tuyệt đối (đổi ra độ cho dễ hình dung nếu cần, ở đây để rad)\n",
    "        feats[\"heading_rel_abs\"] = pd.Series(diff, index=idx, dtype=\"float32\")\n",
    "\n",
    "\n",
    "        # --- 2. FUTURE DISTANCE GAIN (Hiệu quả tránh né) ---\n",
    "        # \"Sau 15 frame (0.5s) hoặc 30 frame (1s), mình có xa nó ra không?\"\n",
    "        \n",
    "        dist_now = np.linalg.norm(rel_vec, axis=1)\n",
    "        s_dist = pd.Series(dist_now, index=idx)\n",
    "        \n",
    "        scales = [15, 30] # 0.5s và 1s\n",
    "        for w in scales:\n",
    "            ws = self._scale(w)\n",
    "            \n",
    "            # Lấy khoảng cách ở tương lai (shift ngược lên)\n",
    "            # s.shift(-ws) là giá trị của t + ws\n",
    "            dist_future = s_dist.shift(-ws)\n",
    "            gain = dist_future - s_dist\n",
    "            \n",
    "            feats[f\"dist_gain_{w}f\"] = gain.fillna(0.0).astype(\"float32\")\n",
    "\n",
    "        return feats\n",
    "    \n",
    "    def _extract_part(self, ctx: AgentContext, part: str) -> Optional[np.ndarray]:\n",
    "        if ctx.raw_df is None: return None\n",
    "        if part not in ctx.raw_df.columns.get_level_values(0): return None\n",
    "        try:\n",
    "            sub_df = ctx.raw_df.xs(part, axis=1, level=0)[[\"x\", \"y\"]].reindex(ctx.idx)\n",
    "        except KeyError: return None\n",
    "        raw = sub_df.to_numpy()\n",
    "        raw = self._forward_fill_nan(raw)\n",
    "        cm = self._to_cm(raw.astype(np.float32))\n",
    "        return self._smooth(cm)\n",
    "    \n",
    "    def _extract_parts_dict(self, ctx: AgentContext, parts: List[str] = None) -> Dict[str, Optional[np.ndarray]]:\n",
    "        out = {}\n",
    "        for p in parts:\n",
    "            out[p] = self._extract_part(ctx, p)\n",
    "        return out\n",
    "        \n",
    "    def _feat_pose_shape(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Placeholder cho các đặc trưng hình dáng (Elongation, Body Angle...).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "\n",
    "        def zero(): return pd.Series(0.0, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        def dist(k1, k2):\n",
    "            p1, p2 = parts.get(k1), parts.get(k2)\n",
    "            if p1 is None or p2 is None: return zero()\n",
    "            d = np.linalg.norm(p1 - p2, axis=1)\n",
    "            return pd.Series(d, index=ctx.idx, dtype=\"float32\")\n",
    "        \n",
    "        def body_angle():\n",
    "            if parts.get(\"nose\") is None: return zero()\n",
    "            if parts.get(\"body_center\") is None: return zero()\n",
    "            if parts.get(\"tail_base\") is None: return zero()\n",
    "\n",
    "            v1 = parts.get(\"nose\") - parts.get(\"body_center\")\n",
    "            v2 = parts.get(\"tail_base\") - parts.get(\"body_center\")\n",
    "            dot_product = np.sum(v1 * v2, axis=1)\n",
    "            mag = np.linalg.norm(v1, axis=1) * np.linalg.norm(v2, axis=1)\n",
    "            cos_angle = np.clip(dot_product / (mag + 1e-6), -1.0, 1.0).astype(\"float32\")\n",
    "            return cos_angle\n",
    "        \n",
    "        def elongation():\n",
    "            if parts.get(\"nose\")          is None: return zero()\n",
    "            if parts.get(\"tail_base\")     is None: return zero()\n",
    "            if parts.get(\"lateral_left\")  is None: return zero()\n",
    "            if parts.get(\"lateral_right\") is None: return zero()\n",
    "\n",
    "            d1 = dist(\"nose\", \"tail_base\")\n",
    "            d2 = dist(\"lateral_left\", \"lateral_right\")\n",
    "            elongation = d1 / (d2 + 1e-6).astype(\"float32\")\n",
    "            return elongation\n",
    "        \n",
    "        def vel(part: str, n_frames_30fps: int) -> Dict:\n",
    "            part_pos = self._extract_part(ctx, part)\n",
    "            if part_pos is None: return zero()\n",
    "            \n",
    "            s_x = pd.Series(part_pos[:, 0], index=ctx.idx)\n",
    "            s_y = pd.Series(part_pos[:, 1], index=ctx.idx)\n",
    "            raw_speed = self._speed_series(s_x, s_y)\n",
    "\n",
    "            ws = self._scale(n_frames_30fps)\n",
    "            val = raw_speed.rolling(ws, min_periods=1, center=True).mean()\n",
    "            return val.astype(\"float32\")\n",
    "\n",
    "\n",
    "        target_parts = [\"nose\", \"neck\", \"body_center\", \"tail_base\", \n",
    "                        \"ear_left\", \"ear_right\", \n",
    "                        \"lateral_left\", \"lateral_right\", \"tail_midpoint\", \"tail_tip\"]\n",
    "        \n",
    "        parts = self._extract_parts_dict(ctx, target_parts)\n",
    "\n",
    "        feats[\"a_body_width\"]                = dist(\"lateral_left\", \"lateral_right\")\n",
    "        feats[\"aa_nose_bodycenter_dist\"]     = dist(\"nose\", \"body_center\")\n",
    "        #feats[\"aa_nose_tailbase_dist\"]       = dist(\"nose\", \"tail_base\")\n",
    "        #feats[\"aa_bodycenter_tailbase_dist\"] = dist(\"body_center\", \"tail_base\")\n",
    "        \n",
    "        feats[\"aa_bodycenter_ear_l_dist\"]    = dist(\"body_center\", \"ear_left\")\n",
    "        feats[\"aa_bodycenter_ear_r_dist\"]    = dist(\"body_center\", \"ear_right\")\n",
    "        feats[\"aa_bodycenter_lateral_l_dist\"]= dist(\"body_center\", \"lateral_left\")\n",
    "        feats[\"aa_bodycenter_lateral_r_dist\"]= dist(\"body_center\", \"lateral_right\")\n",
    "        \n",
    "        feats[\"a_body_angle\"]                = body_angle()\n",
    "        feats[\"a_elongation\"]                = elongation()\n",
    "        feats[\"a_tail_base_vel_500ms\"]       = vel(\"tail_base\", 15)\n",
    "        feats[\"a_tail_base_vel_1000ms\"]      = vel(\"tail_base\", 30)\n",
    "        feats[\"a_tail_base_vel_2000ms\"]      = vel(\"tail_base\", 60)\n",
    "        feats[\"a_tail_base_vel_3000ms\"]      = vel(\"tail_base\", 90)\n",
    "        feats[\"a_nose_vel_500ms\"]            = vel(\"nose\", 15)\n",
    "        feats[\"a_nose_vel_1000ms\"]           = vel(\"nose\", 30)\n",
    "        feats[\"a_nose_vel_2000ms\"]           = vel(\"nose\", 60)\n",
    "        feats[\"a_nose_vel_3000ms\"]           = vel(\"nose\", 90)\n",
    "        # feats[\"a_ear_right_vel_500ms\"]       = vel(\"ear_right\", 15)\n",
    "        # feats[\"a_ear_right_vel_1000ms\"]      = vel(\"ear_right\", 30)\n",
    "        # feats[\"a_ear_right_vel_2000ms\"]      = vel(\"ear_right\", 60)\n",
    "        # feats[\"a_ear_right_vel_3000ms\"]      = vel(\"ear_right\", 90)\n",
    "        # len_1 = dist(\"tail_base\", \"tail_midpoint\")\n",
    "        # len_2 = dist(\"tail_midpoint\", \"tail_tip\")\n",
    "        # len_full = dist(\"tail_base\", \"tail_tip\")\n",
    "        # feats[\"tail_curl\"] = ((len_1 + len_2) / (len_full + 1e-6)).astype(\"float32\")\n",
    "        return feats\n",
    "\n",
    "    def _feat_attack_defend(\n",
    "        self,\n",
    "        ctx: AgentContext,\n",
    "        target_ctx: AgentContext = None,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Feature chuyên để phân biệt attack vs defend cho cặp chuột.\n",
    "    \n",
    "        - attack: cả hai chuyển động mạnh, speed & biến thiên speed lớn,\n",
    "                  khoảng cách nhỏ, đổi hướng loạn xạ.\n",
    "        - defend: agent đứng gần đối thủ, speed thấp, quay mặt về phía đối thủ.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "    \n",
    "        def zero() -> pd.Series:\n",
    "            return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # -----------------------------\n",
    "        # 1. SPEED & BIẾN THIÊN SPEED\n",
    "        # -----------------------------\n",
    "        v_a = ctx.speed_series        # agent speed (cm/s)\n",
    "        v_t = target_ctx.speed_series # target speed\n",
    "    \n",
    "        ws_short = self._scale(10)  # ~0.3s\n",
    "        mp_short = max(ws_short // 3, 1)\n",
    "    \n",
    "        def roll_mean(s: pd.Series) -> pd.Series:\n",
    "            return (\n",
    "                s.rolling(ws_short, min_periods=mp_short)\n",
    "                 .mean()\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        def roll_std(s: pd.Series) -> pd.Series:\n",
    "            return (\n",
    "                s.rolling(ws_short, min_periods=mp_short)\n",
    "                 .std()\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        a_spd_mean = roll_mean(v_a)\n",
    "        t_spd_mean = roll_mean(v_t)\n",
    "        a_spd_std  = roll_std(v_a)\n",
    "        t_spd_std  = roll_std(v_t)\n",
    "    \n",
    "        feats[\"atk_a_speed_mean_10\"] = a_spd_mean\n",
    "        feats[\"atk_t_speed_mean_10\"] = t_spd_mean\n",
    "        feats[\"atk_a_speed_std_10\"]  = a_spd_std\n",
    "        feats[\"atk_t_speed_std_10\"]  = t_spd_std\n",
    "    \n",
    "        # \"Violence\" = tổng biến thiên speed hai bên\n",
    "        feats[\"atk_speed_violence_10\"] = (\n",
    "            a_spd_std + t_spd_std\n",
    "        ).astype(\"float32\")\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 2. RELATIVE DISTANCE & STABILITY\n",
    "        # -------------------------------------------------\n",
    "        # dùng rel_dist từ pairwise nếu có, còn không thì tính lại\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1).astype(\"float32\")\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "        feats[\"atk_rel_dist\"] = rel_dist_s\n",
    "    \n",
    "        # deviance distance (attack: distance thay đổi nhanh, defend: ổn định)\n",
    "        rel_dist_diff = rel_dist_s.diff().abs().fillna(0.0)\n",
    "        feats[\"atk_rel_dist_diff_abs\"] = rel_dist_diff.astype(\"float32\")\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 3. TURNING / DIRECTION CHANGE (cho \"loạn xạ\")\n",
    "        # -------------------------------------------------\n",
    "        vx_a, vy_a = ctx.vel[:, 0], ctx.vel[:, 1]\n",
    "        angle_a = np.arctan2(vy_a, vx_a)\n",
    "        angle_a = pd.Series(angle_a, index=idx)\n",
    "        dtheta = angle_a.diff().fillna(0.0).abs()\n",
    "        dtheta = np.where(dtheta > np.pi, 2 * np.pi - dtheta, dtheta)\n",
    "        dtheta = pd.Series(dtheta, index=idx)\n",
    "    \n",
    "        feats[\"atk_agent_turn_rate_10\"] = (\n",
    "            dtheta.rolling(ws_short, min_periods=mp_short)\n",
    "                  .sum()\n",
    "                  .fillna(0.0)\n",
    "                  .astype(\"float32\")\n",
    "        )\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 4. ORIENTATION CHO DEFEND (quay đầu khè)\n",
    "        # -------------------------------------------------\n",
    "        # body vector agent: tail_base -> nose\n",
    "        parts_a = self._extract_parts_dict(\n",
    "            ctx,\n",
    "            [\"nose\", \"tail_base\"],\n",
    "        )\n",
    "        a_nose = parts_a.get(\"nose\")\n",
    "        a_tail = parts_a.get(\"tail_base\")\n",
    "    \n",
    "        # dùng target body_center làm \"thân\" để đi tới\n",
    "        parts_t = self._extract_parts_dict(\n",
    "            target_ctx,\n",
    "            [\"body_center\"],\n",
    "        )\n",
    "        t_body = parts_t.get(\"body_center\")\n",
    "    \n",
    "        if a_nose is not None and a_tail is not None and t_body is not None:\n",
    "            body_vec_a = a_nose - a_tail  # tail -> head (agent)\n",
    "            vec_to_target = t_body - a_tail\n",
    "    \n",
    "            dot = np.sum(body_vec_a * vec_to_target, axis=1)\n",
    "            mag = np.linalg.norm(body_vec_a, axis=1) * np.linalg.norm(vec_to_target, axis=1)\n",
    "    \n",
    "            cos_orient = np.zeros_like(dot, dtype=\"float32\")\n",
    "            valid = mag > 1e-3\n",
    "            cos_orient[valid] = np.clip(dot[valid] / mag[valid], -1.0, 1.0)\n",
    "    \n",
    "            feats[\"def_body_facing_cos\"] = pd.Series(cos_orient, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            feats[\"def_body_facing_cos\"] = zero()\n",
    "    \n",
    "        # Ý nghĩa:\n",
    "        #   +1 ~ agent tail->head hướng thẳng tới target (đối mặt/khè)\n",
    "        #   0  ~ vuông góc\n",
    "        #   -1 ~ quay lưng\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 5. DEFENSIVE PATTERN: ĐỨNG GẦN, ÍT DI CHUYỂN, QUAY MẶT\n",
    "        # -------------------------------------------------\n",
    "        # score mềm: high nếu \"đứng giữ vị trí + facing\"\n",
    "        near_mask = (rel_dist_s < 5.0).astype(\"float32\")  # <5cm tùy lab chỉnh\n",
    "        low_speed = (a_spd_mean < 3.0).astype(\"float32\")  # cm/s, chỉnh tùy fps\n",
    "    \n",
    "        def_cos = feats[\"def_body_facing_cos\"]\n",
    "    \n",
    "        feats[\"def_posture_score\"] = (\n",
    "            near_mask * low_speed * (0.5 * (def_cos + 1.0))\n",
    "        ).astype(\"float32\")\n",
    "        # def_posture_score ~ 1: gần, chậm, đang đối mặt\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 6. CLEAN NaN / Inf\n",
    "        # -------------------------------------------------\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "\n",
    "\n",
    "    \n",
    "    def _feat_follow_pattern(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng hành vi FOLLOW:\n",
    "          - Agent ở gần target\n",
    "          - Cùng hướng (body + velocity)\n",
    "          - Tốc độ vừa phải\n",
    "          - Khoảng cách tương đối ổn định trong 0.5–1s\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 1. CÁC ĐẠI LƯỢNG CƠ BẢN ---\n",
    "        # Vector Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # Speed agent/target\n",
    "        a_speed = ctx.speed_series.astype(\"float32\")\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "    \n",
    "        # Body vector: nose - tail/body_center\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\", \"ear_left\", \"ear_right\"])\n",
    "        parts_t = self._extract_parts_dict(target_ctx, [\"nose\", \"tail_base\", \"ear_right\", \"ear_left\"])\n",
    "    \n",
    "        def body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"nose\")\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if head is None or tail is None:\n",
    "                return None\n",
    "            return head - tail\n",
    "    \n",
    "        a_body = body_vec(parts_a)\n",
    "        t_body = body_vec(parts_t)\n",
    "    \n",
    "        if a_body is not None and t_body is not None:\n",
    "            dot_bt = np.sum(a_body * t_body, axis=1)\n",
    "            mag_bt = np.linalg.norm(a_body, axis=1) * np.linalg.norm(t_body, axis=1)\n",
    "            cos_body = np.clip(dot_bt / (mag_bt + 1e-6), -1.0, 1.0)\n",
    "            cos_body_s = pd.Series(cos_body, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            cos_body_s = zero()\n",
    "    \n",
    "        # Velocity hướng\n",
    "        a_vel = ctx.vel\n",
    "        t_vel = target_ctx.vel\n",
    "        a_speed_np = np.linalg.norm(a_vel, axis=1)\n",
    "        t_speed_np = np.linalg.norm(t_vel, axis=1)\n",
    "        moving_mask = (a_speed_np > 1e-3) & (t_speed_np > 1e-3)\n",
    "    \n",
    "        # cos giữa hướng velocity 2 con\n",
    "        dot_v = np.sum(a_vel * t_vel, axis=1)\n",
    "        mag_v = a_speed_np * t_speed_np + 1e-6\n",
    "        cos_vel = np.zeros_like(dot_v, dtype=\"float32\")\n",
    "        cos_vel[moving_mask] = np.clip(dot_v[moving_mask] / mag_v[moving_mask], -1.0, 1.0)\n",
    "        cos_vel_s = pd.Series(cos_vel, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 2. WINDOW NGẮN (FOLLOW LÀ PATTERN DÀI HƠN ATTACK) ---\n",
    "        for w30 in [15, 30, 60]:   # ~0.5s, 1s, 2s\n",
    "            ws = self._scale(w30)\n",
    "            min_p = max(ws // 3, 1)\n",
    "    \n",
    "            # Khoảng cách trung bình & độ dao động\n",
    "            m_dist = rel_dist_s.rolling(ws, min_periods=min_p).mean()\n",
    "            s_dist = rel_dist_s.rolling(ws, min_periods=min_p).std()\n",
    "    \n",
    "            # Cùng hướng (body + velocity)\n",
    "            m_cos_body = cos_body_s.rolling(ws, min_periods=min_p).mean()\n",
    "            m_cos_vel  = cos_vel_s.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            # Tốc độ vừa phải\n",
    "            m_sp_a = a_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            m_sp_t = t_speed.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            feats[f\"follow_dist_mean_{w30}\"] = m_dist\n",
    "            feats[f\"follow_dist_std_{w30}\"]  = s_dist\n",
    "            feats[f\"follow_cos_body_mean_{w30}\"] = m_cos_body\n",
    "            feats[f\"follow_cos_vel_mean_{w30}\"]  = m_cos_vel\n",
    "            feats[f\"follow_speed_agent_mean_{w30}\"] = m_sp_a\n",
    "            feats[f\"follow_speed_target_mean_{w30}\"] = m_sp_t\n",
    "    \n",
    "        # Clean\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "\n",
    "    def _feat_submission_temporal(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng 'Ký ức sợ hãi' (Fear Memory) để bắt Submit tĩnh.\n",
    "        Giúp phân biệt Submit (sau khi bị đánh) vs Rest (bình yên).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        if target_ctx is None: return feats\n",
    "        \n",
    "        idx = ctx.idx\n",
    "        \n",
    "        # --- 1. XÂY DỰNG TÍN HIỆU XUNG ĐỘT GỐC (RAW CONFLICT SIGNAL) ---\n",
    "        # Conflict = (Nó nhanh) * (Nó hướng về tôi) * (Ở gần)\n",
    "        \n",
    "        # A. Nó hướng về tôi không? (Gaze Cosine)\n",
    "        # Vector nối Tôi -> Nó\n",
    "        vec_to_target = target_ctx.pos - ctx.pos\n",
    "        dist = np.linalg.norm(vec_to_target, axis=1)\n",
    "        dist_safe = pd.Series(dist, index=idx).replace(0, 1e-6)\n",
    "        \n",
    "        # Vector vận tốc của Nó\n",
    "        t_vel = target_ctx.vel\n",
    "        \n",
    "        # Dot product: Vận tốc Nó . Vector hướng về Tôi (Ngược dấu với vec_to_target)\n",
    "        # vec_target_to_me = -vec_to_target\n",
    "        # dot > 0 nghĩa là nó đang lao về phía tôi\n",
    "        dot_threat = np.sum(t_vel * (-vec_to_target), axis=1)\n",
    "        \n",
    "        # Threat Score tức thời (cm/s hướng về nạn nhân)\n",
    "        # Chỉ tính khi nó lại gần (< 15cm)\n",
    "        threat_raw = (dot_threat / dist_safe).clip(lower=0) \n",
    "        threat_raw = threat_raw * (dist_safe < 15.0).astype(float)\n",
    "        threat_series = pd.Series(threat_raw, index=idx, dtype=\"float32\")\n",
    "\n",
    "        # --- 2. KÝ ỨC SỢ HÃI (FEAR MEMORY - QUAN TRỌNG NHẤT) ---\n",
    "        # Dùng Rolling Max để \"kéo dài\" nỗi sợ.\n",
    "        # Nếu 2 giây trước nó lao vào tôi, thì giờ tôi vẫn đang sợ.\n",
    "        \n",
    "        # Cửa sổ 3 giây (90 frames)\n",
    "        ws_memory = self._scale(90)\n",
    "        \n",
    "        # Fear Level = Max threat trong 3 giây qua\n",
    "        feats[\"fear_memory_3s\"] = threat_series.rolling(ws_memory, min_periods=1).max().astype(\"float32\")\n",
    "\n",
    "        # --- 3. TRẠNG THÁI SUBMIT (KẾT HỢP) ---\n",
    "        # Submit = (Tôi đang đứng yên) * (Tôi đang co cụm) * (Tôi đang sợ)\n",
    "        \n",
    "        # Tôi đứng yên (< 1 cm/s)\n",
    "        my_speed = ctx.speed_series\n",
    "        is_still = (my_speed < 1.0).astype(float)\n",
    "        \n",
    "        # Tôi co cụm (Dùng a_elongation thấp hoặc body_width/length cao)\n",
    "        # Giả sử bạn đã tính a_elongation ở hàm pose (thấp là co cụm)\n",
    "        # Nếu chưa có thì dùng tạm logic: elongation < 1.2\n",
    "        # Ở đây mình tạo feature giả lập độ co cụm nếu chưa có\n",
    "        parts = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\"])\n",
    "        if parts[\"nose\"] is not None:\n",
    "            spine_len = np.linalg.norm(parts[\"nose\"] - parts[\"tail_base\"], axis=1)\n",
    "            is_compact = (spine_len < 8.0).astype(float) # Ví dụ chuột dài < 8cm là co\n",
    "            is_compact = pd.Series(is_compact, index=idx)\n",
    "        else:\n",
    "            is_compact = pd.Series(0.0, index=idx)\n",
    "\n",
    "        # FINAL SCORE\n",
    "        # Đây là feature định danh cho Submit tĩnh\n",
    "        feats[\"static_submit_prob\"] = (\n",
    "            is_still * is_compact * feats[\"fear_memory_3s\"]\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        return feats\n",
    "\n",
    "\n",
    "    def _feat_pairwise(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng tương tác cặp đôi (Pairwise): Khoảng cách, Tốc độ tiếp cận.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None: \n",
    "            return feats\n",
    "\n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "\n",
    "        # --- 1. KHOẢNG CÁCH CƠ BẢN (DISTANCES) ---\n",
    "        # Vector nối Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        feats[\"rel_dist\"] = pd.Series(dist, index=idx, dtype=\"float32\")\n",
    "\n",
    "        # --- 2. KHOẢNG CÁCH CHI TIẾT (NOSE-TO-PART) ---\n",
    "        # Lấy các bộ phận quan trọng\n",
    "        my_parts = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\"])\n",
    "        target_parts = self._extract_parts_dict(target_ctx, \n",
    "            [\"nose\", \"tail_base\", \"body_center\", \"ear_left\", \"ear_right\", \n",
    "             \"lateral_left\", \"lateral_right\"])\n",
    "\n",
    "        def dist_ab(pt_a, pt_b):\n",
    "            if pt_a is None or pt_b is None: return zero()\n",
    "            d = np.linalg.norm(pt_a - pt_b, axis=1)\n",
    "            return pd.Series(d, index=idx, dtype=\"float32\")\n",
    "\n",
    "        an, tn = my_parts[\"nose\"], target_parts[\"nose\"]\n",
    "        feats[\"dist_nose_nose\"] = dist_ab(an, tn)\n",
    "        feats[\"dist_nose_tail\"] = dist_ab(an, target_parts[\"tail_base\"])\n",
    "        feats[\"dist_nose_body\"] = dist_ab(an, target_parts[\"body_center\"])\n",
    "        feats[\"dist_nose_el\"]   = dist_ab(an, target_parts[\"ear_left\"])\n",
    "        feats[\"dist_nose_er\"]   = dist_ab(an, target_parts[\"ear_right\"])\n",
    "        feats[\"dist_nose_tll\"]  = dist_ab(an, target_parts[\"lateral_left\"])\n",
    "        feats[\"dist_nose_tlr\"]  = dist_ab(an, target_parts[\"lateral_right\"])\n",
    "        feats[\"dist_tail_tail\"] = dist_ab(my_parts[\"tail_base\"], target_parts[\"tail_base\"])\n",
    "\n",
    "        # --- 3. ĐỊNH HƯỚNG & GÓC NHÌN (ORIENTATION & GAZE) ---\n",
    "        # Helper lấy vector cơ thể (Mũi - Đuôi/Thân)\n",
    "        def get_body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"nose\")\n",
    "            # Ưu tiên đuôi, nếu ko có thì dùng thân\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if tail is None: tail = parts_dict.get(\"body_center\") # Fallback\n",
    "            \n",
    "            if head is not None and tail is not None:\n",
    "                return head - tail\n",
    "            return None\n",
    "\n",
    "        a_vec = get_body_vec(my_parts)\n",
    "        t_vec = get_body_vec(target_parts)\n",
    "\n",
    "        # A. Body Cosine: Hai con cùng chiều hay ngược chiều?\n",
    "        if a_vec is not None and t_vec is not None:\n",
    "            dot = np.sum(a_vec * t_vec, axis=1)\n",
    "            mags = np.linalg.norm(a_vec, axis=1) * np.linalg.norm(t_vec, axis=1)\n",
    "            feats[\"body_cosine\"] = pd.Series(\n",
    "                np.clip(dot / (mags + 1e-6), -1.0, 1.0), index=idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"body_cosine\"] = zero()\n",
    "\n",
    "        # B. Gaze Cosine: Tôi có đang nhìn về phía Target không?\n",
    "        # Vector ánh nhìn = Target_Pos - My_Pos = rel_vec\n",
    "        if a_vec is not None:\n",
    "            dot_gaze = np.sum(a_vec * rel_vec, axis=1)\n",
    "            mag_a = np.linalg.norm(a_vec, axis=1)\n",
    "            # dist đã tính ở bước 1\n",
    "            feats[\"gaze_cosine\"] = pd.Series(\n",
    "                np.clip(dot_gaze / (mag_a * dist + 1e-6), -1.0, 1.0),\n",
    "                index=idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"gaze_cosine\"] = zero()\n",
    "\n",
    "        # --- 4. PHÂN RÃ VẬN TỐC (VELOCITY DECOMPOSITION) - CHÌA KHÓA CHO AVOID/ESCAPE ---\n",
    "        # Vector đơn vị hướng về địch (u)\n",
    "        dist_safe = dist.copy()\n",
    "        dist_safe[dist_safe == 0] = 1e-6\n",
    "        u_vec = rel_vec / dist_safe[:, None]\n",
    "\n",
    "        # a_vel và t_vel lấy từ Context\n",
    "        a_vel, t_vel = ctx.vel, target_ctx.vel\n",
    "\n",
    "        # A. Approach Speed (Vận tốc dọc trục nối 2 con)\n",
    "        # Dương: Lao vào nhau | Âm: Chạy ra xa nhau\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1) # Target hướng ngược lại\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)\n",
    "\n",
    "        # B. Lateral Speed (Vận tốc ngang - Vuông góc trục nối)\n",
    "        # Vector chiếu: v_proj = (v . u) * u\n",
    "        a_proj = a_along[:, None] * u_vec\n",
    "        a_lat_vec = a_vel - a_proj\n",
    "        a_lat_speed = np.linalg.norm(a_lat_vec, axis=1)\n",
    "\n",
    "        feats[\"approach_speed_agent\"]  = pd.Series(a_along, index=idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_target\"] = pd.Series(t_along, index=idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_rel\"]    = pd.Series(rel_along, index=idx, dtype=\"float32\")\n",
    "        feats[\"lateral_speed_agent\"]   = pd.Series(a_lat_speed, index=idx, dtype=\"float32\")\n",
    "        return feats\n",
    "    \n",
    "    def _feat_shortburst_social(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        feats = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- Lấy lại vài quantity cơ bản từ pairwise/avoidance ---\n",
    "        # vector Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # unit vector\n",
    "        rel_dist_safe = np.where(rel_dist == 0, 1e-6, rel_dist)\n",
    "        u_vec = rel_vec / rel_dist_safe[:, None]\n",
    "    \n",
    "        # velocity dọc trục nối (approach speed)\n",
    "        a_vel = ctx.vel\n",
    "        t_vel = target_ctx.vel\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)                # +: lao vào target\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1)             # +: target lao vào agent\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)    # +: lại gần nhau\n",
    "    \n",
    "        a_along_s = pd.Series(a_along, index=idx, dtype=\"float32\")\n",
    "        t_along_s = pd.Series(t_along, index=idx, dtype=\"float32\")\n",
    "        rel_along_s = pd.Series(rel_along, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # speed agent / target\n",
    "        a_speed = ctx.speed_series\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\"\n",
    "        )\n",
    "    \n",
    "        # heading_rel_cos ~ escape / approach\n",
    "        # vector body của agent\n",
    "        # (reuse idea từ _feat_pairwise)\n",
    "        # head ~ nose, tail ~ tail_base/body_center\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\"])\n",
    "        head_a = parts_a.get(\"nose\")\n",
    "        tail_a = parts_a.get(\"tail_base\")\n",
    "    \n",
    "        if head_a is not None and tail_a is not None:\n",
    "            body_vec_a = head_a - tail_a\n",
    "            dot = np.sum(body_vec_a * rel_vec, axis=1)\n",
    "            mag = np.linalg.norm(body_vec_a, axis=1) * rel_dist_safe\n",
    "            heading_cos = np.clip(dot / (mag + 1e-6), -1.0, 1.0)\n",
    "            heading_cos_s = pd.Series(heading_cos, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            heading_cos_s = zero()\n",
    "    \n",
    "        # --- Rolling window 10, 20, 30 frames (ở fps gốc) ---\n",
    "        for w30 in [10, 20, 30]:\n",
    "            ws = self._scale(w30)\n",
    "            min_p = max(1, ws // 3)\n",
    "    \n",
    "            # Attack-like: approach mạnh, khoảng cách giảm nhanh\n",
    "            feats[f\"sb_att_approach_mean_{w30}\"] = a_along_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_att_rel_along_mean_{w30}\"] = rel_along_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_att_dist_delta_{w30}\"] = (rel_dist_s - rel_dist_s.shift(ws)).fillna(0.0)\n",
    "    \n",
    "            # Chase-like: agent & target đều nhanh, dist tương đối nhỏ\n",
    "            feats[f\"sb_chase_speed_agent_mean_{w30}\"] = a_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_chase_speed_target_mean_{w30}\"] = t_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_chase_dist_mean_{w30}\"] = rel_dist_s.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            # Escape-like: heading ngược, dist tăng nhanh\n",
    "            feats[f\"sb_esc_heading_cos_mean_{w30}\"] = heading_cos_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_esc_dist_gain_{w30}\"] = (rel_dist_s.shift(-ws) - rel_dist_s).fillna(0.0)\n",
    "    \n",
    "        # clip & fillna\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = v.replace([np.inf, -np.inf], np.nan).fillna(0.0).astype(\"float32\")\n",
    "    \n",
    "        return feats\n",
    "\n",
    "\n",
    "    # --- Methods tương thích ---\n",
    "    \n",
    "    def build_pose_tensor(self, tracking: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Chuyển dữ liệu tracking (DataFrame) sang Tensor [Frames, Mice, 2] và Dict chi tiết.\n",
    "        \"\"\"\n",
    "        tracking = tracking.sort_values(\"video_frame\")\n",
    "        frames = np.sort(tracking[\"video_frame\"].unique())\n",
    "        \n",
    "        pvid = tracking.pivot(\n",
    "            index=\"video_frame\", \n",
    "            columns=[\"mouse_id\", \"bodypart\"], \n",
    "            values=[\"x\", \"y\"]\n",
    "        )\n",
    "        pvid = pvid.reorder_levels([1, 2, 0], axis=1).sort_index(axis=1).astype(\"float32\")\n",
    "        mouse_ids = list(pvid.columns.get_level_values(0).unique())\n",
    "        pos = np.full((len(frames), len(mouse_ids), 2), np.nan, dtype=np.float32)\n",
    "        per_mouse_df = {}\n",
    "        \n",
    "        for i, mid in enumerate(mouse_ids):\n",
    "            single = pvid[mid]\n",
    "            per_mouse_df[mid] = single\n",
    "            \n",
    "            if \"body_center\" in single.columns.get_level_values(0):\n",
    "                cx = single[\"body_center\"][\"x\"]\n",
    "                cy = single[\"body_center\"][\"y\"]\n",
    "            else:\n",
    "                cx = single.xs(\"x\", level=1, axis=1).mean(axis=1)\n",
    "                cy = single.xs(\"y\", level=1, axis=1).mean(axis=1)\n",
    "            \n",
    "            pos[:, i, 0] = cx.reindex(frames).values\n",
    "            pos[:, i, 1] = cy.reindex(frames).values\n",
    "            \n",
    "        return frames, mouse_ids, pos, per_mouse_df\n",
    "\n",
    "    def extract_agent_target(\n",
    "        self, \n",
    "        frames: np.ndarray, \n",
    "        mouse_ids: List[Any], \n",
    "        pos: np.ndarray, \n",
    "        agent_id: Any, \n",
    "        target_id: Any, \n",
    "        per_mouse_df: Dict = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Trích xuất đặc trưng cho cặp (Agent, Target).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            aid_idx = mouse_ids.index(agent_id)\n",
    "        except ValueError:\n",
    "            return pd.DataFrame() \n",
    "\n",
    "        # 1. Build Agent Context\n",
    "        ctx_agent = self._build_context(\n",
    "            frames, \n",
    "            pos[:, aid_idx, :], \n",
    "            per_mouse_df.get(agent_id) if per_mouse_df else None\n",
    "        )\n",
    "\n",
    "        # 2. Build Target Context\n",
    "        ctx_target = None\n",
    "        if self.cfg.use_pairwise and target_id is not None and target_id in mouse_ids:\n",
    "             tid_idx = mouse_ids.index(target_id)\n",
    "             ctx_target = self._build_context(\n",
    "                 frames, \n",
    "                 pos[:, tid_idx, :], \n",
    "                 per_mouse_df.get(target_id) if per_mouse_df else None\n",
    "             )\n",
    "\n",
    "        # 3. Run all features\n",
    "        all_data = {}\n",
    "        for func_name, func in self.feature_registry.items():\n",
    "            out_dict = func(ctx_agent, target_ctx=ctx_target)\n",
    "            all_data.update(out_dict)\n",
    "\n",
    "        df_out = pd.DataFrame(all_data, index=ctx_agent.idx)\n",
    "        df_out = df_out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        \n",
    "        return df_out.reindex(sorted(df_out.columns), axis=1)\n",
    "\n",
    "#=====================================================================================\n",
    "#=====================================================================================\n",
    "#=====================================================================================\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# (Trên Kaggle) dùng metric chính thức\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/usr/lib/mabe-f-beta\")\n",
    "from metric import score   # hàm score(submission_df, dataset_df)\n",
    "\n",
    "# =========================================================\n",
    "# 1. ĐƯỜNG DẪN & CẤU HÌNH\n",
    "# =========================================================\n",
    "\n",
    "INPUT_DIR = Path(\"/kaggle/input/MABe-mouse-behavior-detection\")\n",
    "TRAIN_TRACKING_DIR = INPUT_DIR / \"train_tracking\"\n",
    "TRAIN_ANNOTATION_DIR = INPUT_DIR / \"train_annotation\"\n",
    "TEST_TRACKING_DIR = INPUT_DIR / \"test_tracking\"\n",
    "\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "RESULTS_DIR = Path(r\"/kaggle/input/results-xgb-fe\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "INDEX_COLS = [\"video_id\", \"agent_id\", \"target_id\", \"video_frame\"]\n",
    "\n",
    "# hành vi “self” vs “pair” giống notebook (có thể chỉnh nếu muốn)\n",
    "SELF_BEHAVIORS = [\n",
    "    \"biteobject\", \"climb\", \"dig\", \"exploreobject\", \"freeze\",\n",
    "    \"genitalgroom\", \"huddle\", \"rear\", \"rest\", \"run\", \"selfgroom\",\n",
    "]\n",
    "PAIR_BEHAVIORS = [\n",
    "    \"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\",\n",
    "    \"chase\", \"chaseattack\", \"defend\", \"disengage\", \"dominance\",\n",
    "    \"dominancegroom\", \"dominancemount\", \"ejaculate\", \"escape\",\n",
    "    \"flinch\", \"follow\", \"intromit\", \"mount\", \"reciprocalsniff\",\n",
    "    \"shepherd\", \"sniff\", \"sniffbody\", \"sniffface\", \"sniffgenital\",\n",
    "    \"submit\", \"tussle\",\n",
    "]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. ĐỌC METADATA & HELPER\n",
    "# =========================================================\n",
    "\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    train_meta = pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "    return train_meta\n",
    "\n",
    "\n",
    "def get_video_params(video_id: Any, meta: pd.DataFrame) -> Tuple[float, float]:\n",
    "    \"\"\"Lấy fps, pix_per_cm cho video từ train.csv.\"\"\"\n",
    "    row = meta.loc[meta[\"video_id\"] == video_id]\n",
    "    if row.empty:\n",
    "        raise KeyError(f\"video_id={video_id} không có trong train.csv\")\n",
    "    row = row.iloc[0]\n",
    "\n",
    "    # giống notebook: cột \"frames per second\" & \"pix per cm (approx)\"\n",
    "    fps = float(row[\"frames_per_second\"])\n",
    "    pix_per_cm = float(row[\"pix_per_cm_approx\"])\n",
    "    if not np.isfinite(pix_per_cm) or pix_per_cm <= 0:\n",
    "        pix_per_cm = 1.0\n",
    "    return fps, pix_per_cm\n",
    "\n",
    "\n",
    "def load_tracking(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    \"\"\"Đọc tracking parquet → pandas (schema: video_frame, mouse_id, bodypart, x, y).\"\"\"\n",
    "    path = TRAIN_TRACKING_DIR / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    df = pd.read_parquet(path)\n",
    "    return df\n",
    "\n",
    "def load_tracking_test(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    \"\"\"Đọc tracking parquet của test → pandas.\"\"\"\n",
    "    path = INPUT_DIR / \"test_tracking\" / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "def load_annotation(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    \"\"\"Đọc annotation (agent_id, target_id, action, start_frame, stop_frame).\"\"\"\n",
    "    path = TRAIN_ANNOTATION_DIR / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists():\n",
    "        # không có label cho video này\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "        )\n",
    "    ann = pd.read_parquet(path)\n",
    "    return ann[[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. TÍNH FEATURE PER-FRAME BẰNG FEATUREEXTRACTOR\n",
    "# =========================================================\n",
    "\n",
    "# Cache: (lab, video, agent, target) -> (frames, feature_df)\n",
    "_feature_cache: Dict[Tuple[str, int, int, int], Tuple[np.ndarray, pd.DataFrame]] = {}\n",
    "\n",
    "\n",
    "def get_frame_features_for_pair(\n",
    "    lab_id: str,\n",
    "    video_id: int,\n",
    "    agent_id: int,\n",
    "    target_id: int,\n",
    "    meta: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Tính (hoặc lấy cache) feature per-frame cho 1 video + (agent, target).\n",
    "    Trả về: frames [F], features_df [F, D]\n",
    "    \"\"\"\n",
    "    key = (str(lab_id), int(video_id), int(agent_id), int(target_id))\n",
    "    if key in _feature_cache:\n",
    "        return _feature_cache[key]\n",
    "\n",
    "    fps, pix_per_cm = get_video_params(video_id, meta)\n",
    "    tracking = load_tracking(lab_id, video_id)\n",
    "\n",
    "    fe = FeatureExtractor(\n",
    "        fps=fps,\n",
    "        pix_per_cm=pix_per_cm,\n",
    "        smooth_sigma=1.0,\n",
    "        use_pairwise=True,\n",
    "    )\n",
    "\n",
    "    frames, mouse_ids, pos, per_mouse_df = fe.build_pose_tensor(tracking)\n",
    "\n",
    "    # agent/target có thể là cùng chuột (self) hoặc khác chuột (pair)\n",
    "    features_df: pd.DataFrame = fe.extract_agent_target(\n",
    "        frames=frames,\n",
    "        mouse_ids=mouse_ids,\n",
    "        pos=pos,\n",
    "        agent_id=agent_id,\n",
    "        target_id=target_id,\n",
    "        per_mouse_df=per_mouse_df,\n",
    "    )\n",
    "    # index chính là frame\n",
    "    features_df.index = frames\n",
    "\n",
    "    _feature_cache[key] = (frames, features_df)\n",
    "    return frames, features_df\n",
    "\n",
    "_feature_cache: Dict[Tuple[str, int, Any, Any], Tuple[np.ndarray, pd.DataFrame]] = {}\n",
    "\n",
    "def get_frame_features_for_pair_test(\n",
    "    lab_id: str,\n",
    "    video_id: int,\n",
    "    agent_id: Any,\n",
    "    target_id: Any,\n",
    "    test_meta: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Feature per-frame cho test (video_id, agent, target).\n",
    "    Trả về: frames [F], features_df [F, D]\n",
    "    \"\"\"\n",
    "    key = (f\"test_{lab_id}\", int(video_id), agent_id, target_id)\n",
    "    if key in _feature_cache:\n",
    "        return _feature_cache[key]\n",
    "\n",
    "    # Lấy fps, pix_per_cm_approx từ test.csv\n",
    "    row = test_meta[test_meta[\"video_id\"] == video_id].iloc[0]\n",
    "    fps = float(row[\"frames_per_second\"])\n",
    "    pix_per_cm = float(row[\"pix_per_cm_approx\"])\n",
    "    if not np.isfinite(pix_per_cm) or pix_per_cm <= 0:\n",
    "        pix_per_cm = 1.0\n",
    "\n",
    "    tracking = load_tracking_test(lab_id, video_id)\n",
    "\n",
    "    fe = FeatureExtractor(\n",
    "        fps=fps,\n",
    "        pix_per_cm=pix_per_cm,\n",
    "        smooth_sigma=1.0,\n",
    "        use_pairwise=True,\n",
    "    )\n",
    "\n",
    "    frames, mouse_ids, pos, per_mouse_df = fe.build_pose_tensor(tracking)\n",
    "\n",
    "    features_df = fe.extract_agent_target(\n",
    "        frames=frames,\n",
    "        mouse_ids=mouse_ids,\n",
    "        pos=pos,\n",
    "        agent_id=agent_id,\n",
    "        target_id=target_id,\n",
    "        per_mouse_df=per_mouse_df,\n",
    "    )\n",
    "    features_df.index = frames\n",
    "\n",
    "    _feature_cache[key] = (frames, features_df)\n",
    "    return frames, features_df\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. BUILD FRAME-LEVEL DATASET CHO 1 (lab_id, behavior)\n",
    "# =========================================================\n",
    "\n",
    "def build_frame_dataset_for_lab_behavior(\n",
    "    lab_id: str,\n",
    "    behavior: str,\n",
    "    train_meta: pd.DataFrame,\n",
    "    mode: str = \"self\",\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Xây tập frame-level (indices, features, labels) cho 1 (lab, behavior).\n",
    "\n",
    "    indices: DataFrame với cột INDEX_COLS\n",
    "    features: DataFrame per-frame features\n",
    "    labels: np.ndarray nhị phân (0/1)\n",
    "    \"\"\"\n",
    "\n",
    "    videos = (\n",
    "        train_meta[train_meta[\"lab_id\"] == lab_id][\"video_id\"]\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    index_list = []\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for video_id in videos:\n",
    "        ann = load_annotation(lab_id, video_id)\n",
    "        if ann.empty:\n",
    "            continue\n",
    "\n",
    "        # chỉ lấy annotation của behavior này\n",
    "        ann_bhv = ann[ann[\"action\"] == behavior]\n",
    "        if ann_bhv.empty:\n",
    "            continue\n",
    "\n",
    "        # các (agent, target) cần xem\n",
    "        pairs = ann_bhv[[\"agent_id\", \"target_id\"]].drop_duplicates().values.tolist()\n",
    "\n",
    "        for (agent_id, target_id) in pairs:\n",
    "            if mode == \"self\":\n",
    "                target_id_use = agent_id\n",
    "            else:\n",
    "                target_id_use = target_id\n",
    "\n",
    "            frames, feat_df = get_frame_features_for_pair(\n",
    "                lab_id=lab_id,\n",
    "                video_id=video_id,\n",
    "                agent_id=agent_id,\n",
    "                target_id=target_id_use,\n",
    "                meta=train_meta,\n",
    "            )\n",
    "\n",
    "            # label per-frame: frame ∈ bất kỳ [start, stop) của (agent,target,behavior)\n",
    "            ann_pair = ann_bhv[\n",
    "                (ann_bhv[\"agent_id\"] == agent_id)\n",
    "                & (ann_bhv[\"target_id\"] == target_id)\n",
    "            ]\n",
    "            if ann_pair.empty and mode == \"self\":\n",
    "                ann_pair = ann_bhv[ann_bhv[\"agent_id\"] == agent_id]\n",
    "\n",
    "            pos_frames = set()\n",
    "            for _, r in ann_pair.iterrows():\n",
    "                pos_frames.update(range(int(r[\"start_frame\"]), int(r[\"stop_frame\"])))\n",
    "\n",
    "            if len(pos_frames) == 0:\n",
    "                continue\n",
    "\n",
    "            label = np.isin(frames, list(pos_frames)).astype(\"int8\")\n",
    "            if label.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            idx_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"video_id\": video_id,\n",
    "                    \"agent_id\": agent_id,\n",
    "                    \"target_id\": target_id,\n",
    "                    \"video_frame\": frames,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            index_list.append(idx_df)\n",
    "            feature_list.append(feat_df.reset_index(drop=True))\n",
    "            label_list.append(label)\n",
    "\n",
    "    if not index_list:\n",
    "        return (\n",
    "            pd.DataFrame(columns=INDEX_COLS),\n",
    "            pd.DataFrame(),\n",
    "            np.zeros(0, dtype=\"int8\"),\n",
    "        )\n",
    "\n",
    "    indices = pd.concat(index_list, ignore_index=True)\n",
    "    features = pd.concat(feature_list, ignore_index=True)\n",
    "    labels = np.concatenate(label_list).astype(\"int8\")\n",
    "\n",
    "    assert len(indices) == len(features) == len(labels)\n",
    "\n",
    "    return indices, features, labels\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. TRAIN + OOF CHO 1 (lab_id, behavior)\n",
    "# =========================================================\n",
    "\n",
    "def tune_threshold(oof_pred: np.ndarray, y: np.ndarray) -> float:\n",
    "    ths = np.arange(0.0, 1.005, 0.005)\n",
    "    scores = [f1_score(y, (oof_pred >= th), zero_division=0) for th in ths]\n",
    "    return float(ths[int(np.argmax(scores))])\n",
    "\n",
    "#\n",
    "def train_validate_one(\n",
    "    lab_id: str,\n",
    "    behavior: str,\n",
    "    indices: pd.DataFrame,\n",
    "    features: pd.DataFrame,\n",
    "    labels: np.ndarray,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Train XGBoost binary cho 1 (lab, behavior) + lưu OOF prediction.\n",
    "    Trả về: F1 trên toàn bộ OOF (frame-level).\n",
    "    \"\"\"\n",
    "    result_dir = RESULTS_DIR / lab_id / behavior\n",
    "    result_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    n = len(labels)\n",
    "\n",
    "    if n == 0 or labels.sum() == 0:\n",
    "        oof_df = indices.copy()\n",
    "        oof_df[\"fold\"] = -1\n",
    "        oof_df[\"prediction\"] = 0.0\n",
    "        oof_df[\"predicted_label\"] = 0\n",
    "        oof_df.to_parquet(result_dir / \"oof_predictions.parquet\", index=False)\n",
    "        (result_dir / \"f1.txt\").write_text(\"0.0\\n\")\n",
    "        return 0.0\n",
    "\n",
    "    X = features.values.astype(\"float32\")\n",
    "    y = labels.astype(\"int8\")\n",
    "    groups = indices[\"video_id\"].values\n",
    "\n",
    "    folds = np.ones(n, dtype=\"int8\") * -1\n",
    "    oof_pred = np.zeros(n, dtype=\"float32\")\n",
    "    oof_label = np.zeros(n, dtype=\"int8\")\n",
    "\n",
    "    cv = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups=groups)):\n",
    "        fold_dir = result_dir / f\"fold_{fold}\"\n",
    "        fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "        X_va, y_va = X[va_idx], y[va_idx]\n",
    "\n",
    "        # scale_pos_weight\n",
    "        pos = y_tr.sum()\n",
    "        neg = len(y_tr) - pos\n",
    "        scale_pos_weight = float(neg / pos) if pos > 0 else 1.0\n",
    "\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"logloss\",\n",
    "            \"device\": \"cuda\",\n",
    "            \"tree_method\": \"hist\",\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"max_depth\": 6,\n",
    "            \"min_child_weight\": 5,\n",
    "            \"subsample\": 0.8,\n",
    "            \"colsample_bytree\": 0.8,\n",
    "            \"scale_pos_weight\": scale_pos_weight,\n",
    "            \"max_bin\": 64,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "\n",
    "        dtrain = xgb.QuantileDMatrix(\n",
    "            X_tr,\n",
    "            label=y_tr,\n",
    "            feature_names=features.columns.tolist(),\n",
    "            max_bin=64,\n",
    "        )\n",
    "        dvalid = xgb.DMatrix(\n",
    "            X_va,\n",
    "            label=y_va,\n",
    "            feature_names=features.columns.tolist(),\n",
    "        )\n",
    "\n",
    "        evals_result: Dict[str, Dict[str, List[float]]] = {}\n",
    "\n",
    "        early_stop = xgb.callback.EarlyStopping(\n",
    "            rounds=10, metric_name=\"logloss\", data_name=\"valid\", maximize=False\n",
    "        )\n",
    "\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=250,\n",
    "            evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "            callbacks=[early_stop],\n",
    "            evals_result=evals_result,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "\n",
    "        pred_va = model.predict(dvalid)\n",
    "        th = tune_threshold(pred_va, y_va)\n",
    "\n",
    "        folds[va_idx] = fold\n",
    "        oof_pred[va_idx] = pred_va\n",
    "        oof_label[va_idx] = (pred_va >= th).astype(\"int8\")\n",
    "\n",
    "        model.save_model(fold_dir / \"model.json\")\n",
    "        with open(fold_dir / \"threshold.txt\", \"w\") as f:\n",
    "            f.write(f\"{th}\\n\")\n",
    "\n",
    "    # lưu OOF\n",
    "    oof_df = indices.copy()\n",
    "    oof_df[\"fold\"] = folds\n",
    "    oof_df[\"prediction\"] = oof_pred\n",
    "    oof_df[\"predicted_label\"] = oof_label\n",
    "    oof_df.to_parquet(result_dir / \"oof_predictions.parquet\", index=False)\n",
    "\n",
    "    f1 = f1_score(y, oof_label, zero_division=0)\n",
    "    (result_dir / \"f1.txt\").write_text(f\"{f1:.6f}\\n\")\n",
    "    return float(f1)\n",
    "\n",
    "def load_models_for_behavior_infer(lab_id: str, behavior: str):\n",
    "    \"\"\"\n",
    "    Đọc các fold model + threshold cho (lab, behavior) từ RESULTS_DIR.\n",
    "    Dùng cho inference (test).\n",
    "    \"\"\"\n",
    "    base_dir = RESULTS_DIR / lab_id / behavior\n",
    "    if not base_dir.exists():\n",
    "        return []\n",
    "\n",
    "    models = []\n",
    "    for fold_dir in sorted(base_dir.glob(\"fold_*\")):\n",
    "        model_file = fold_dir / \"model.json\"\n",
    "        thr_file = fold_dir / \"threshold.txt\"\n",
    "        if not model_file.exists():\n",
    "            continue\n",
    "\n",
    "        booster = xgb.Booster()\n",
    "        booster.load_model(str(model_file))\n",
    "\n",
    "        if thr_file.exists():\n",
    "            thr = float(thr_file.read_text().strip())\n",
    "        else:\n",
    "            thr = 0.5\n",
    "\n",
    "        models.append((booster, thr))\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. LOOP QUA TẤT CẢ BEHAVIORS TRONG 1 LAB\n",
    "#    (train_all_labs_behaviors vẫn giữ nguyên, nhưng main\n",
    "#     sẽ filter train_meta chỉ còn 1 lab)\n",
    "# =========================================================\n",
    "\n",
    "def train_all_labs_behaviors(train_meta: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Loop qua từng lab trong train_meta (ở đây main đã filter chỉ còn 1 lab):\n",
    "      - đọc annotation của tất cả video\n",
    "      - lấy unique action xuất hiện trong lab đó\n",
    "      - train 1 model/frame-level cho từng (lab, action)\n",
    "    \"\"\"\n",
    "    labs = train_meta[\"lab_id\"].unique().tolist()\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    for lab_id in labs:\n",
    "        # tập video của lab này\n",
    "        videos = train_meta[train_meta[\"lab_id\"] == lab_id][\"video_id\"].unique().tolist()\n",
    "\n",
    "        # gom toàn bộ action thực sự có trong annotation của lab này\n",
    "        behaviors_set = set()\n",
    "        for vid in videos:\n",
    "            ann = load_annotation(lab_id, vid)\n",
    "            if ann.empty:\n",
    "                continue\n",
    "            behaviors_set.update(ann[\"action\"].unique().tolist())\n",
    "\n",
    "        behaviors = sorted(behaviors_set)\n",
    "        print(f\"\\n===== LAB {lab_id}: {len(behaviors)} behaviors =====\")\n",
    "\n",
    "        for behavior in behaviors:\n",
    "            # if behavior != \"submit\": continue\n",
    "\n",
    "            mode = \"self\" if behavior in SELF_BEHAVIORS else \"pair\"\n",
    "\n",
    "            print(f\"\\n=== LAB={lab_id} | behavior={behavior} | mode={mode} ===\")\n",
    "            indices, features, labels = build_frame_dataset_for_lab_behavior(\n",
    "                lab_id=str(lab_id),\n",
    "                behavior=behavior,\n",
    "                train_meta=train_meta,\n",
    "                mode=mode,\n",
    "            )\n",
    "            print(\n",
    "                f\"frames: {len(labels):,}, positives: {labels.sum():,}, features: \"\n",
    "                f\"{features.shape[1] if not features.empty else 0}\"\n",
    "            )\n",
    "\n",
    "            if len(labels) == 0:\n",
    "                print(\" -> skip (no samples)\")\n",
    "                continue\n",
    "\n",
    "            f1 = train_validate_one(str(lab_id), behavior, indices, features, labels)\n",
    "            elapsed = time.perf_counter() - start_time\n",
    "            print(f\" -> OOF F1 (frame-level): {f1:.3f} | elapsed={elapsed/60:.1f} min\")\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. GOM OOF PREDICTION → SEGMENT & TÍNH SCORE()\n",
    "# =========================================================\n",
    "\n",
    "def build_oof_submission_from_parquet(\n",
    "    target_lab_id: Optional[str] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Đọc tất cả oof_predictions.parquet trong RESULTS_DIR,\n",
    "    gom thành frame-level table rồi nối thành segment-level prediction\n",
    "    giống inference notebook (simplified).\n",
    "\n",
    "    Nếu target_lab_id != None thì chỉ lấy OOF của lab đó\n",
    "    (vd \"AdaptableSnail\").\n",
    "    \"\"\"\n",
    "    oof_files = list(RESULTS_DIR.glob(\"*/**/oof_predictions.parquet\"))\n",
    "    if not oof_files:\n",
    "        raise RuntimeError(\"Không tìm thấy OOF parquet, hãy train trước.\")\n",
    "\n",
    "    frame_preds = []\n",
    "\n",
    "    for path in oof_files:\n",
    "        # path: results_xgb_fe/lab/behavior/oof_predictions.parquet\n",
    "        parts = path.parts\n",
    "        behavior = parts[-2]\n",
    "        lab_id = parts[-3]\n",
    "\n",
    "        # chỉ lấy file thuộc lab mong muốn (nếu có)\n",
    "        if target_lab_id is not None and lab_id != target_lab_id:\n",
    "            continue\n",
    "\n",
    "        df = pd.read_parquet(path)\n",
    "        df = df[INDEX_COLS + [\"prediction\"]].copy()\n",
    "        df[\"lab_id\"] = lab_id\n",
    "        df[\"action\"] = behavior\n",
    "        frame_preds.append(df)\n",
    "\n",
    "    if not frame_preds:\n",
    "        raise RuntimeError(\n",
    "            f\"Không có OOF predictions nào cho lab_id={target_lab_id}\"\n",
    "        )\n",
    "\n",
    "    frame_df = pd.concat(frame_preds, ignore_index=True)\n",
    "\n",
    "    # sắp xếp\n",
    "    frame_df = frame_df.sort_values(\n",
    "        [\"lab_id\", \"video_id\", \"agent_id\", \"target_id\", \"action\", \"video_frame\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Convert frame-level prob -> hard label + segments\n",
    "    segments = []\n",
    "    for (lab_id, video_id, agent_id, target_id, action), group in frame_df.groupby(\n",
    "        [\"lab_id\", \"video_id\", \"agent_id\", \"target_id\", \"action\"], sort=False\n",
    "    ):\n",
    "        frames = group[\"video_frame\"].values\n",
    "        scores = group[\"prediction\"].values\n",
    "\n",
    "        # dùng một threshold fix (vd 0.5) cho demo\n",
    "        # (hoặc bạn có thể lưu threshold per (lab,behavior) và apply)\n",
    "        hard = scores >= 0.5\n",
    "\n",
    "        in_seg = False\n",
    "        start = None\n",
    "        prev_f = None\n",
    "\n",
    "        for f, h in zip(frames, hard):\n",
    "            if h and not in_seg:\n",
    "                in_seg = True\n",
    "                start = int(f)\n",
    "            elif (not h) and in_seg:\n",
    "                stop = int(prev_f + 1)  # [start, stop)\n",
    "                segments.append(\n",
    "                    {\n",
    "                        \"lab_id\": lab_id,\n",
    "                        \"video_id\": int(video_id),\n",
    "                        \"agent_id\": int(agent_id),\n",
    "                        \"target_id\": int(target_id),\n",
    "                        \"action\": action,\n",
    "                        \"start_frame\": start,\n",
    "                        \"stop_frame\": stop,\n",
    "                    }\n",
    "                )\n",
    "                in_seg = False\n",
    "            prev_f = f\n",
    "\n",
    "        if in_seg:\n",
    "            stop = int(frames[-1] + 1)\n",
    "            segments.append(\n",
    "                {\n",
    "                    \"lab_id\": lab_id,\n",
    "                    \"video_id\": int(video_id),\n",
    "                    \"agent_id\": int(agent_id),\n",
    "                    \"target_id\": int(target_id),\n",
    "                    \"action\": action,\n",
    "                    \"start_frame\": start,\n",
    "                    \"stop_frame\": stop,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not segments:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"lab_id\",\n",
    "                \"video_id\",\n",
    "                \"agent_id\",\n",
    "                \"target_id\",\n",
    "                \"action\",\n",
    "                \"start_frame\",\n",
    "                \"stop_frame\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    submission = pd.DataFrame(segments)\n",
    "    submission = submission.sort_values(\n",
    "        [\"lab_id\", \"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return submission\n",
    "\n",
    "BAD_VIDEOS = [143861384, 1596473327, 1212811043, 878123481]\n",
    "\n",
    "def compute_validation_score(\n",
    "    submission: pd.DataFrame,\n",
    "    lab_id: Optional[str] = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Gọi metric `score()` chính thức trên train set.\n",
    "    Nếu lab_id != None → chỉ validate trên lab đó.\n",
    "    \"\"\"\n",
    "    # ===== THAY ĐỔI Ở ĐÂY =====\n",
    "    # Không dùng train.csv, mà phải đọc toàn bộ annotations\n",
    "    train_meta = pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "    \n",
    "    if lab_id is not None:\n",
    "        train_meta = train_meta[train_meta[\"lab_id\"] == lab_id].reset_index(drop=True)\n",
    "\n",
    "    if BAD_VIDEOS:\n",
    "        train_meta = train_meta[~train_meta[\"video_id\"].isin(BAD_VIDEOS)]\n",
    "    \n",
    "    # Đọc tất cả annotation files\n",
    "    all_annotations = []\n",
    "    for _, row in train_meta.iterrows():\n",
    "        lab = row[\"lab_id\"]\n",
    "        vid = row[\"video_id\"]\n",
    "        ann = load_annotation(lab, vid)\n",
    "        if not ann.empty:\n",
    "            ann[\"lab_id\"] = lab\n",
    "            ann[\"video_id\"] = vid\n",
    "            ann[\"behaviors_labeled\"] = row[\"behaviors_labeled\"]\n",
    "            all_annotations.append(ann)\n",
    "    \n",
    "    if not all_annotations:\n",
    "        print(\"Không có annotation nào để validate!\")\n",
    "        return 0.0\n",
    "    \n",
    "    dataset = pd.concat(all_annotations, ignore_index=True)\n",
    "    \n",
    "    # Filter submission theo lab nếu cần\n",
    "    if lab_id is not None:\n",
    "        submission = submission[submission[\"lab_id\"] == lab_id].reset_index(drop=True)\n",
    "    \n",
    "    # ===== GỌI METRIC =====\n",
    "    s = score(dataset, submission, row_id_column_name=\"row_id\")\n",
    "\n",
    "    print(\n",
    "        f\"Official validation score\"\n",
    "        f\"{' (lab=' + lab_id + ')' if lab_id is not None else ''}: {s:.6f}\"\n",
    "    )\n",
    "    return float(s)\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. MAIN\n",
    "# =========================================================\n",
    "def str_to_mouse_id(s: str) -> int:\n",
    "    if s == \"self\":\n",
    "        return -1\n",
    "    return int(str(s).replace(\"mouse\", \"\"))\n",
    "\n",
    "\n",
    "def predict_behaviors_for_pair(\n",
    "    lab_id: str,\n",
    "    video_id: int,\n",
    "    agent_internal_id: Any,\n",
    "    target_internal_id: Any,\n",
    "    behaviors: List[str],\n",
    "    test_meta: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Chạy inference cho 1 cặp (video, agent_internal_id, target_internal_id)\n",
    "    với list behaviors (cùng mode: all self hoặc all pair).\n",
    "    Trả về segment-level DataFrame: video_id, action, start_frame, stop_frame.\n",
    "    \"\"\"\n",
    "    if lab_id != \"SparklingTapir\": return None\n",
    "    frames, feat_df = get_frame_features_for_pair_test(\n",
    "        lab_id=lab_id,\n",
    "        video_id=video_id,\n",
    "        agent_id=agent_internal_id,\n",
    "        target_id=target_internal_id,\n",
    "        test_meta=test_meta,\n",
    "    )\n",
    "    if feat_df.empty:\n",
    "        return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "\n",
    "    feat_df = feat_df.astype(\"float32\")\n",
    "    n_frames = len(feat_df)\n",
    "\n",
    "    scores_per_behavior = {}\n",
    "    for behavior in behaviors:\n",
    "        models = load_models_for_behavior_infer(lab_id, behavior)\n",
    "        if not models:\n",
    "            continue\n",
    "\n",
    "        req_feats = models[0][0].feature_names\n",
    "        # Build X_test với đúng bộ feature của model\n",
    "        X_test = pd.DataFrame(\n",
    "            0.0,\n",
    "            index=feat_df.index,\n",
    "            columns=req_feats,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        common = list(set(req_feats) & set(feat_df.columns))\n",
    "        if common:\n",
    "            X_test[common] = feat_df[common]\n",
    "\n",
    "        dtest = xgb.DMatrix(X_test, feature_names=req_feats)\n",
    "\n",
    "        agg_scores = np.zeros(n_frames, dtype=np.float32)\n",
    "        for booster, thr in models:\n",
    "            probs = booster.predict(dtest)\n",
    "            labels = (probs >= thr).astype(np.int8)\n",
    "            agg_scores += probs * labels\n",
    "\n",
    "        agg_scores /= max(len(models), 1)\n",
    "        scores_per_behavior[behavior] = agg_scores\n",
    "\n",
    "        del dtest, X_test\n",
    "        gc.collect()\n",
    "\n",
    "    if not scores_per_behavior:\n",
    "        return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "\n",
    "    beh_list = list(scores_per_behavior.keys())\n",
    "    score_mat = np.vstack([scores_per_behavior[b] for b in beh_list]).T  # [F, B]\n",
    "\n",
    "    max_idx = score_mat.argmax(axis=1)\n",
    "    max_scores = score_mat.max(axis=1)\n",
    "    labels = np.where(max_scores == 0.0, \"none\", np.array(beh_list)[max_idx])\n",
    "\n",
    "    # frame-level → segment\n",
    "    segments = []\n",
    "    prev_lab = \"none\"\n",
    "    prev_start = None\n",
    "    prev_f = None\n",
    "\n",
    "    for f, lab in zip(frames, labels):\n",
    "        if lab != prev_lab:\n",
    "            if prev_lab != \"none\":\n",
    "                segments.append(\n",
    "                    {\n",
    "                        \"video_id\": int(video_id),\n",
    "                        \"action\": prev_lab,\n",
    "                        \"start_frame\": int(prev_start),\n",
    "                        \"stop_frame\": int(prev_f + 1),\n",
    "                    }\n",
    "                )\n",
    "            prev_lab = lab\n",
    "            prev_start = f\n",
    "        prev_f = f\n",
    "\n",
    "    if prev_lab != \"none\":\n",
    "        segments.append(\n",
    "            {\n",
    "                \"video_id\": int(video_id),\n",
    "                \"action\": prev_lab,\n",
    "                \"start_frame\": int(prev_start),\n",
    "                \"stop_frame\": int(prev_f + 1),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not segments:\n",
    "        return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "\n",
    "    return pd.DataFrame(segments)\n",
    "\n",
    "\n",
    "\n",
    "target_lab = \"SparklingTapir\"\n",
    "print(f\"Đọc test.csv cho lab {target_lab} ...\")\n",
    "test_meta = pd.read_csv(INPUT_DIR / \"test.csv\")\n",
    "test_meta = test_meta[test_meta[\"lab_id\"] == target_lab].reset_index(drop=True)\n",
    "video_size_meta = test_meta[[\"video_id\", \"video_width_pix\", \"video_height_pix\"]].copy()\n",
    "\n",
    "\n",
    "# Lấy danh sách behavior đã train (thư mục con trong RESULTS_DIR/AdaptableSnail)\n",
    "lab_result_dir = RESULTS_DIR / target_lab\n",
    "if lab_result_dir.exists():\n",
    "    trained_behaviors = sorted(\n",
    "        [p.name for p in lab_result_dir.iterdir() if p.is_dir()]\n",
    "    )\n",
    "else:\n",
    "    trained_behaviors = []\n",
    "\n",
    "self_behaviors_in_lab = [b for b in trained_behaviors if b in SELF_BEHAVIORS]\n",
    "pair_behaviors_in_lab = [b for b in trained_behaviors if b in PAIR_BEHAVIORS]\n",
    "\n",
    "print(\"Behaviors (self) dùng để predict:\", self_behaviors_in_lab)\n",
    "print(\"Behaviors (pair) dùng để predict:\", pair_behaviors_in_lab)\n",
    "\n",
    "all_segments = []\n",
    "\n",
    "# Loop từng video test của lab\n",
    "for video_id in sorted(test_meta[\"video_id\"].unique()):\n",
    "    print(f\"Predict video_id={video_id} ...\")\n",
    "\n",
    "    tracking = load_tracking_test(target_lab, video_id)\n",
    "    mouse_ids_internal = sorted(tracking[\"mouse_id\"].unique().tolist())\n",
    "\n",
    "    # Map internal mouse_id -> string để đưa vào submission\n",
    "    def to_submit_id(mid):\n",
    "        s = str(mid)\n",
    "        return s if s.startswith(\"mouse\") else f\"mouse{s}\"\n",
    "\n",
    "    # SELF behaviors: agent == target (self)\n",
    "    if self_behaviors_in_lab:\n",
    "        for mid in mouse_ids_internal:\n",
    "            seg_df = predict_behaviors_for_pair(\n",
    "                lab_id=target_lab,\n",
    "                video_id=video_id,\n",
    "                agent_internal_id=mid,\n",
    "                target_internal_id=mid,  # self\n",
    "                behaviors=self_behaviors_in_lab,\n",
    "                test_meta=test_meta,\n",
    "            )\n",
    "            if not seg_df.empty:\n",
    "                seg_df[\"agent_id\"] = to_submit_id(mid)\n",
    "                seg_df[\"target_id\"] = \"self\"\n",
    "                all_segments.append(seg_df)\n",
    "\n",
    "    # PAIR behaviors: mọi cặp agent != target\n",
    "    if pair_behaviors_in_lab and len(mouse_ids_internal) > 1:\n",
    "        for agent_internal, target_internal in itertools.permutations(\n",
    "            mouse_ids_internal, 2\n",
    "        ):\n",
    "            seg_df = predict_behaviors_for_pair(\n",
    "                lab_id=target_lab,\n",
    "                video_id=video_id,\n",
    "                agent_internal_id=agent_internal,\n",
    "                target_internal_id=target_internal,\n",
    "                behaviors=pair_behaviors_in_lab,\n",
    "                test_meta=test_meta,\n",
    "            )\n",
    "            if not seg_df.empty:\n",
    "                seg_df[\"agent_id\"] = to_submit_id(agent_internal)\n",
    "                seg_df[\"target_id\"] = to_submit_id(target_internal)\n",
    "                all_segments.append(seg_df)\n",
    "\n",
    "# Gộp tất cả segments → submission.csv\n",
    "# Gộp tất cả segments → submission2.csv\n",
    "if all_segments:\n",
    "    submission7 = pd.concat(all_segments, ignore_index=True)\n",
    "    submission7 = submission7[\n",
    "        [\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "    ]\n",
    "    submission7 = submission7.sort_values(\n",
    "        [\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\"]\n",
    "    ).reset_index(drop=True)\n",
    "else:\n",
    "    # DataFrame rỗng, KHÔNG dummy row\n",
    "    submission7 = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"video_id\",\n",
    "            \"agent_id\",\n",
    "            \"target_id\",\n",
    "            \"action\",\n",
    "            \"start_frame\",\n",
    "            \"stop_frame\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Thêm row_id (kể cả khi rỗng)\n",
    "submission7.insert(0, \"row_id\", np.arange(len(submission7), dtype=np.int64))\n",
    "\n",
    "sub_path = WORKING_DIR / \"submission7.csv\"\n",
    "submission7.to_csv(sub_path, index=False)\n",
    "print(f\"Saved SparklingTapir submission to {sub_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ff0556",
   "metadata": {
    "papermill": {
     "duration": 0.020878,
     "end_time": "2025-12-13T17:39:18.405660",
     "exception": false,
     "start_time": "2025-12-13T17:39:18.384782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TranquilPanther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e0ae47c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:18.448195Z",
     "iopub.status.busy": "2025-12-13T17:39:18.447674Z",
     "iopub.status.idle": "2025-12-13T17:39:18.572880Z",
     "shell.execute_reply": "2025-12-13T17:39:18.572084Z"
    },
    "papermill": {
     "duration": 0.148026,
     "end_time": "2025-12-13T17:39:18.574198",
     "exception": false,
     "start_time": "2025-12-13T17:39:18.426172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "\n",
    "# 1) Xóa mọi thứ trong /kaggle/working trừ .csv\n",
    "for path in WORKING_DIR.iterdir():\n",
    "    # giữ lại file .csv\n",
    "    if path.is_file() and path.suffix == \".csv\":\n",
    "        continue\n",
    "\n",
    "    if path.is_file():\n",
    "        try:\n",
    "            path.unlink()\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove file {path}: {e}\")\n",
    "    elif path.is_dir():\n",
    "        try:\n",
    "            shutil.rmtree(path, ignore_errors=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove dir {path}: {e}\")\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "512661f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:18.615711Z",
     "iopub.status.busy": "2025-12-13T17:39:18.615470Z",
     "iopub.status.idle": "2025-12-13T17:39:18.771061Z",
     "shell.execute_reply": "2025-12-13T17:39:18.770076Z"
    },
    "papermill": {
     "duration": 0.178352,
     "end_time": "2025-12-13T17:39:18.772389",
     "exception": false,
     "start_time": "2025-12-13T17:39:18.594037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== START INFERENCE ===\n",
      "\n",
      "Done! Saved submission to /kaggle/working/submission8.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "np.seterr(invalid=\"ignore\", divide=\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    \"\"\"\n",
    "    Chứa cấu hình tham số (Hyperparameters).\n",
    "    \"\"\"\n",
    "    fps: float = 30.0\n",
    "    pix_per_cm: float = 1.0\n",
    "    smooth_sigma: float = 1.0\n",
    "    use_pairwise: bool = True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. AGENT CONTEXT\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class AgentContext:\n",
    "    \"\"\"\n",
    "    Container chứa dữ liệu đã tiền xử lý của một con chuột.\n",
    "    Giúp tránh việc tính toán lại vận tốc/gia tốc nhiều lần.\n",
    "    \"\"\"\n",
    "    idx: pd.Index          # Index frame\n",
    "    pos: np.ndarray        # [F, 2] cm\n",
    "    vel: np.ndarray        # [F, 2] cm/s\n",
    "    speed: np.ndarray      # [F, 1] cm/s\n",
    "    acc: np.ndarray        # [F, 2] cm/s^2\n",
    "    \n",
    "    cx: pd.Series          # Series tọa độ X (để dùng rolling)\n",
    "    cy: pd.Series          # Series tọa độ Y\n",
    "    speed_series: pd.Series # Series tốc độ\n",
    "    \n",
    "    raw_df: Optional[pd.DataFrame] = None # Dữ liệu gốc các bộ phận \n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. FEATURE EXTRACTOR\n",
    "# =============================================================================\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Class trích xuất đặc trưng hành vi từ dữ liệu tracking.\n",
    "    \"\"\"\n",
    "    def __init__(self, fps: float, pix_per_cm: float, smooth_sigma: float = 1.0, use_pairwise: bool = True):\n",
    "        # Map tham số từ init vào Config\n",
    "        self.cfg = FeatureConfig(\n",
    "            fps=float(fps), \n",
    "            pix_per_cm=float(pix_per_cm), \n",
    "            smooth_sigma=smooth_sigma,\n",
    "            use_pairwise=use_pairwise\n",
    "        )\n",
    "        \n",
    "        # Đăng ký các hàm feature sẽ chạy\n",
    "        self.feature_registry = {\n",
    "            \"kinematics\": self._feat_basic_kinematics,\n",
    "            \"multiscale\": self._feat_multiscale,\n",
    "            \"long_range\": self._feat_long_range,\n",
    "            \"cumulative\": self._feat_cumulative,\n",
    "            \"curvature\": self._feat_curvature,\n",
    "            \"speed_asym\": self._feat_speed_asym,\n",
    "            \"gauss_shift\": self._feat_gauss_shift,\n",
    "            \"pose_shape\": self._feat_pose_shape,\n",
    "            \"pairwise\": self._feat_pairwise,\n",
    "            \"follow\": self._feat_follow_pattern,\n",
    "            \"short\": self._feat_shortburst_social,\n",
    "            \"a\": self._feat_attack_sniff,\n",
    "            \"b\": self._feat_climb,\n",
    "            \"c\": self._feat_ejaculate_temporal\n",
    "        }\n",
    "\n",
    "    # --- Helpers ---\n",
    "    def _scale(self, n_frames_30fps: int) -> int:\n",
    "        \"\"\"Quy đổi số frame từ chuẩn 30fps sang fps thực tế của video.\"\"\"\n",
    "        return max(1, int(round(n_frames_30fps * self.cfg.fps / 30.0)))\n",
    "\n",
    "    def _to_cm(self, arr):\n",
    "        \"\"\"Chuyển pixel -> cm.\"\"\"\n",
    "        return arr / self.cfg.pix_per_cm\n",
    "\n",
    "    def _smooth(self, x):\n",
    "        \"\"\"Làm mượt dữ liệu bằng Gaussian filter.\"\"\"\n",
    "        if self.cfg.smooth_sigma is None or x.shape[0] < 3: return x\n",
    "        if np.all(np.isnan(x)): return x\n",
    "        return gaussian_filter1d(x, sigma=self.cfg.smooth_sigma, axis=0, mode=\"nearest\")\n",
    "\n",
    "    def _forward_fill_nan(self, pos):\n",
    "        \"\"\"\n",
    "        Điền dữ liệu thiếu (NaN) bằng giá trị hợp lệ trước đó (Forward Fill).\n",
    "        \"\"\"\n",
    "        if np.all(np.isnan(pos)):\n",
    "            return np.zeros_like(pos)\n",
    "\n",
    "        pos_ffill = pos.copy()\n",
    "        mask = np.any(~np.isnan(pos_ffill), axis=1)\n",
    "        if not mask.any():\n",
    "            return np.zeros_like(pos_ffill)\n",
    "\n",
    "        valid_idx = np.where(mask)[0]\n",
    "        first, last = valid_idx[0], valid_idx[-1]\n",
    "        pos_ffill[:first] = pos_ffill[first]\n",
    "        pos_ffill[last + 1:] = pos_ffill[last]\n",
    "        df_temp = pd.DataFrame(pos_ffill)\n",
    "        df_temp = df_temp.ffill()\n",
    "        return df_temp.to_numpy()\n",
    "    \n",
    "    def _speed_series(self, cx: pd.Series, cy: pd.Series) -> pd.Series:\n",
    "        dx = cx.diff()\n",
    "        dy = cy.diff()\n",
    "        v = np.hypot(dx, dy).fillna(0.0) * self.cfg.fps\n",
    "        return v.astype(\"float32\")\n",
    "    \n",
    "    def _roll_future_mean(self, s: pd.Series, w: int, min_p: int = 1) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).mean().iloc[::-1]\n",
    "\n",
    "    def _roll_future_var(self, s: pd.Series, w: int, min_p: int = 2) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).var().iloc[::-1]\n",
    "\n",
    "    # --- Core Logic ---\n",
    "    def _compute_kinematics(self, pos_px: np.ndarray):\n",
    "        \"\"\"\n",
    "        Tính toán vật lý cơ bản: Pos(cm), Vel, Speed, Acc.\n",
    "        Input: Array [Frames, 2] (pixel).\n",
    "        Output: Tuple (pos_cm, vel, speed, acc).\n",
    "        \"\"\"\n",
    "        pos_ffill = self._forward_fill_nan(pos_px)\n",
    "        pos_cm = self._to_cm(pos_ffill.astype(np.float32))\n",
    "        pos_cm = self._smooth(pos_cm)                                               # [F, 2]\n",
    "\n",
    "        dt = 1.0 / self.cfg.fps\n",
    "        vel = np.zeros_like(pos_cm, dtype=np.float32)\n",
    "        vel[1:] = (pos_cm[1:] - pos_cm[:-1]) / dt                                   # [F, 2: (vx, vy)]\n",
    "        speed = np.linalg.norm(vel, axis=1, keepdims=True).astype(np.float32)       # [F, 1]\n",
    "\n",
    "        acc = np.zeros_like(pos_cm, dtype=np.float32)                          \n",
    "        acc[1:] = (vel[1:] - vel[:-1]) / dt                                         # [F, 2:(ax, ay)]\n",
    "        return pos_cm.astype(np.float32), vel, speed, acc\n",
    "\n",
    "    def _build_context(self, frames, pos_px, mouse_df=None) -> AgentContext:\n",
    "        \"\"\"\n",
    "        Tạo AgentContext chứa đầy đủ thông tin vật lý của 1 con chuột.\n",
    "        \"\"\"\n",
    "        p, v, s, a = self._compute_kinematics(pos_px)\n",
    "        idx = pd.Index(frames, name=\"frame\")\n",
    "        \n",
    "        return AgentContext(\n",
    "            idx=idx, pos=p, vel=v, speed=s, acc=a, \n",
    "            cx=pd.Series(p[:, 0], index=idx), \n",
    "            cy=pd.Series(p[:, 1], index=idx), \n",
    "            speed_series=pd.Series(s[:, 0], index=idx), \n",
    "            raw_df=mouse_df\n",
    "        )\n",
    "\n",
    "    # --- Feature Modules ---\n",
    "    def _feat_basic_kinematics(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Lấy các giá trị thô: tọa độ x, y, vận tốc vx, vy, tốc độ, gia tốc ax, ay.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"a_x\": ctx.pos[:, 0], \"a_y\": ctx.pos[:, 1],\n",
    "            \"a_vx\": ctx.vel[:, 0], \"a_vy\": ctx.vel[:, 1],\n",
    "            \"a_speed\": ctx.speed[:, 0],\n",
    "            \"a_ax\": ctx.acc[:, 0], \"a_ay\": ctx.acc[:, 1]\n",
    "        }\n",
    "\n",
    "    def _feat_multiscale(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tính tốc độ trung bình (Mean) và độ lệch chuẩn (Std) ở đa mức thời gian.\n",
    "        Feature 'sp_ratio' đo độ bùng nổ (Burstiness).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        speed = ctx.speed_series\n",
    "        frame_scales = [10, 40, 160]\n",
    "        for scale in frame_scales:\n",
    "            ws = self._scale(scale)\n",
    "            if len(speed) >= ws:\n",
    "                roller = speed.rolling(ws, min_periods=max(1, ws//4), center=True)\n",
    "                feats[f\"sp_m{scale}\"] = roller.mean().astype(\"float32\")\n",
    "                feats[f\"sp_s{scale}\"] = roller.std().astype(\"float32\")\n",
    "        feats[f\"sp_ratio\"] = feats[\"sp_m10\"] / (feats[\"sp_m160\"] + 1e-6)\n",
    "        return feats \n",
    "        \n",
    "    def _feat_long_range(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng ngữ cảnh dài hạn:\n",
    "        - x_ml, y_ml: Vị trí trung bình trong quá khứ.\n",
    "        - sp_pct: Xếp hạng (percentile) của tốc độ hiện tại so với quá khứ.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        speed = ctx.speed_series\n",
    "\n",
    "        for window in [120, 240]:\n",
    "            ws = self._scale(window)\n",
    "            if len(ctx.cx) >= ws:\n",
    "                feats[f\"x_ml{window}\"] = ctx.cx.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "                feats[f\"y_ml{window}\"] = ctx.cy.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "\n",
    "        for span in [60, 120]:\n",
    "            s = self._scale(span)\n",
    "            feats[f\"x_e{span}\"] = ctx.cx.ewm(span=s, min_periods=1).mean()\n",
    "            feats[f\"y_e{span}\"] = ctx.cy.ewm(span=s, min_periods=1).mean()\n",
    "\n",
    "        for window in [60, 120]:\n",
    "            ws = self._scale(window)\n",
    "            if len(speed) >= ws:\n",
    "                feats[f\"sp_pct{window}\"] = speed.rolling(\n",
    "                    ws, min_periods=max(5, ws // 6), center=True\n",
    "                ).rank(pct=True)\n",
    "        return feats\n",
    "    \n",
    "\n",
    "    def _feat_curvature(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        feats = {}\n",
    "\n",
    "        vel_x, vel_y = ctx.vel[:, 0], ctx.vel[:, 1]\n",
    "        acc_x, acc_y = ctx.acc[:, 0], ctx.acc[:, 1]\n",
    "        cross_prod = vel_x * acc_y - vel_y * acc_x\n",
    "        vel_mag = np.sqrt(vel_x**2 + vel_y**2)\n",
    "        moving_mask = vel_mag > 2.0\n",
    "        vel_mag_safe = np.maximum(vel_mag, 0.1 / self.cfg.fps)\n",
    "        raw_curv = cross_prod / (vel_mag_safe**3)\n",
    "        raw_curv = np.where(moving_mask, raw_curv, 0.0)\n",
    "        min_turn_radius_cm = 0.5\n",
    "        max_k = 1.0 / min_turn_radius_cm\n",
    "        raw_curv = np.clip(raw_curv, -max_k, max_k)\n",
    "        abs_curv = np.abs(raw_curv)\n",
    "        abs_curv_series = pd.Series(abs_curv, index=ctx.idx)\n",
    "\n",
    "        for w in [30, 60]:\n",
    "            ws = self._scale(w)\n",
    "            min_p = max(ws // 3, 1)\n",
    "            feats[f\"curv_mean_{w}\"] = abs_curv_series.rolling(ws, min_periods=min_p).mean()\n",
    "\n",
    "        angle = np.arctan2(vel_y, vel_x)\n",
    "        angle_series = pd.Series(angle, index=ctx.idx)\n",
    "        angle_change = np.abs(angle_series.diff().fillna(0.0))\n",
    "        angle_change = np.where(angle_change > np.pi, 2 * np.pi - angle_change, angle_change)\n",
    "        angle_change_series = pd.Series(angle_change, index=ctx.idx)\n",
    "        angle_change_series = pd.Series(np.where(moving_mask, angle_change_series, 0.0), index=ctx.idx)\n",
    "\n",
    "        ws = self._scale(30)\n",
    "        feats[\"turn_rate_30\"] = angle_change_series.rolling(ws, min_periods=max(ws // 3, 1)).sum()\n",
    "\n",
    "        return feats\n",
    "    \n",
    "    def _feat_cumulative(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tổng quãng đường di chuyển trong một khoảng thời gian dài xung quanh frame hiện tại.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        L = max(1, self._scale(180))\n",
    "        step = np.hypot(ctx.cx.diff(), ctx.cy.diff()).fillna(0.0)\n",
    "        path = step.rolling(2 * L + 1, min_periods=max(5, L // 6), center=True).sum()\n",
    "        feats[\"path_cum180\"] =  path.fillna(0.0).astype(\"float32\")\n",
    "        return feats\n",
    "\n",
    "    def _feat_speed_asym(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Bất đối xứng tốc độ (Tương lai - Quá khứ).\n",
    "        \"\"\"\n",
    "        w = max(3, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        v_past = v.rolling(w, min_periods=1).mean()\n",
    "        v_fut = self._roll_future_mean(v, w, min_p=1)\n",
    "        return {\"spd_asym_1s\": (v_fut - v_past).fillna(0.0)}\n",
    "    \n",
    "    def _feat_gauss_shift(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Độ lệch Gaussian (KL Divergence) giữa quá khứ và tương lai.\n",
    "        Đo lường sự thay đổi trạng thái thống kê.\n",
    "        \"\"\"\n",
    "        w = max(5, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        mu_p = v.rolling(w, min_periods=1).mean()\n",
    "        va_p = v.rolling(w, min_periods=1).var().clip(lower=1e-6)\n",
    "        mu_f = self._roll_future_mean(v, w, min_p=1)\n",
    "        va_f = self._roll_future_var(v, w, min_p=1).clip(lower=1e-6)\n",
    "\n",
    "        kl_pf = 0.5 * (\n",
    "            (va_p / va_f) + ((mu_f - mu_p) ** 2) / va_f - 1.0 + np.log(va_f / va_p)\n",
    "        )\n",
    "        kl_fp = 0.5 * (\n",
    "            (va_f / va_p) + ((mu_p - mu_f) ** 2) / va_p - 1.0 + np.log(va_p / va_f)\n",
    "        )\n",
    "        return {\n",
    "            \"spd_symkl_1s\": (kl_pf + kl_fp).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        }\n",
    "    \n",
    "    def _extract_part(self, ctx: AgentContext, part: str) -> Optional[np.ndarray]:\n",
    "        if ctx.raw_df is None: return None\n",
    "        if part not in ctx.raw_df.columns.get_level_values(0): return None\n",
    "        try:\n",
    "            sub_df = ctx.raw_df.xs(part, axis=1, level=0)[[\"x\", \"y\"]].reindex(ctx.idx)\n",
    "        except KeyError: return None\n",
    "        raw = sub_df.to_numpy()\n",
    "        raw = self._forward_fill_nan(raw)\n",
    "        cm = self._to_cm(raw.astype(np.float32))\n",
    "        return self._smooth(cm)\n",
    "    \n",
    "    def _extract_parts_dict(self, ctx: AgentContext, parts: List[str] = None) -> Dict[str, Optional[np.ndarray]]:\n",
    "        out = {}\n",
    "        for p in parts:\n",
    "            out[p] = self._extract_part(ctx, p)\n",
    "        return out\n",
    "        \n",
    "    def _feat_pose_shape(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Placeholder cho các đặc trưng hình dáng (Elongation, Body Angle...).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "\n",
    "        def zero(): return pd.Series(0.0, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        def dist(k1, k2):\n",
    "            p1, p2 = parts.get(k1), parts.get(k2)\n",
    "            if p1 is None or p2 is None: return zero()\n",
    "            d = np.linalg.norm(p1 - p2, axis=1)\n",
    "            return pd.Series(d, index=ctx.idx, dtype=\"float32\")\n",
    "        \n",
    "        def body_angle():\n",
    "            if parts.get(\"nose\") is None: return zero()\n",
    "            if parts.get(\"neck\") is None: return zero()\n",
    "            if parts.get(\"tail_base\") is None: return zero()\n",
    "\n",
    "            v1 = parts.get(\"nose\") - parts.get(\"neck\")\n",
    "            v2 = parts.get(\"tail_base\") - parts.get(\"neck\")\n",
    "            dot_product = np.sum(v1 * v2, axis=1)\n",
    "            mag = np.linalg.norm(v1, axis=1) * np.linalg.norm(v2, axis=1)\n",
    "            cos_angle = np.clip(dot_product / (mag + 1e-6), -1.0, 1.0).astype(\"float32\")\n",
    "            return cos_angle\n",
    "        \n",
    "        def elongation():\n",
    "            if parts.get(\"nose\")          is None: return zero()\n",
    "            if parts.get(\"tail_base\")     is None: return zero()\n",
    "            if parts.get(\"hip_left\")  is None: return zero()\n",
    "            if parts.get(\"hip_right\") is None: return zero()\n",
    "\n",
    "            d1 = dist(\"nose\", \"tail_base\")\n",
    "            d2 = dist(\"hip_left\", \"hip_right\")\n",
    "            elongation = d1 / (d2 + 1e-6).astype(\"float32\")\n",
    "            return elongation\n",
    "        \n",
    "        def part_speed(part: str, n_frames_30fps: int) -> Dict:\n",
    "            part_pos = self._extract_part(ctx, part)\n",
    "            if part_pos is None: return zero()\n",
    "            \n",
    "            s_x = pd.Series(part_pos[:, 0], index=ctx.idx)\n",
    "            s_y = pd.Series(part_pos[:, 1], index=ctx.idx)\n",
    "            raw_speed = self._speed_series(s_x, s_y)\n",
    "\n",
    "            ws = self._scale(n_frames_30fps)\n",
    "            val = raw_speed.rolling(ws, min_periods=1, center=True).mean()\n",
    "            return val.astype(\"float32\")\n",
    "\n",
    "\n",
    "        target_parts = [\"nose\", \"hip_left\", \"hip_right\", \"ear_left\", \"ear_right\", \"tail_base\", \"neck\"]\n",
    "        \n",
    "        parts = self._extract_parts_dict(ctx, target_parts)\n",
    "\n",
    "        feats[\"aa_nose_tailbase_dist\"]       = dist(\"nose\", \"tail_base\")\n",
    "        feats[\"aa_earleft_tailbase_dist\"]    = dist(\"ear_left\", \"tail_base\")\n",
    "        feats[\"aa_earright_tailbase_dist\"]   = dist(\"ear_right\", \"tail_base\")\n",
    "        feats[\"aa_nose_earleft_dist\"]        = dist(\"ear_left\", \"nose\")\n",
    "        feats[\"aa_nose_ear_right_dist\"]      = dist(\"ear_right\", \"nose\")\n",
    "        feats[\"aa_nose_hip_left_dist\"]       = dist(\"nose\", \"hip_left\")\n",
    "        feats[\"aa_nose_hip_right_dist\"]      = dist(\"nose\", \"hip_right\")\n",
    "        feats[\"aa_neck_tailbase_dist\"] = dist(\"neck\", \"tail_base\")\n",
    "        \n",
    "        feats[\"a_elongation\"]                = elongation()\n",
    "        feats[\"a_bodyangle\"]                 = body_angle()\n",
    "\n",
    "        a_tail_base_vel_500ms     = part_speed(\"tail_base\", 15)\n",
    "        a_tail_base_vel_1000ms    = part_speed(\"tail_base\", 30)\n",
    "        a_tail_base_vel_2000ms    = part_speed(\"tail_base\", 60)\n",
    "        a_tail_base_vel_3000ms    = part_speed(\"tail_base\", 90)\n",
    "\n",
    "\n",
    "        a_hip_left_vel_500ms          = part_speed(\"hip_left\", 15)\n",
    "        a_hip_left_vel_1000ms         = part_speed(\"hip_left\", 30)\n",
    "        a_hip_left_vel_2000ms         = part_speed(\"hip_left\", 60)\n",
    "        a_hip_left_vel_3000ms         = part_speed(\"hip_left\", 90)\n",
    "\n",
    "        a_hip_right_vel_500ms          = part_speed(\"hip_left\", 15)\n",
    "        a_hip_right_vel_1000ms         = part_speed(\"hip_left\", 30)\n",
    "        a_hip_right_vel_2000ms         = part_speed(\"hip_left\", 60)\n",
    "        a_hip_right_vel_3000ms         = part_speed(\"hip_left\", 90)\n",
    "\n",
    "        feats[\"a_upper_vel_500ms\"]            = (a_tail_base_vel_500ms + a_hip_left_vel_500ms + a_hip_right_vel_500ms)/3.0\n",
    "        feats[\"a_upper_vel_1000ms\"]           = (a_tail_base_vel_1000ms + a_hip_left_vel_1000ms + a_hip_right_vel_1000ms)/3.0\n",
    "        feats[\"a_upper_vel_2000ms\"]           = (a_tail_base_vel_2000ms + a_hip_left_vel_2000ms + a_hip_right_vel_2000ms)/3.0\n",
    "        feats[\"a_upper_vel_3000ms\"]           = (a_tail_base_vel_3000ms + a_hip_left_vel_3000ms + a_hip_right_vel_3000ms)/3.0\n",
    "\n",
    "\n",
    "        feats[\"a_nose_vel_500ms\"]            = part_speed(\"nose\", 15)\n",
    "        feats[\"a_nose_vel_1000ms\"]           = part_speed(\"nose\", 30)\n",
    "        feats[\"a_nose_vel_2000ms\"]           = part_speed(\"nose\", 60)\n",
    "        feats[\"a_nose_vel_3000ms\"]           = part_speed(\"nose\", 90)\n",
    "\n",
    "        # feats[\"a_ear_right_vel_500ms\"]       = part_speed(\"hip_right\", 15)\n",
    "        # feats[\"a_ear_right_vel_1000ms\"]      = part_speed(\"hip_right\", 30)\n",
    "        # feats[\"a_ear_right_vel_2000ms\"]      = part_speed(\"hip_right\", 60)\n",
    "        # feats[\"a_ear_right_vel_3000ms\"]      = part_speed(\"hip_right\", 90)\n",
    "        # feats[\"a_ear_left_vel_500ms\"]        = part_speed(\"ear_left\", 15)\n",
    "        # feats[\"a_ear_left_vel_1000ms\"]       = part_speed(\"ear_left\", 30)\n",
    "        # feats[\"a_ear_left_vel_2000ms\"]       = part_speed(\"ear_left\", 60)\n",
    "        # feats[\"a_ear_left_vel_3000ms\"]       = part_speed(\"ear_left\", 90)\n",
    "        \n",
    "        return feats\n",
    "\n",
    "    def _feat_attack_sniff(\n",
    "        self,\n",
    "        ctx: AgentContext,\n",
    "        target_ctx: AgentContext = None,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng phân biệt attack vs sniff cho lab 2-mouse (agent=1, target=2).\n",
    "    \n",
    "        Ý tưởng:\n",
    "          - attack: speed 2 con biến động mạnh, đổi hướng nhiều, body overlap cao.\n",
    "          - sniff : mũi gần cổ/thân, overlap thấp hơn, motion nhẹ/ổn định hơn.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "    \n",
    "        def zero():\n",
    "            return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "\n",
    "        # helper khoảng cách\n",
    "        def dist(p1, p2):\n",
    "            if p1 is None or p2 is None:\n",
    "                return zero()\n",
    "            d = np.linalg.norm(p1 - p2, axis=1)\n",
    "            return pd.Series(d, index=idx, dtype=\"float32\")\n",
    "\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\"])\n",
    "        parts_t = self._extract_parts_dict(target_ctx, [\"nose\", \"tail_base\"])\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 2) ĐIỂM ĐẠI DIỆN THÂN (BODY CENTER) CHO MỖI CON\n",
    "        #    dùng trung bình neck – hips – tail_base\n",
    "        # ---------------------------------------------------------\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 4) MỨC ĐỘ “BẠO LỰC”: DAO ĐỘNG TỐC ĐỘ & ĐỔI HƯỚNG\n",
    "        # ---------------------------------------------------------\n",
    "        # speed 2 con từ velocity\n",
    "        a_speed = pd.Series(\n",
    "            np.linalg.norm(ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "\n",
    "        ws_05 = self._scale(15)  # ~0.5s\n",
    "        mp_05 = max(ws_05 // 3, 1)\n",
    "    \n",
    "        feats[\"as_a_speed_std_05\"] = (\n",
    "            a_speed.rolling(ws_05, min_periods=mp_05).std().fillna(0.0).astype(\"float32\")\n",
    "        )\n",
    "        feats[\"as_t_speed_std_05\"] = (\n",
    "            t_speed.rolling(ws_05, min_periods=mp_05).std().fillna(0.0).astype(\"float32\")\n",
    "        )\n",
    "        feats[\"as_speed_std_sum_05\"] = (\n",
    "            feats[\"as_a_speed_std_05\"] + feats[\"as_t_speed_std_05\"]\n",
    "        )\n",
    "    \n",
    "        # Đổi hướng (jerk góc) của agent\n",
    "        a_angle = np.arctan2(ctx.vel[:, 1], ctx.vel[:, 0])\n",
    "        a_angle_diff = np.abs(np.diff(a_angle))\n",
    "        a_angle_diff = np.where(\n",
    "            a_angle_diff > np.pi, 2 * np.pi - a_angle_diff, a_angle_diff\n",
    "        )\n",
    "        a_angle_diff = np.concatenate([[0.0], a_angle_diff])\n",
    "        a_angle_diff_s = pd.Series(a_angle_diff, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        feats[\"as_a_turn_jerk_05\"] = (\n",
    "            a_angle_diff_s.rolling(ws_05, min_periods=mp_05)\n",
    "            .sum()\n",
    "            .fillna(0.0)\n",
    "            .astype(\"float32\")\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 5) XẤP XỈ OVERLAP CƠ THỂ (BODY OVERLAP)\n",
    "        #    dùng bbox từ các bộ phận thân\n",
    "        # ---------------------------------------------------------\n",
    "        def build_bbox(parts: Dict[str, Optional[np.ndarray]]):\n",
    "            arrs = []\n",
    "            for k in [\"nose\", \"hip_left\", \"hip_right\", \"ear_left\", \"ear_right\", \"tail_base\"]:\n",
    "                if parts.get(k) is not None:\n",
    "                    arrs.append(parts[k])\n",
    "            if not arrs:\n",
    "                return None\n",
    "            stack = np.stack(arrs, axis=1)  # [F, K, 2]\n",
    "            xs = stack[:, :, 0]\n",
    "            ys = stack[:, :, 1]\n",
    "            xmin = np.nanmin(xs, axis=1)\n",
    "            xmax = np.nanmax(xs, axis=1)\n",
    "            ymin = np.nanmin(ys, axis=1)\n",
    "            ymax = np.nanmax(ys, axis=1)\n",
    "            return np.stack([xmin, ymin, xmax, ymax], axis=1).astype(\"float32\")\n",
    "    \n",
    "        def iou_box(box1: np.ndarray, box2: np.ndarray):\n",
    "            # box: [F, 4] = (xmin, ymin, xmax, ymax)\n",
    "            x1 = np.maximum(box1[:, 0], box2[:, 0])\n",
    "            y1 = np.maximum(box1[:, 1], box2[:, 1])\n",
    "            x2 = np.minimum(box1[:, 2], box2[:, 2])\n",
    "            y2 = np.minimum(box1[:, 3], box2[:, 3])\n",
    "    \n",
    "            inter_w = np.clip(x2 - x1, 0.0, None)\n",
    "            inter_h = np.clip(y2 - y1, 0.0, None)\n",
    "            inter = inter_w * inter_h\n",
    "    \n",
    "            area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "            area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "            union = area1 + area2 - inter + 1e-6\n",
    "            iou = inter / union\n",
    "            return iou.astype(\"float32\")\n",
    "\n",
    "        bbox_a = build_bbox(parts_a)\n",
    "        bbox_t = build_bbox(parts_t)\n",
    "        if bbox_a is not None and bbox_t is not None:\n",
    "            iou = iou_box(bbox_a, bbox_t)\n",
    "            iou_s = pd.Series(iou, index=idx, dtype=\"float32\")\n",
    "    \n",
    "            feats[\"as_body_iou\"] = iou_s\n",
    "    \n",
    "            ws_1s = self._scale(30)\n",
    "            mp_1s = max(ws_1s // 3, 1)\n",
    "            feats[\"as_body_iou_mean_1s\"] = (\n",
    "                iou_s.rolling(ws_1s, min_periods=mp_1s).mean().fillna(0.0).astype(\"float32\")\n",
    "            )\n",
    "        else:\n",
    "            feats[\"as_body_iou\"] = zero()\n",
    "            feats[\"as_body_iou_mean_1s\"] = zero()\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 6) DỌN NẠN NaN / Inf\n",
    "        # ---------------------------------------------------------\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "\n",
    "    def _feat_climb(self, ctx: AgentContext, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Feature chuyên cho hành vi climb trong arena hình chữ nhật (33 x 19 cm).\n",
    "    \n",
    "        Ý tưởng:\n",
    "          - Chuột đi gần tường: dist_wall giảm nhanh.\n",
    "          - Khi climb: sát tường (dist_wall nhỏ), v_normal ~ 0,\n",
    "            nhưng vẫn có v_tangent (bò ngang trên tường / di chuyển dọc biên).\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        idx = ctx.idx\n",
    "    \n",
    "        def zero() -> pd.Series:\n",
    "            return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 1. Arena size (cm) ---\n",
    "        # Nếu bạn đã set trong FeatureConfig thì dùng:\n",
    "        # W = self.cfg.arena_width_cm or 33.0\n",
    "        # H = self.cfg.arena_height_cm or 19.0\n",
    "        # Ở đây fix luôn cho lab này:\n",
    "        W = 28.0\n",
    "        H = 18.0\n",
    "        parts = self._extract_parts_dict(ctx, [\"nose\"])\n",
    "        head = parts.get(\"nose\")\n",
    "        \n",
    "        if head is not None:\n",
    "            # head đã ở đơn vị cm (vì _extract_part đã to_cm + smooth)\n",
    "            cx = pd.Series(head[:, 0], index=idx)\n",
    "            cy = pd.Series(head[:, 1], index=idx)\n",
    "        else:\n",
    "            # fallback: nếu không có head thì dùng body_center như cũ\n",
    "            cx = ctx.cx\n",
    "            cy = ctx.cy\n",
    "\n",
    "\n",
    "        # # --- 2. Khoảng cách tới 4 bức tường ---\n",
    "        # cx = ctx.cx  # Series\n",
    "        # cy = ctx.cy  # Series\n",
    "    \n",
    "        dist_left   = cx - 0.0\n",
    "        dist_right  = W - cx\n",
    "        dist_bottom = cy - 0.0\n",
    "        dist_top    = H - cy\n",
    "    \n",
    "        d_all = np.stack(\n",
    "            [dist_left.values, dist_right.values, dist_bottom.values, dist_top.values],\n",
    "            axis=1,  # [F, 4]\n",
    "        )\n",
    "    \n",
    "        dist_wall = np.min(d_all, axis=1)          # khoảng cách tới tường gần nhất\n",
    "        wall_idx  = np.argmin(d_all, axis=1)       # 0:left, 1:right, 2:bottom, 3:top\n",
    "    \n",
    "        dist_wall_s = pd.Series(dist_wall, index=idx, dtype=\"float32\")\n",
    "        feats[\"climb_dist_wall\"] = dist_wall_s\n",
    "    \n",
    "        # --- 3. Vận tốc theo NORMAL & TANGENT của tường gần nhất ---\n",
    "        vx = ctx.vel[:, 0]\n",
    "        vy = ctx.vel[:, 1]\n",
    "    \n",
    "        # normal hướng VÀO trong arena từ tường\n",
    "        nx = np.zeros_like(vx, dtype=\"float32\")\n",
    "        ny = np.zeros_like(vy, dtype=\"float32\")\n",
    "\n",
    "        # left  wall (x=0)    → normal = (+1, 0)\n",
    "        # right wall (x=W)    → normal = (-1, 0)\n",
    "        # bottom wall (y=0)   → normal = (0, +1)\n",
    "        # top wall (y=H)      → normal = (0, -1)\n",
    "        nx[wall_idx == 0] =  1.0\n",
    "        nx[wall_idx == 1] = -1.0\n",
    "        ny[wall_idx == 2] =  1.0\n",
    "        ny[wall_idx == 3] = -1.0\n",
    "    \n",
    "        # v_normal = v ⋅ n\n",
    "        v_normal = vx * nx + vy * ny\n",
    "    \n",
    "        # thành phần song song tường: v_tan = v - (v⋅n)n\n",
    "        v_proj_x = v_normal * nx\n",
    "        v_proj_y = v_normal * ny\n",
    "        v_tan_x = vx - v_proj_x\n",
    "        v_tan_y = vy - v_proj_y\n",
    "        v_tangent = np.sqrt(v_tan_x ** 2 + v_tan_y ** 2)\n",
    "    \n",
    "        v_normal_s  = pd.Series(v_normal,  index=idx, dtype=\"float32\")\n",
    "        v_tangent_s = pd.Series(v_tangent, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        feats[\"climb_normal_vel\"]  = v_normal_s\n",
    "        feats[\"climb_tangent_vel\"] = v_tangent_s\n",
    "    \n",
    "        # --- 4. Approach speed: dist_wall giảm mạnh (lao vào tường) ---\n",
    "        ws = self._scale(15)  # ~0.5s (15 frame ở 30fps)\n",
    "        min_p = max(ws // 3, 1)\n",
    "\n",
    "        # diff_dw > 0 khi dist_wall giảm (đi về phía tường)\n",
    "        diff_dw = -dist_wall_s.diff().fillna(0.0)  # dấu trừ để \"giảm\" → dương\n",
    "        approach = diff_dw.rolling(ws, min_periods=min_p).mean()\n",
    "        feats[\"climb_approach_speed_wall\"] = approach.astype(\"float32\")\n",
    "    \n",
    "        # --- 5. Stick score: sát tường + không còn lao vào (v_normal nhỏ) ---\n",
    "        # gần tường\n",
    "        thr_cm = 3.0  # tuỳ chỉnh (3cm sát tường)\n",
    "        near_wall = (dist_wall_s < thr_cm).astype(\"float32\")\n",
    "    \n",
    "        # ít lao vào nữa: |v_normal| nhỏ\n",
    "        stick = near_wall * (1.0 / (1.0 + v_normal_s.abs()))\n",
    "\n",
    "        # Nếu muốn climb thực sự có chút chuyển động dọc tường:\n",
    "        # yêu cầu v_tangent > một ngưỡng nhỏ (ví dụ 0.5 cm/s)\n",
    "        stick = stick * (v_tangent_s > 0.5).astype(\"float32\")\n",
    "    \n",
    "        feats[\"climb_wall_stick_score\"] = stick.astype(\"float32\")\n",
    "    \n",
    "        # --- 6. Clean NaN/Inf ---\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "\n",
    "\n",
    "    def _feat_pairwise(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng tương tác cặp đôi (Pairwise): Khoảng cách, Tốc độ tiếp cận.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        if target_ctx is None: \n",
    "            return feats\n",
    "\n",
    "        def zero(): return pd.Series(0.0, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        def dist_ab(pt_a, pt_b):\n",
    "            if pt_a is None or pt_b is None: return zero()\n",
    "            d = np.linalg.norm(pt_a - pt_b, axis=1)\n",
    "            return pd.Series(d, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        feats[\"rel_dist\"] = pd.Series(dist, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        # Khoảng cách\n",
    "        my_parts = self._extract_parts_dict(ctx, [\"nose\", \"ear_left\", \"ear_right\", \"body_center\", \"tail_base\", \"hip_left\", \"hip_right\", \"neck\"])\n",
    "        target_parts = self._extract_parts_dict(target_ctx, [\"nose\", \"ear_left\", \"ear_right\", \"body_center\", \"tail_base\", \"hip_left\", \"hip_right\", \"neck\"])\n",
    "\n",
    "        an, tn = my_parts[\"nose\"], target_parts[\"nose\"]\n",
    "        feats[\"dist_nose_nose\"] = dist_ab(an, tn)\n",
    "        feats[\"dist_nose_tail\"] = dist_ab(an, target_parts[\"tail_base\"])\n",
    "        feats[\"dist_nose_el\"]   = dist_ab(an, target_parts[\"ear_left\"])\n",
    "        feats[\"dist_nose_er\"]   = dist_ab(an, target_parts[\"ear_right\"])\n",
    "        feats[\"dist_nose_hip_l\"] = dist_ab(an, target_parts[\"hip_left\"])\n",
    "        feats[\"dist_nose_hip_r\"] = dist_ab(an, target_parts[\"hip_right\"])\n",
    "        feats[\"dist_nose_neck\"] = dist_ab(an, target_parts[\"neck\"])\n",
    "\n",
    "        \n",
    "        #  Hướng - góc nhìn\n",
    "        def get_body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"nose\")\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if head is not None and tail is not None:\n",
    "                return head - tail\n",
    "            return None\n",
    "\n",
    "        a_vec = get_body_vec(my_parts)\n",
    "        t_vec = get_body_vec(target_parts)\n",
    "\n",
    "        if a_vec is not None and t_vec is not None:\n",
    "            dot = np.sum(a_vec * t_vec, axis=1)\n",
    "            mags = np.linalg.norm(a_vec, axis=1) * np.linalg.norm(t_vec, axis=1)\n",
    "            feats[\"body_cosine\"] = pd.Series(\n",
    "                np.clip(dot / (mags + 1e-6), -1.0, 1.0), index=ctx.idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"body_cosine\"] = zero()\n",
    "\n",
    "        # Vector ánh nhìn = Target_Pos - My_Pos = rel_vec\n",
    "        if a_vec is not None:\n",
    "            dot_gaze = np.sum(a_vec * rel_vec, axis=1)\n",
    "            mag_a = np.linalg.norm(a_vec, axis=1)\n",
    "            feats[\"gaze_cosine\"] = pd.Series(\n",
    "                np.clip(dot_gaze / (mag_a * dist + 1e-6), -1.0, 1.0),\n",
    "                index=ctx.idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"gaze_cosine\"] = zero()\n",
    "\n",
    "        # Vector đơn vị hướng về địch (u)\n",
    "        dist_safe = dist.copy()\n",
    "        dist_safe[dist_safe == 0] = 1e-6\n",
    "        u_vec = rel_vec / dist_safe[:, None]\n",
    "\n",
    "        # a_vel và t_vel lấy từ Context\n",
    "        a_vel, t_vel = ctx.vel, target_ctx.vel\n",
    "\n",
    "        # A. Approach Speed (Vận tốc dọc trục nối 2 con)\n",
    "        # Dương: Lao vào nhau | Âm: Chạy ra xa nhau\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1) # Target hướng ngược lại\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)\n",
    "\n",
    "        # B. Lateral Speed (Vận tốc ngang - Vuông góc trục nối)\n",
    "        # Vector chiếu: v_proj = (v . u) * u\n",
    "        a_proj = a_along[:, None] * u_vec\n",
    "        a_lat_vec = a_vel - a_proj\n",
    "        a_lat_speed = np.linalg.norm(a_lat_vec, axis=1)\n",
    "\n",
    "        feats[\"approach_speed_agent\"]  = pd.Series(a_along, index=ctx.idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_target\"] = pd.Series(t_along, index=ctx.idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_rel\"]    = pd.Series(rel_along, index=ctx.idx, dtype=\"float32\")\n",
    "        feats[\"lateral_speed_agent\"]   = pd.Series(a_lat_speed, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        return feats\n",
    "\n",
    "\n",
    "    def _feat_ejaculate_temporal(\n",
    "        self,\n",
    "        ctx: AgentContext,\n",
    "        target_ctx: AgentContext = None,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng cho hành vi 'ejaculate' (pair):\n",
    "          - 2 con dính sát, agent gần vùng đuôi/genital của target.\n",
    "          - Trước đó có giai đoạn hoạt động mạnh (mount/intromit/thrust).\n",
    "          - Thời điểm ejaculate: agent gần như đứng yên nhưng vẫn sát target.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "    \n",
    "        def zero() -> pd.Series:\n",
    "            return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 1. PARTS: APPROX GENITAL & BODY\n",
    "        # -------------------------------------------------\n",
    "        # Agent: dùng body_center + nose\n",
    "        parts_a = self._extract_parts_dict(\n",
    "            ctx,\n",
    "            [\"nose\", \"body_center\", \"tail_base\", \"hip_left\", \"hip_right\"]\n",
    "        )\n",
    "        # Target: genital ~ tail_base, thân ~ body_center\n",
    "        parts_t = self._extract_parts_dict(\n",
    "            target_ctx,\n",
    "            [\"body_center\", \"tail_base\"]\n",
    "        )\n",
    "    \n",
    "        a_nose = parts_a.get(\"nose\")\n",
    "        a_bc   = parts_a.get(\"body_center\")\n",
    "        a_tail = parts_a.get(\"tail_base\")\n",
    "        t_bc   = parts_t.get(\"body_center\")\n",
    "        t_tail = parts_t.get(\"tail_base\")\n",
    "\n",
    "        # fallback body_center nếu thiếu\n",
    "        if a_bc is None and a_tail is not None:\n",
    "            a_bc = a_tail\n",
    "        if t_bc is None and t_tail is not None:\n",
    "            t_bc = t_tail\n",
    "    \n",
    "        def dist_series(p1: Optional[np.ndarray],\n",
    "                        p2: Optional[np.ndarray]) -> pd.Series:\n",
    "            if p1 is None or p2 is None:\n",
    "                return zero()\n",
    "            d = np.linalg.norm(p1 - p2, axis=1).astype(\"float32\")\n",
    "            return pd.Series(d, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # khoảng cách thân–thân và agent body → target genital\n",
    "        dist_body = dist_series(a_bc, t_bc)          # \"ôm\" nhau chặt hay không\n",
    "        dist_gen  = dist_series(a_bc, t_tail)       # agent body gần đuôi target\n",
    "        dist_nose_gen = dist_series(a_nose, t_tail) # mũi agent gần genital\n",
    "    \n",
    "        feats[\"ejac_dist_body\"]      = dist_body\n",
    "        feats[\"ejac_dist_gen_body\"]  = dist_gen\n",
    "        feats[\"ejac_dist_gen_nose\"]  = dist_nose_gen\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 2. PROXIMITY SCORE (khoảng cách nhỏ → score lớn)\n",
    "        # -------------------------------------------------\n",
    "        # scale ~ 5 cm, có thể chỉnh nếu arena nhỏ/lớn\n",
    "        prox_body = np.exp(-dist_body.to_numpy() / 5.0).astype(\"float32\")\n",
    "        prox_gen  = 1.0 / (1.0 + dist_gen.to_numpy())\n",
    "        prox_nose = 1.0 / (1.0 + dist_nose_gen.to_numpy())\n",
    "    \n",
    "        feats[\"ejac_prox_body\"] = pd.Series(prox_body, index=idx, dtype=\"float32\")\n",
    "        feats[\"ejac_prox_gen\"]  = pd.Series(prox_gen,  index=idx, dtype=\"float32\")\n",
    "        feats[\"ejac_prox_nose_gen\"] = pd.Series(prox_nose, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 3. BUILD-UP MEMORY: HOẠT ĐỘNG MẠNH TRƯỚC ĐÓ\n",
    "        # -------------------------------------------------\n",
    "        # dung speed của agent nhưng chỉ tính khi đang dính sát body\n",
    "        v = ctx.speed_series  # cm/s\n",
    "        close_mask = (dist_body < 5.0).astype(\"float32\")  # ở rất gần\n",
    "        v_contact = (v * close_mask).astype(\"float32\")\n",
    "\n",
    "        ws_mem = self._scale(90)  # ~3s\n",
    "        ws_mem = max(ws_mem, 1)\n",
    "    \n",
    "        ejac_mem = (\n",
    "            v_contact.rolling(ws_mem, min_periods=1)\n",
    "                     .max()\n",
    "                     .fillna(0.0)\n",
    "                     .astype(\"float32\")\n",
    "        )\n",
    "        feats[\"ejac_activity_memory_3s\"] = ejac_mem\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 4. HIỆN TẠI: ĐỨNG YÊN NHƯNG VẪN DÍNH SÁT\n",
    "        # -------------------------------------------------\n",
    "        # agent gần như đứng yên\n",
    "        is_still = (v < 1.5).astype(\"float32\")  # ngưỡng speed thấp, tuỳ lab\n",
    "        feats[\"ejac_is_still\"] = is_still\n",
    "    \n",
    "        # khoảng cách ổn định (không kéo xa/đẩy gần quá nhanh)\n",
    "        dist_body_diff = dist_body.diff().abs().fillna(0.0)\n",
    "        feats[\"ejac_dist_body_diff\"] = dist_body_diff.astype(\"float32\")\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 5. FINAL SCORE (gợi ý): cao khi ejaculate\n",
    "        # -------------------------------------------------\n",
    "        # điều kiện:\n",
    "        #  - trước đó hoạt động mạnh (ejac_mem lớn)\n",
    "        #  - bây giờ đứng yên (is_still ~1)\n",
    "        #  - vẫn dính sát, gần vùng genital\n",
    "        prox_comb = (\n",
    "            feats[\"ejac_prox_body\"] *\n",
    "            feats[\"ejac_prox_gen\"]  *\n",
    "            feats[\"ejac_prox_nose_gen\"]\n",
    "        )\n",
    "    \n",
    "        feats[\"ejac_static_score\"] = (\n",
    "            is_still * prox_comb * ejac_mem\n",
    "        ).astype(\"float32\")\n",
    "    \n",
    "        # -------------------------------------------------\n",
    "        # 6. CLEAN NaN / Inf\n",
    "        # -------------------------------------------------\n",
    "        for k, s in feats.items():\n",
    "            feats[k] = (\n",
    "                s.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "\n",
    "\n",
    "    def _feat_follow_pattern(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng hành vi FOLLOW:\n",
    "          - Agent ở gần target\n",
    "          - Cùng hướng (body + velocity)\n",
    "          - Tốc độ vừa phải\n",
    "          - Khoảng cách tương đối ổn định trong 0.5–1s\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 1. CÁC ĐẠI LƯỢNG CƠ BẢN ---\n",
    "        # Vector Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # Speed agent/target\n",
    "        a_speed = ctx.speed_series.astype(\"float32\")\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "    \n",
    "        # Body vector: nose - tail/body_center\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"nose\", \"ear_left\", \"ear_right\", \"body_center\", \"tail_base\", \"hip_left\", \"hip_right\", \"neck\"])\n",
    "        parts_t = self._extract_parts_dict(target_ctx, [\"nose\", \"ear_left\", \"ear_right\", \"body_center\", \"tail_base\", \"hip_left\", \"hip_right\", \"neck\"])\n",
    "    \n",
    "        def body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"nose\")\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if head is None or tail is None:\n",
    "                return None\n",
    "            return head - tail\n",
    "    \n",
    "        a_body = body_vec(parts_a)\n",
    "        t_body = body_vec(parts_t)\n",
    "    \n",
    "        if a_body is not None and t_body is not None:\n",
    "            dot_bt = np.sum(a_body * t_body, axis=1)\n",
    "            mag_bt = np.linalg.norm(a_body, axis=1) * np.linalg.norm(t_body, axis=1)\n",
    "            cos_body = np.clip(dot_bt / (mag_bt + 1e-6), -1.0, 1.0)\n",
    "            cos_body_s = pd.Series(cos_body, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            cos_body_s = zero()\n",
    "    \n",
    "        # Velocity hướng\n",
    "        a_vel = ctx.vel\n",
    "        t_vel = target_ctx.vel\n",
    "        a_speed_np = np.linalg.norm(a_vel, axis=1)\n",
    "        t_speed_np = np.linalg.norm(t_vel, axis=1)\n",
    "        moving_mask = (a_speed_np > 1e-3) & (t_speed_np > 1e-3)\n",
    "    \n",
    "        # cos giữa hướng velocity 2 con\n",
    "        dot_v = np.sum(a_vel * t_vel, axis=1)\n",
    "        mag_v = a_speed_np * t_speed_np + 1e-6\n",
    "        cos_vel = np.zeros_like(dot_v, dtype=\"float32\")\n",
    "        cos_vel[moving_mask] = np.clip(dot_v[moving_mask] / mag_v[moving_mask], -1.0, 1.0)\n",
    "        cos_vel_s = pd.Series(cos_vel, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 2. WINDOW NGẮN (FOLLOW LÀ PATTERN DÀI HƠN ATTACK) ---\n",
    "        for w30 in [15, 30, 60]:   # ~0.5s, 1s, 2s\n",
    "            ws = self._scale(w30)\n",
    "            min_p = max(ws // 3, 1)\n",
    "    \n",
    "            # Khoảng cách trung bình & độ dao động\n",
    "            m_dist = rel_dist_s.rolling(ws, min_periods=min_p).mean()\n",
    "            s_dist = rel_dist_s.rolling(ws, min_periods=min_p).std()\n",
    "    \n",
    "            # Cùng hướng (body + velocity)\n",
    "            m_cos_body = cos_body_s.rolling(ws, min_periods=min_p).mean()\n",
    "            m_cos_vel  = cos_vel_s.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            # Tốc độ vừa phải\n",
    "            m_sp_a = a_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            m_sp_t = t_speed.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            feats[f\"follow_dist_mean_{w30}\"] = m_dist\n",
    "            feats[f\"follow_dist_std_{w30}\"]  = s_dist\n",
    "            feats[f\"follow_cos_body_mean_{w30}\"] = m_cos_body\n",
    "            feats[f\"follow_cos_vel_mean_{w30}\"]  = m_cos_vel\n",
    "            feats[f\"follow_speed_agent_mean_{w30}\"] = m_sp_a\n",
    "            feats[f\"follow_speed_target_mean_{w30}\"] = m_sp_t\n",
    "    \n",
    "        # Clean\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "    \n",
    "    def _feat_shortburst_social(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        feats = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- Lấy lại vài quantity cơ bản từ pairwise/avoidance ---\n",
    "        # vector Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # unit vector\n",
    "        rel_dist_safe = np.where(rel_dist == 0, 1e-6, rel_dist)\n",
    "        u_vec = rel_vec / rel_dist_safe[:, None]\n",
    "    \n",
    "        # velocity dọc trục nối (approach speed)\n",
    "        a_vel = ctx.vel\n",
    "        t_vel = target_ctx.vel\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)                # +: lao vào target\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1)             # +: target lao vào agent\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)    # +: lại gần nhau\n",
    "    \n",
    "        a_along_s = pd.Series(a_along, index=idx, dtype=\"float32\")\n",
    "        t_along_s = pd.Series(t_along, index=idx, dtype=\"float32\")\n",
    "        rel_along_s = pd.Series(rel_along, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # speed agent / target\n",
    "        a_speed = ctx.speed_series\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\"\n",
    "        )\n",
    "    \n",
    "        # heading_rel_cos ~ escape / approach\n",
    "        # vector body của agent\n",
    "        # (reuse idea từ _feat_pairwise)\n",
    "        # head ~ nose, tail ~ tail_base/body_center\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\"])\n",
    "        head_a = parts_a.get(\"nose\")\n",
    "        tail_a = parts_a.get(\"tail_base\")\n",
    "    \n",
    "        if head_a is not None and tail_a is not None:\n",
    "            body_vec_a = head_a - tail_a\n",
    "            dot = np.sum(body_vec_a * rel_vec, axis=1)\n",
    "            mag = np.linalg.norm(body_vec_a, axis=1) * rel_dist_safe\n",
    "            heading_cos = np.clip(dot / (mag + 1e-6), -1.0, 1.0)\n",
    "            heading_cos_s = pd.Series(heading_cos, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            heading_cos_s = zero()\n",
    "    \n",
    "        # --- Rolling window 10, 20, 30 frames (ở fps gốc) ---\n",
    "        for w30 in [10, 20, 30]:\n",
    "            ws = self._scale(w30)\n",
    "            min_p = max(1, ws // 3)\n",
    "    \n",
    "            # Attack-like: approach mạnh, khoảng cách giảm nhanh\n",
    "            feats[f\"sb_att_approach_mean_{w30}\"] = a_along_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_att_rel_along_mean_{w30}\"] = rel_along_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_att_dist_delta_{w30}\"] = (rel_dist_s - rel_dist_s.shift(ws)).fillna(0.0)\n",
    "    \n",
    "            # Chase-like: agent & target đều nhanh, dist tương đối nhỏ\n",
    "            feats[f\"sb_chase_speed_agent_mean_{w30}\"] = a_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_chase_speed_target_mean_{w30}\"] = t_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_chase_dist_mean_{w30}\"] = rel_dist_s.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            # Escape-like: heading ngược, dist tăng nhanh\n",
    "            feats[f\"sb_esc_heading_cos_mean_{w30}\"] = heading_cos_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_esc_dist_gain_{w30}\"] = (rel_dist_s.shift(-ws) - rel_dist_s).fillna(0.0)\n",
    "    \n",
    "        # clip & fillna\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = v.replace([np.inf, -np.inf], np.nan).fillna(0.0).astype(\"float32\")\n",
    "    \n",
    "        return feats\n",
    "\n",
    "    \n",
    "    def build_pose_tensor(self, tracking: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Chuyển dữ liệu tracking (DataFrame) sang Tensor [Frames, Mice, 2] và Dict chi tiết.\n",
    "        \"\"\"\n",
    "        tracking = tracking.sort_values(\"video_frame\")\n",
    "        frames = np.sort(tracking[\"video_frame\"].unique())\n",
    "        \n",
    "        pvid = tracking.pivot(\n",
    "            index=\"video_frame\", \n",
    "            columns=[\"mouse_id\", \"bodypart\"], \n",
    "            values=[\"x\", \"y\"]\n",
    "        )\n",
    "        pvid = pvid.reorder_levels([1, 2, 0], axis=1).sort_index(axis=1).astype(\"float32\")\n",
    "        mouse_ids = list(pvid.columns.get_level_values(0).unique())\n",
    "        pos = np.full((len(frames), len(mouse_ids), 2), np.nan, dtype=np.float32)\n",
    "        per_mouse_df = {}\n",
    "        \n",
    "        for i, mid in enumerate(mouse_ids):\n",
    "            single = pvid[mid]\n",
    "            per_mouse_df[mid] = single\n",
    "            \n",
    "            if \"body_center\" in single.columns.get_level_values(0):\n",
    "                cx = single[\"body_center\"][\"x\"]\n",
    "                cy = single[\"body_center\"][\"y\"]\n",
    "            else:\n",
    "                cx = single.xs(\"x\", level=1, axis=1).mean(axis=1)\n",
    "                cy = single.xs(\"y\", level=1, axis=1).mean(axis=1)\n",
    "            \n",
    "            pos[:, i, 0] = cx.reindex(frames).values\n",
    "            pos[:, i, 1] = cy.reindex(frames).values\n",
    "            \n",
    "        return frames, mouse_ids, pos, per_mouse_df\n",
    "\n",
    "    def extract_agent_target(\n",
    "        self, \n",
    "        frames: np.ndarray, \n",
    "        mouse_ids: List[Any], \n",
    "        pos: np.ndarray, \n",
    "        agent_id: Any, \n",
    "        target_id: Any, \n",
    "        per_mouse_df: Dict = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Trích xuất đặc trưng cho cặp (Agent, Target).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            aid_idx = mouse_ids.index(agent_id)\n",
    "        except ValueError:\n",
    "            return pd.DataFrame() \n",
    "\n",
    "        # 1. Build Agent Context\n",
    "        ctx_agent = self._build_context(\n",
    "            frames, \n",
    "            pos[:, aid_idx, :], \n",
    "            per_mouse_df.get(agent_id) if per_mouse_df else None\n",
    "        )\n",
    "\n",
    "        # 2. Build Target Context\n",
    "        ctx_target = None\n",
    "        if self.cfg.use_pairwise and target_id is not None and target_id in mouse_ids:\n",
    "             tid_idx = mouse_ids.index(target_id)\n",
    "             ctx_target = self._build_context(\n",
    "                 frames, \n",
    "                 pos[:, tid_idx, :], \n",
    "                 per_mouse_df.get(target_id) if per_mouse_df else None\n",
    "             )\n",
    "\n",
    "        # 3. Run all features\n",
    "        all_data = {}\n",
    "        for func_name, func in self.feature_registry.items():\n",
    "            out_dict = func(ctx_agent, target_ctx=ctx_target)\n",
    "            all_data.update(out_dict)\n",
    "\n",
    "        df_out = pd.DataFrame(all_data, index=ctx_agent.idx)\n",
    "        df_out = df_out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        \n",
    "        return df_out.reindex(sorted(df_out.columns), axis=1)\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# === IMPORT MODEL & OPTUNA ===\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "# Cấu hình\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "np.seterr(invalid=\"ignore\", divide=\"ignore\")\n",
    "\n",
    "# Metric\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/usr/lib/mabe-f-beta\")\n",
    "try:\n",
    "    from metric import score\n",
    "except ImportError:\n",
    "    def score(*args, **kwargs): return 0.0\n",
    "\n",
    "# =========================================================\n",
    "# 1. CẤU HÌNH & SEED\n",
    "# =========================================================\n",
    "SEED = 42\n",
    "def seed_everything(seed=42):\n",
    "    np.random.seed(seed)\n",
    "seed_everything(SEED)\n",
    "\n",
    "INPUT_DIR = Path(\"/kaggle/input/MABe-mouse-behavior-detection\")\n",
    "TRAIN_TRACKING_DIR = INPUT_DIR / \"train_tracking\"\n",
    "TRAIN_ANNOTATION_DIR = INPUT_DIR / \"train_annotation\"\n",
    "TEST_TRACKING_DIR = INPUT_DIR / \"test_tracking\"\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "RESULTS_DIR = Path(r\"/kaggle/input/results-ensemble-optuna3\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "SELF_BEHAVIORS = [\"biteobject\", \"climb\", \"dig\", \"exploreobject\", \"freeze\", \"genitalgroom\", \"huddle\", \"rear\", \"rest\", \"run\", \"selfgroom\"]\n",
    "PAIR_BEHAVIORS = [\"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\", \"chase\", \"chaseattack\", \"defend\", \"disengage\", \"dominance\", \"dominancegroom\", \"dominancemount\", \"ejaculate\", \"escape\", \"flinch\", \"follow\", \"intromit\", \"mount\", \"reciprocalsniff\", \"shepherd\", \"sniff\", \"sniffbody\", \"sniffface\", \"sniffgenital\", \"submit\", \"tussle\"]\n",
    "BAD_VIDEOS = []\n",
    "\n",
    "# =========================================================\n",
    "# 2. DATA LOADING & PREPARATION (NO CACHE)\n",
    "# =========================================================\n",
    "\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    return pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "\n",
    "def get_video_params(video_id: Any, meta: pd.DataFrame) -> Tuple[float, float]:\n",
    "    row = meta.loc[meta[\"video_id\"] == video_id]\n",
    "    if row.empty: return 30.0, 1.0\n",
    "    row = row.iloc[0]\n",
    "    return float(row[\"frames_per_second\"]), float(row[\"pix_per_cm_approx\"])\n",
    "\n",
    "def load_tracking(lab_id: str, video_id: Any, is_test=False) -> pd.DataFrame:\n",
    "    d = TEST_TRACKING_DIR if is_test else TRAIN_TRACKING_DIR\n",
    "    path = d / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists(): raise FileNotFoundError(path)\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def load_annotation(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    path = TRAIN_ANNOTATION_DIR / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists(): return pd.DataFrame(columns=[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "    return pd.read_parquet(path)[[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]]\n",
    "\n",
    "# Hàm lấy feature KHÔNG CACHE để tránh tràn RAM\n",
    "def get_frame_features_no_cache(lab_id, video_id, agent_id, target_id, meta, is_test=False):\n",
    "    if is_test:\n",
    "        row = meta[meta[\"video_id\"] == video_id].iloc[0]\n",
    "        fps, pix = float(row[\"frames_per_second\"]), float(row[\"pix_per_cm_approx\"])\n",
    "        pix = pix if np.isfinite(pix) and pix > 0 else 1.0\n",
    "    else:\n",
    "        fps, pix = get_video_params(video_id, meta)\n",
    "\n",
    "    tracking = load_tracking(lab_id, video_id, is_test)\n",
    "    \n",
    "    # === GỌI CLASS FeatureExtractor (Đã có ở cell trước) ===\n",
    "    fe = FeatureExtractor(fps=fps, pix_per_cm=pix, smooth_sigma=1.0, use_pairwise=True)\n",
    "    \n",
    "    frames, mouse_ids, pos, per_mouse_df = fe.build_pose_tensor(tracking)\n",
    "    \n",
    "    features_df = fe.extract_agent_target(\n",
    "        frames=frames, mouse_ids=mouse_ids, pos=pos,\n",
    "        agent_id=agent_id, target_id=target_id, per_mouse_df=per_mouse_df\n",
    "    )\n",
    "    features_df.index = frames\n",
    "    return frames, features_df\n",
    "\n",
    "def build_frame_dataset_for_lab_behavior(lab_id, behavior, train_meta, mode=\"self\"):\n",
    "    videos = train_meta[train_meta[\"lab_id\"] == lab_id][\"video_id\"].unique().tolist()\n",
    "    index_list, feature_list, label_list = [], [], []\n",
    "\n",
    "    for video_id in videos:\n",
    "        ann = load_annotation(lab_id, video_id)\n",
    "        if ann.empty: continue\n",
    "        \n",
    "        ann_bhv = ann[ann[\"action\"] == behavior]\n",
    "        if ann_bhv.empty: continue\n",
    "\n",
    "        pairs = ann_bhv[[\"agent_id\", \"target_id\"]].drop_duplicates().values.tolist()\n",
    "        for (agent_id, target_id) in pairs:\n",
    "            target_id_use = agent_id if mode == \"self\" else target_id\n",
    "            \n",
    "            # Lấy features (tính trực tiếp)\n",
    "            frames, feat_df = get_frame_features_no_cache(lab_id, video_id, agent_id, target_id_use, train_meta)\n",
    "\n",
    "            ann_pair = ann_bhv[(ann_bhv[\"agent_id\"] == agent_id) & (ann_bhv[\"target_id\"] == target_id)]\n",
    "            if ann_pair.empty and mode == \"self\": ann_pair = ann_bhv[ann_bhv[\"agent_id\"] == agent_id]\n",
    "\n",
    "            pos_frames = set()\n",
    "            for _, r in ann_pair.iterrows(): pos_frames.update(range(int(r[\"start_frame\"]), int(r[\"stop_frame\"])))\n",
    "            \n",
    "            if not pos_frames: continue\n",
    "            label = np.isin(frames, list(pos_frames)).astype(\"int8\")\n",
    "            if label.sum() == 0: continue\n",
    "\n",
    "            # Lưu vào list và reset index ngay để giảm memory overhead\n",
    "            index_list.append(pd.DataFrame({\"video_id\": video_id, \"agent_id\": agent_id, \"target_id\": target_id, \"video_frame\": frames}))\n",
    "            feature_list.append(feat_df.reset_index(drop=True))\n",
    "            label_list.append(label)\n",
    "            \n",
    "            # Dọn dẹp ngay\n",
    "            del frames, feat_df, label\n",
    "\n",
    "    if not index_list: return pd.DataFrame(), pd.DataFrame(), np.zeros(0, dtype=\"int8\")\n",
    "    \n",
    "    return pd.concat(index_list, ignore_index=True), pd.concat(feature_list, ignore_index=True), np.concatenate(label_list).astype(\"int8\")\n",
    "\n",
    "# =========================================================\n",
    "# 3. TRAINING & ENSEMBLE HELPERS\n",
    "# =========================================================\n",
    "\n",
    "def train_catboost_fold(X_tr, y_tr, X_va, y_va, sw=1.0):\n",
    "    p = {\n",
    "        'iterations': 1000, 'learning_rate': 0.05, 'depth': 6, 'scale_pos_weight': sw,\n",
    "        'task_type': 'GPU', 'devices': '0', 'verbose': 0, 'allow_writing_files': False,\n",
    "        'l2_leaf_reg': 5, 'bootstrap_type': 'Bernoulli', 'subsample': 0.8, 'random_seed': SEED\n",
    "    }\n",
    "    m = cb.CatBoostClassifier(**p)\n",
    "    m.fit(cb.Pool(X_tr, y_tr), eval_set=cb.Pool(X_va, y_va), early_stopping_rounds=20, use_best_model=True)\n",
    "    return m\n",
    "\n",
    "def train_lightgbm_fold(X_tr, y_tr, X_va, y_va, sw=1.0):\n",
    "    p = {\n",
    "        'objective': 'binary', 'metric': 'binary_logloss', 'learning_rate': 0.05,\n",
    "        'max_depth': 6, 'num_leaves': 31, 'scale_pos_weight': sw, 'device': 'gpu',\n",
    "        'verbosity': -1, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8,\n",
    "        'subsample_freq': 1, 'seed': SEED\n",
    "    }\n",
    "    m = lgb.train(p, lgb.Dataset(X_tr, y_tr), 1000, valid_sets=[lgb.Dataset(X_va, y_va)], callbacks=[lgb.early_stopping(20, verbose=False)])\n",
    "    return m\n",
    "\n",
    "def optimize_ensemble_weights(oof_dict, y_true):\n",
    "    models = list(oof_dict.keys())\n",
    "    def obj(trial):\n",
    "        w = [trial.suggest_float(m, 0.0, 1.0) for m in models]\n",
    "        s = sum(w) + 1e-6; w = [x/s for x in w]\n",
    "        p = np.zeros_like(y_true, dtype=float)\n",
    "        for i, m in enumerate(models): p += oof_dict[m] * w[i]\n",
    "        th = trial.suggest_float(\"th\", 0.1, 0.9)\n",
    "        return f1_score(y_true, (p >= th).astype(int), zero_division=0)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "    study.optimize(obj, n_trials=50)\n",
    "    best = study.best_params\n",
    "    th = best.pop(\"th\")\n",
    "    rw = [best[m] for m in models]; s = sum(rw)+1e-6\n",
    "    return {m: w/s for m, w in zip(models, rw)}, th\n",
    "\n",
    "def train_validate_ensemble(lab_id, behavior, indices, features, labels):\n",
    "    res_dir = RESULTS_DIR / lab_id / behavior\n",
    "    res_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if len(labels) == 0 or labels.sum() == 0: return 0.0\n",
    "\n",
    "    X = features.values.astype(\"float32\")\n",
    "    y = labels.astype(\"int8\")\n",
    "    groups = indices[\"video_id\"].values\n",
    "    \n",
    "    oof_preds = {m: np.zeros(len(y), dtype=\"float32\") for m in [\"xgb\", \"cat\", \"lgb\"]}\n",
    "    folds = np.ones(len(y), dtype=\"int8\") * -1\n",
    "\n",
    "    cv = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "    for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups=groups)):\n",
    "        print(f\"   Fold {fold}...\", end=\" \")\n",
    "        fd_dir = res_dir / f\"fold_{fold}\"; fd_dir.mkdir(parents=True, exist_ok=True)\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]; X_va, y_va = X[va_idx], y[va_idx]\n",
    "        pos = y_tr.sum(); neg = len(y_tr) - pos\n",
    "        sw = float(neg/pos) if pos > 0 else 1.0\n",
    "\n",
    "        # 1. XGBoost\n",
    "        dtr = xgb.QuantileDMatrix(X_tr, label=y_tr, feature_names=features.columns.tolist(), max_bin=64)\n",
    "        dva = xgb.DMatrix(X_va, label=y_va, feature_names=features.columns.tolist())\n",
    "        xp = {\n",
    "            \"objective\":\"binary:logistic\", \"eval_metric\":\"logloss\", \"device\":\"cuda\", \n",
    "            \"tree_method\":\"hist\", \"learning_rate\":0.05, \"max_depth\":6, \"scale_pos_weight\":sw,\n",
    "            \"min_child_weight\":5, \"subsample\":0.8, \"colsample_bytree\":0.8, \"max_bin\":64, \"seed\": SEED\n",
    "        }\n",
    "        \n",
    "        # === ĐÃ THÊM 'evals=' VÀO DÒNG DƯỚI ===\n",
    "        mx = xgb.train(\n",
    "            params=xp, \n",
    "            dtrain=dtr, \n",
    "            num_boost_round=1000, \n",
    "            evals=[(dva, \"valid\")],\n",
    "            callbacks=[xgb.callback.EarlyStopping(rounds=20, save_best=True)], \n",
    "            verbose_eval=False\n",
    "        )\n",
    "        mx.save_model(fd_dir / \"model_xgb.json\")\n",
    "        oof_preds[\"xgb\"][va_idx] = mx.predict(dva)\n",
    "\n",
    "        # 2. CatBoost\n",
    "        mc = train_catboost_fold(X_tr, y_tr, X_va, y_va, sw)\n",
    "        mc.save_model(str(fd_dir / \"model_cat.cbm\"))\n",
    "        oof_preds[\"cat\"][va_idx] = mc.predict_proba(X_va)[:,1]\n",
    "\n",
    "        # 3. LightGBM\n",
    "        ml = train_lightgbm_fold(X_tr, y_tr, X_va, y_va, sw)\n",
    "        ml.save_model(fd_dir / \"model_lgb.txt\")\n",
    "        oof_preds[\"lgb\"][va_idx] = ml.predict(X_va)\n",
    "        folds[va_idx] = fold\n",
    "        \n",
    "        print(\"Done.\")\n",
    "        del X_tr, y_tr, X_va, y_va, dtr, dva, mx, mc, ml\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"   Optimizing Weights...\", end=\" \")\n",
    "    weights, th = optimize_ensemble_weights(oof_preds, y)\n",
    "    with open(res_dir / \"ensemble_params.json\", \"w\") as f: json.dump({\"weights\": weights, \"threshold\": th}, f)\n",
    "    \n",
    "    final_pred = sum(oof_preds[m] * weights[m] for m in weights)\n",
    "    final_lbl = (final_pred >= th).astype(\"int8\")\n",
    "    \n",
    "    # Save OOF\n",
    "    df = indices.copy(); df[\"fold\"] = folds; df[\"pred\"] = final_pred; df[\"lbl\"] = final_lbl\n",
    "    df.to_parquet(res_dir / \"oof.parquet\", index=False)\n",
    "    \n",
    "    f1 = f1_score(y, final_lbl, zero_division=0)\n",
    "    print(f\"Best F1: {f1:.4f} (Th={th:.2f}, W={weights})\")\n",
    "    (res_dir / \"f1.txt\").write_text(f\"{f1:.6f}\")\n",
    "    return float(f1)\n",
    "\n",
    "# =========================================================\n",
    "# 4. INFERENCE\n",
    "# =========================================================\n",
    "\n",
    "def load_ensemble_models(lab_id, behavior):\n",
    "    base = RESULTS_DIR / lab_id / behavior\n",
    "    if not base.exists(): return []\n",
    "    models = []\n",
    "    for fd in sorted(base.glob(\"fold_*\")):\n",
    "        if not (fd / \"model_xgb.json\").exists(): continue\n",
    "        \n",
    "        xgb_b = xgb.Booster(); xgb_b.load_model(str(fd / \"model_xgb.json\"))\n",
    "        cat_m = cb.CatBoostClassifier(); \n",
    "        try: cat_m.load_model(str(fd / \"model_cat.cbm\"))\n",
    "        except: cat_m = None\n",
    "        try: lgb_m = lgb.Booster(model_file=str(fd / \"model_lgb.txt\"))\n",
    "        except: lgb_m = None\n",
    "        models.append({\"xgb\": xgb_b, \"cat\": cat_m, \"lgb\": lgb_m})\n",
    "    return models\n",
    "\n",
    "def predict_behaviors_for_pair(lab_id, video_id, aid, tid, behaviors, test_meta):\n",
    "    if lab_id != \"TranquilPanther\": return None\n",
    "    frames, feat_df = get_frame_features_no_cache(lab_id, video_id, aid, tid, test_meta, is_test=True)\n",
    "    if feat_df.empty: return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "    \n",
    "    scores = {}\n",
    "    for bhv in behaviors:\n",
    "        base = RESULTS_DIR / lab_id / bhv\n",
    "        if not (base / \"ensemble_params.json\").exists(): continue\n",
    "        with open(base / \"ensemble_params.json\") as f: p = json.load(f)\n",
    "        ws, th = p[\"weights\"], p[\"threshold\"]\n",
    "        \n",
    "        folds = load_ensemble_models(lab_id, bhv)\n",
    "        if not folds: continue\n",
    "        \n",
    "        cols = folds[0][\"xgb\"].feature_names\n",
    "        X = pd.DataFrame(0.0, index=feat_df.index, columns=cols, dtype=np.float32)\n",
    "        c = list(set(cols) & set(feat_df.columns))\n",
    "        if c: X[c] = feat_df[c]\n",
    "        dtest = xgb.DMatrix(X, feature_names=cols)\n",
    "        \n",
    "        agg = np.zeros(len(feat_df), dtype=np.float32)\n",
    "        for m in folds:\n",
    "            px = m[\"xgb\"].predict(dtest)\n",
    "            pc = m[\"cat\"].predict_proba(X)[:,1] if m[\"cat\"] else np.zeros_like(px)\n",
    "            pl = m[\"lgb\"].predict(X) if m[\"lgb\"] else np.zeros_like(px)\n",
    "            \n",
    "            avg = px*ws.get(\"xgb\", 0.33) + pc*ws.get(\"cat\", 0.33) + pl*ws.get(\"lgb\", 0.33)\n",
    "            agg += avg * (avg >= th).astype(\"int8\")\n",
    "        \n",
    "        if folds: scores[bhv] = agg / len(folds)\n",
    "        \n",
    "        del X, dtest\n",
    "        gc.collect()\n",
    "\n",
    "    if not scores: return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "    \n",
    "    bl = list(scores.keys()); mat = np.vstack([scores[b] for b in bl]).T\n",
    "    lbls = np.where(mat.max(1)==0, \"none\", np.array(bl)[mat.argmax(1)])\n",
    "    \n",
    "    segs = []; prev = \"none\"; start = None; pf = None\n",
    "    for f, l in zip(frames, lbls):\n",
    "        if l != prev:\n",
    "            if prev != \"none\": segs.append({\"video_id\": int(video_id), \"action\": prev, \"start_frame\": int(start), \"stop_frame\": int(pf)+1})\n",
    "            prev = l; start = f\n",
    "        pf = f\n",
    "    if prev != \"none\": segs.append({\"video_id\": int(video_id), \"action\": prev, \"start_frame\": int(start), \"stop_frame\": int(pf)+1})\n",
    "    \n",
    "    return pd.DataFrame(segs)\n",
    "\n",
    "# =========================================================\n",
    "# 5. MAIN\n",
    "# =========================================================\n",
    "target_lab = \"TranquilPanther\"\n",
    "print(\"\\n=== START INFERENCE ===\")\n",
    "test_meta = pd.read_csv(INPUT_DIR / \"test.csv\")\n",
    "test_meta = test_meta[test_meta[\"lab_id\"] == target_lab].reset_index(drop=True)\n",
    "\n",
    "trained = sorted([p.name for p in (RESULTS_DIR/target_lab).iterdir() if p.is_dir()])\n",
    "sb, pb = [b for b in trained if b in SELF_BEHAVIORS], [b for b in trained if b in PAIR_BEHAVIORS]\n",
    "\n",
    "all_segs = []\n",
    "def fid(i): return str(i) if str(i).startswith(\"mouse\") else f\"mouse{i}\"\n",
    "\n",
    "for vid in sorted(test_meta[\"video_id\"].unique()):\n",
    "    print(f\"Predicting Video {vid}...\")\n",
    "    tr = load_tracking(target_lab, vid, is_test=True)\n",
    "    mids = sorted(tr[\"mouse_id\"].unique())\n",
    "    \n",
    "    if sb:\n",
    "        for m in mids:\n",
    "            df = predict_behaviors_for_pair(target_lab, vid, m, m, sb, test_meta)\n",
    "            if df is not None and not df.empty:\n",
    "                df[\"agent_id\"] = fid(m); df[\"target_id\"] = \"self\"\n",
    "                all_segs.append(df)\n",
    "    if pb and len(mids) > 1:\n",
    "        for a, t in itertools.permutations(mids, 2):\n",
    "            df = predict_behaviors_for_pair(target_lab, vid, a, t, pb, test_meta)\n",
    "            if df is not None and not df.empty:\n",
    "                df[\"agent_id\"] = fid(a); df[\"target_id\"] = fid(t)\n",
    "                all_segs.append(df)\n",
    "    del tr\n",
    "    gc.collect()\n",
    "\n",
    "cols = [\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "\n",
    "if all_segs:\n",
    "    sub4 = pd.concat(all_segs, ignore_index=True)\n",
    "    sub4 = sub4[cols].sort_values([\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\"]).reset_index(drop=True)    \n",
    "    sub4.insert(0, \"row_id\", np.arange(len(sub4), dtype=np.int64))\n",
    "else:\n",
    "    sub4 = pd.DataFrame(columns=[\"row_id\"] + cols)\n",
    "\n",
    "sub4.to_csv(WORKING_DIR / \"submission8.csv\", index=False)\n",
    "print(f\"\\nDone! Saved submission to {WORKING_DIR / 'submission8.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5e4ce",
   "metadata": {
    "papermill": {
     "duration": 0.018339,
     "end_time": "2025-12-13T17:39:18.810259",
     "exception": false,
     "start_time": "2025-12-13T17:39:18.791920",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NiftyGoldfinch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19ef25e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:18.849136Z",
     "iopub.status.busy": "2025-12-13T17:39:18.848799Z",
     "iopub.status.idle": "2025-12-13T17:39:18.968101Z",
     "shell.execute_reply": "2025-12-13T17:39:18.967284Z"
    },
    "papermill": {
     "duration": 0.139891,
     "end_time": "2025-12-13T17:39:18.969511",
     "exception": false,
     "start_time": "2025-12-13T17:39:18.829620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "\n",
    "# 1) Xóa mọi thứ trong /kaggle/working trừ .csv\n",
    "for path in WORKING_DIR.iterdir():\n",
    "    # giữ lại file .csv\n",
    "    if path.is_file() and path.suffix == \".csv\":\n",
    "        continue\n",
    "\n",
    "    if path.is_file():\n",
    "        try:\n",
    "            path.unlink()\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove file {path}: {e}\")\n",
    "    elif path.is_dir():\n",
    "        try:\n",
    "            shutil.rmtree(path, ignore_errors=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove dir {path}: {e}\")\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba78ee25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:19.011851Z",
     "iopub.status.busy": "2025-12-13T17:39:19.011601Z",
     "iopub.status.idle": "2025-12-13T17:39:19.149500Z",
     "shell.execute_reply": "2025-12-13T17:39:19.148701Z"
    },
    "papermill": {
     "duration": 0.160838,
     "end_time": "2025-12-13T17:39:19.150742",
     "exception": false,
     "start_time": "2025-12-13T17:39:18.989904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== START INFERENCE ===\n",
      "\n",
      "Done! Saved submission to /kaggle/working/submission9.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "np.seterr(invalid=\"ignore\", divide=\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    \"\"\"\n",
    "    Chứa cấu hình tham số (Hyperparameters).\n",
    "    \"\"\"\n",
    "    fps: float = 30.0\n",
    "    pix_per_cm: float = 1.0\n",
    "    smooth_sigma: float = 1.0\n",
    "    use_pairwise: bool = True\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. AGENT CONTEXT\n",
    "# =============================================================================\n",
    "@dataclass\n",
    "class AgentContext:\n",
    "    \"\"\"\n",
    "    Container chứa dữ liệu đã tiền xử lý của một con chuột.\n",
    "    Giúp tránh việc tính toán lại vận tốc/gia tốc nhiều lần.\n",
    "    \"\"\"\n",
    "    idx: pd.Index          # Index frame\n",
    "    pos: np.ndarray        # [F, 2] cm\n",
    "    vel: np.ndarray        # [F, 2] cm/s\n",
    "    speed: np.ndarray      # [F, 1] cm/s\n",
    "    acc: np.ndarray        # [F, 2] cm/s^2\n",
    "    \n",
    "    cx: pd.Series          # Series tọa độ X (để dùng rolling)\n",
    "    cy: pd.Series          # Series tọa độ Y\n",
    "    speed_series: pd.Series # Series tốc độ\n",
    "    \n",
    "    raw_df: Optional[pd.DataFrame] = None # Dữ liệu gốc các bộ phận \n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. FEATURE EXTRACTOR\n",
    "# =============================================================================\n",
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Class trích xuất đặc trưng hành vi từ dữ liệu tracking.\n",
    "    \"\"\"\n",
    "    def __init__(self, fps: float, pix_per_cm: float, smooth_sigma: float = 1.0, use_pairwise: bool = True):\n",
    "        # Map tham số từ init vào Config\n",
    "        self.cfg = FeatureConfig(\n",
    "            fps=float(fps), \n",
    "            pix_per_cm=float(pix_per_cm), \n",
    "            smooth_sigma=smooth_sigma,\n",
    "            use_pairwise=use_pairwise\n",
    "        )\n",
    "        \n",
    "        # Đăng ký các hàm feature sẽ chạy\n",
    "        self.feature_registry = {\n",
    "            \"kinematics\": self._feat_basic_kinematics,\n",
    "            \"multiscale\": self._feat_multiscale,\n",
    "            \"long_range\": self._feat_long_range,\n",
    "            \"cumulative\": self._feat_cumulative,\n",
    "            \"curvature\": self._feat_curvature,\n",
    "            \"speed_asym\": self._feat_speed_asym,\n",
    "            \"gauss_shift\": self._feat_gauss_shift,\n",
    "            \"pose_shape\": self._feat_pose_shape,\n",
    "            \"pairwise\": self._feat_pairwise,\n",
    "            \"follow\": self._feat_follow_pattern,\n",
    "            \"short\": self._feat_shortburst_social,\n",
    "            \"a\": self._feat_attack_sniff,\n",
    "            \"b\": self._feat_climb\n",
    "        }\n",
    "\n",
    "    # --- Helpers ---\n",
    "    def _scale(self, n_frames_30fps: int) -> int:\n",
    "        \"\"\"Quy đổi số frame từ chuẩn 30fps sang fps thực tế của video.\"\"\"\n",
    "        return max(1, int(round(n_frames_30fps * self.cfg.fps / 30.0)))\n",
    "\n",
    "    def _to_cm(self, arr):\n",
    "        \"\"\"Chuyển pixel -> cm.\"\"\"\n",
    "        return arr / self.cfg.pix_per_cm\n",
    "\n",
    "    def _smooth(self, x):\n",
    "        \"\"\"Làm mượt dữ liệu bằng Gaussian filter.\"\"\"\n",
    "        if self.cfg.smooth_sigma is None or x.shape[0] < 3: return x\n",
    "        if np.all(np.isnan(x)): return x\n",
    "        return gaussian_filter1d(x, sigma=self.cfg.smooth_sigma, axis=0, mode=\"nearest\")\n",
    "\n",
    "    def _forward_fill_nan(self, pos):\n",
    "        \"\"\"\n",
    "        Điền dữ liệu thiếu (NaN) bằng giá trị hợp lệ trước đó (Forward Fill).\n",
    "        \"\"\"\n",
    "        if np.all(np.isnan(pos)):\n",
    "            return np.zeros_like(pos)\n",
    "\n",
    "        pos_ffill = pos.copy()\n",
    "        mask = np.any(~np.isnan(pos_ffill), axis=1)\n",
    "        if not mask.any():\n",
    "            return np.zeros_like(pos_ffill)\n",
    "\n",
    "        valid_idx = np.where(mask)[0]\n",
    "        first, last = valid_idx[0], valid_idx[-1]\n",
    "        pos_ffill[:first] = pos_ffill[first]\n",
    "        pos_ffill[last + 1:] = pos_ffill[last]\n",
    "        df_temp = pd.DataFrame(pos_ffill)\n",
    "        df_temp = df_temp.ffill()\n",
    "        return df_temp.to_numpy()\n",
    "    \n",
    "    def _speed_series(self, cx: pd.Series, cy: pd.Series) -> pd.Series:\n",
    "        dx = cx.diff()\n",
    "        dy = cy.diff()\n",
    "        v = np.hypot(dx, dy).fillna(0.0) * self.cfg.fps\n",
    "        return v.astype(\"float32\")\n",
    "    \n",
    "    def _roll_future_mean(self, s: pd.Series, w: int, min_p: int = 1) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).mean().iloc[::-1]\n",
    "\n",
    "    def _roll_future_var(self, s: pd.Series, w: int, min_p: int = 2) -> pd.Series:\n",
    "        return s.iloc[::-1].rolling(w, min_periods=min_p).var().iloc[::-1]\n",
    "\n",
    "    # --- Core Logic ---\n",
    "    def _compute_kinematics(self, pos_px: np.ndarray):\n",
    "        \"\"\"\n",
    "        Tính toán vật lý cơ bản: Pos(cm), Vel, Speed, Acc.\n",
    "        Input: Array [Frames, 2] (pixel).\n",
    "        Output: Tuple (pos_cm, vel, speed, acc).\n",
    "        \"\"\"\n",
    "        pos_ffill = self._forward_fill_nan(pos_px)\n",
    "        pos_cm = self._to_cm(pos_ffill.astype(np.float32))\n",
    "        pos_cm = self._smooth(pos_cm)                                               # [F, 2]\n",
    "\n",
    "        dt = 1.0 / self.cfg.fps\n",
    "        vel = np.zeros_like(pos_cm, dtype=np.float32)\n",
    "        vel[1:] = (pos_cm[1:] - pos_cm[:-1]) / dt                                   # [F, 2: (vx, vy)]\n",
    "        speed = np.linalg.norm(vel, axis=1, keepdims=True).astype(np.float32)       # [F, 1]\n",
    "\n",
    "        acc = np.zeros_like(pos_cm, dtype=np.float32)                          \n",
    "        acc[1:] = (vel[1:] - vel[:-1]) / dt                                         # [F, 2:(ax, ay)]\n",
    "        return pos_cm.astype(np.float32), vel, speed, acc\n",
    "\n",
    "    def _build_context(self, frames, pos_px, mouse_df=None) -> AgentContext:\n",
    "        \"\"\"\n",
    "        Tạo AgentContext chứa đầy đủ thông tin vật lý của 1 con chuột.\n",
    "        \"\"\"\n",
    "        p, v, s, a = self._compute_kinematics(pos_px)\n",
    "        idx = pd.Index(frames, name=\"frame\")\n",
    "        \n",
    "        return AgentContext(\n",
    "            idx=idx, pos=p, vel=v, speed=s, acc=a, \n",
    "            cx=pd.Series(p[:, 0], index=idx), \n",
    "            cy=pd.Series(p[:, 1], index=idx), \n",
    "            speed_series=pd.Series(s[:, 0], index=idx), \n",
    "            raw_df=mouse_df\n",
    "        )\n",
    "\n",
    "    # --- Feature Modules ---\n",
    "    def _feat_basic_kinematics(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Lấy các giá trị thô: tọa độ x, y, vận tốc vx, vy, tốc độ, gia tốc ax, ay.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"a_x\": ctx.pos[:, 0], \"a_y\": ctx.pos[:, 1],\n",
    "            \"a_vx\": ctx.vel[:, 0], \"a_vy\": ctx.vel[:, 1],\n",
    "            \"a_speed\": ctx.speed[:, 0],\n",
    "            \"a_ax\": ctx.acc[:, 0], \"a_ay\": ctx.acc[:, 1]\n",
    "        }\n",
    "\n",
    "    def _feat_multiscale(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tính tốc độ trung bình (Mean) và độ lệch chuẩn (Std) ở đa mức thời gian.\n",
    "        Feature 'sp_ratio' đo độ bùng nổ (Burstiness).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        speed = ctx.speed_series\n",
    "        frame_scales = [10, 40, 160]\n",
    "        for scale in frame_scales:\n",
    "            ws = self._scale(scale)\n",
    "            if len(speed) >= ws:\n",
    "                roller = speed.rolling(ws, min_periods=max(1, ws//4), center=True)\n",
    "                feats[f\"sp_m{scale}\"] = roller.mean().astype(\"float32\")\n",
    "                feats[f\"sp_s{scale}\"] = roller.std().astype(\"float32\")\n",
    "        feats[f\"sp_ratio\"] = feats[\"sp_m10\"] / (feats[\"sp_m160\"] + 1e-6)\n",
    "        return feats \n",
    "        \n",
    "    def _feat_long_range(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng ngữ cảnh dài hạn:\n",
    "        - x_ml, y_ml: Vị trí trung bình trong quá khứ.\n",
    "        - sp_pct: Xếp hạng (percentile) của tốc độ hiện tại so với quá khứ.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        speed = ctx.speed_series\n",
    "\n",
    "        for window in [120, 240]:\n",
    "            ws = self._scale(window)\n",
    "            if len(ctx.cx) >= ws:\n",
    "                feats[f\"x_ml{window}\"] = ctx.cx.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "                feats[f\"y_ml{window}\"] = ctx.cy.rolling(ws, min_periods=max(5, ws // 6), center=True).mean()\n",
    "\n",
    "        for span in [60, 120]:\n",
    "            s = self._scale(span)\n",
    "            feats[f\"x_e{span}\"] = ctx.cx.ewm(span=s, min_periods=1).mean()\n",
    "            feats[f\"y_e{span}\"] = ctx.cy.ewm(span=s, min_periods=1).mean()\n",
    "\n",
    "        for window in [60, 120]:\n",
    "            ws = self._scale(window)\n",
    "            if len(speed) >= ws:\n",
    "                feats[f\"sp_pct{window}\"] = speed.rolling(\n",
    "                    ws, min_periods=max(5, ws // 6), center=True\n",
    "                ).rank(pct=True)\n",
    "        return feats\n",
    "    \n",
    "\n",
    "    def _feat_curvature(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        feats = {}\n",
    "\n",
    "        vel_x, vel_y = ctx.vel[:, 0], ctx.vel[:, 1]\n",
    "        acc_x, acc_y = ctx.acc[:, 0], ctx.acc[:, 1]\n",
    "        cross_prod = vel_x * acc_y - vel_y * acc_x\n",
    "        vel_mag = np.sqrt(vel_x**2 + vel_y**2)\n",
    "        moving_mask = vel_mag > 2.0\n",
    "        vel_mag_safe = np.maximum(vel_mag, 0.1 / self.cfg.fps)\n",
    "        raw_curv = cross_prod / (vel_mag_safe**3)\n",
    "        raw_curv = np.where(moving_mask, raw_curv, 0.0)\n",
    "        min_turn_radius_cm = 0.5\n",
    "        max_k = 1.0 / min_turn_radius_cm\n",
    "        raw_curv = np.clip(raw_curv, -max_k, max_k)\n",
    "        abs_curv = np.abs(raw_curv)\n",
    "        abs_curv_series = pd.Series(abs_curv, index=ctx.idx)\n",
    "\n",
    "        for w in [30, 60]:\n",
    "            ws = self._scale(w)\n",
    "            min_p = max(ws // 3, 1)\n",
    "            feats[f\"curv_mean_{w}\"] = abs_curv_series.rolling(ws, min_periods=min_p).mean()\n",
    "\n",
    "        angle = np.arctan2(vel_y, vel_x)\n",
    "        angle_series = pd.Series(angle, index=ctx.idx)\n",
    "        angle_change = np.abs(angle_series.diff().fillna(0.0))\n",
    "        angle_change = np.where(angle_change > np.pi, 2 * np.pi - angle_change, angle_change)\n",
    "        angle_change_series = pd.Series(angle_change, index=ctx.idx)\n",
    "        angle_change_series = pd.Series(np.where(moving_mask, angle_change_series, 0.0), index=ctx.idx)\n",
    "\n",
    "        ws = self._scale(30)\n",
    "        feats[\"turn_rate_30\"] = angle_change_series.rolling(ws, min_periods=max(ws // 3, 1)).sum()\n",
    "\n",
    "        return feats\n",
    "    \n",
    "    def _feat_cumulative(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Tổng quãng đường di chuyển trong một khoảng thời gian dài xung quanh frame hiện tại.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        L = max(1, self._scale(180))\n",
    "        step = np.hypot(ctx.cx.diff(), ctx.cy.diff()).fillna(0.0)\n",
    "        path = step.rolling(2 * L + 1, min_periods=max(5, L // 6), center=True).sum()\n",
    "        feats[\"path_cum180\"] =  path.fillna(0.0).astype(\"float32\")\n",
    "        return feats\n",
    "\n",
    "    def _feat_speed_asym(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Bất đối xứng tốc độ (Tương lai - Quá khứ).\n",
    "        \"\"\"\n",
    "        w = max(3, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        v_past = v.rolling(w, min_periods=1).mean()\n",
    "        v_fut = self._roll_future_mean(v, w, min_p=1)\n",
    "        return {\"spd_asym_1s\": (v_fut - v_past).fillna(0.0)}\n",
    "    \n",
    "    def _feat_gauss_shift(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Độ lệch Gaussian (KL Divergence) giữa quá khứ và tương lai.\n",
    "        Đo lường sự thay đổi trạng thái thống kê.\n",
    "        \"\"\"\n",
    "        w = max(5, self._scale(30))\n",
    "        v = ctx.speed_series\n",
    "        mu_p = v.rolling(w, min_periods=1).mean()\n",
    "        va_p = v.rolling(w, min_periods=1).var().clip(lower=1e-6)\n",
    "        mu_f = self._roll_future_mean(v, w, min_p=1)\n",
    "        va_f = self._roll_future_var(v, w, min_p=1).clip(lower=1e-6)\n",
    "\n",
    "        kl_pf = 0.5 * (\n",
    "            (va_p / va_f) + ((mu_f - mu_p) ** 2) / va_f - 1.0 + np.log(va_f / va_p)\n",
    "        )\n",
    "        kl_fp = 0.5 * (\n",
    "            (va_f / va_p) + ((mu_p - mu_f) ** 2) / va_p - 1.0 + np.log(va_p / va_f)\n",
    "        )\n",
    "        return {\n",
    "            \"spd_symkl_1s\": (kl_pf + kl_fp).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        }\n",
    "    \n",
    "    def _extract_part(self, ctx: AgentContext, part: str) -> Optional[np.ndarray]:\n",
    "        if ctx.raw_df is None: return None\n",
    "        if part not in ctx.raw_df.columns.get_level_values(0): return None\n",
    "        try:\n",
    "            sub_df = ctx.raw_df.xs(part, axis=1, level=0)[[\"x\", \"y\"]].reindex(ctx.idx)\n",
    "        except KeyError: return None\n",
    "        raw = sub_df.to_numpy()\n",
    "        raw = self._forward_fill_nan(raw)\n",
    "        cm = self._to_cm(raw.astype(np.float32))\n",
    "        return self._smooth(cm)\n",
    "    \n",
    "    def _extract_parts_dict(self, ctx: AgentContext, parts: List[str] = None) -> Dict[str, Optional[np.ndarray]]:\n",
    "        out = {}\n",
    "        for p in parts:\n",
    "            out[p] = self._extract_part(ctx, p)\n",
    "        return out\n",
    "        \n",
    "    def _feat_pose_shape(self, ctx: AgentContext, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Placeholder cho các đặc trưng hình dáng (Elongation, Body Angle...).\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "\n",
    "        def zero(): return pd.Series(0.0, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        def dist(k1, k2):\n",
    "            p1, p2 = parts.get(k1), parts.get(k2)\n",
    "            if p1 is None or p2 is None: return zero()\n",
    "            d = np.linalg.norm(p1 - p2, axis=1)\n",
    "            return pd.Series(d, index=ctx.idx, dtype=\"float32\")\n",
    "        \n",
    "        def body_angle():\n",
    "            if parts.get(\"nose\") is None: return zero()\n",
    "            if parts.get(\"body_center\") is None: return zero()\n",
    "            if parts.get(\"tail_base\") is None: return zero()\n",
    "\n",
    "            v1 = parts.get(\"nose\") - parts.get(\"body_center\")\n",
    "            v2 = parts.get(\"tail_base\") - parts.get(\"body_center\")\n",
    "            dot_product = np.sum(v1 * v2, axis=1)\n",
    "            mag = np.linalg.norm(v1, axis=1) * np.linalg.norm(v2, axis=1)\n",
    "            cos_angle = np.clip(dot_product / (mag + 1e-6), -1.0, 1.0).astype(\"float32\")\n",
    "            return cos_angle\n",
    "        \n",
    "        def elongation():\n",
    "            if parts.get(\"nose\")          is None: return zero()\n",
    "            if parts.get(\"tail_base\")     is None: return zero()\n",
    "            if parts.get(\"ear_left\")  is None: return zero()\n",
    "            if parts.get(\"ear_right\") is None: return zero()\n",
    "\n",
    "            d1 = dist(\"nose\", \"tail_base\")\n",
    "            d2 = dist(\"ear_left\", \"ear_right\")\n",
    "            elongation = d1 / (d2 + 1e-6).astype(\"float32\")\n",
    "            return elongation\n",
    "        \n",
    "        def part_speed(part: str, n_frames_30fps: int) -> Dict:\n",
    "            part_pos = self._extract_part(ctx, part)\n",
    "            if part_pos is None: return zero()\n",
    "            \n",
    "            s_x = pd.Series(part_pos[:, 0], index=ctx.idx)\n",
    "            s_y = pd.Series(part_pos[:, 1], index=ctx.idx)\n",
    "            raw_speed = self._speed_series(s_x, s_y)\n",
    "\n",
    "            ws = self._scale(n_frames_30fps)\n",
    "            val = raw_speed.rolling(ws, min_periods=1, center=True).mean()\n",
    "            return val.astype(\"float32\")\n",
    "\n",
    "\n",
    "        target_parts = [\"nose\", \"body_center\", \"ear_left\", \"ear_right\", \"tail_base\"]\n",
    "        \n",
    "        parts = self._extract_parts_dict(ctx, target_parts)\n",
    "\n",
    "        feats[\"aa_nose_tailbase_dist\"]       = dist(\"nose\", \"tail_base\")\n",
    "        feats[\"aa_earleft_tailbase_dist\"]    = dist(\"ear_left\", \"tail_base\")\n",
    "        feats[\"aa_earright_tailbase_dist\"]   = dist(\"ear_right\", \"tail_base\")\n",
    "        feats[\"aa_nose_earleft_dist\"]        = dist(\"ear_left\", \"nose\")\n",
    "        feats[\"aa_nose_ear_right_dist\"]      = dist(\"ear_right\", \"nose\")\n",
    "        feats[\"aa_nose_bodycenter_dist\"]     = dist(\"nose\", \"body_center\")\n",
    "        \n",
    "        feats[\"a_elongation\"]                = elongation()\n",
    "        feats[\"a_bodyangle\"]                 = body_angle()\n",
    "        feats[\"a_tail_base_vel_500ms\"]       = part_speed(\"tail_base\", 15)\n",
    "        feats[\"a_tail_base_vel_1000ms\"]      = part_speed(\"tail_base\", 30)\n",
    "        feats[\"a_tail_base_vel_2000ms\"]      = part_speed(\"tail_base\", 60)\n",
    "        feats[\"a_tail_base_vel_3000ms\"]      = part_speed(\"tail_base\", 90)\n",
    "\n",
    "        feats[\"a_nose_vel_500ms\"]            = part_speed(\"nose\", 15)\n",
    "        feats[\"a_nose_vel_1000ms\"]           = part_speed(\"nose\", 30)\n",
    "        feats[\"a_nose_vel_2000ms\"]           = part_speed(\"nose\", 60)\n",
    "        feats[\"a_nose_vel_3000ms\"]           = part_speed(\"nose\", 90)\n",
    "\n",
    "        feats[\"a_ear_right_vel_500ms\"]       = part_speed(\"ear_right\", 15)\n",
    "        feats[\"a_ear_right_vel_1000ms\"]      = part_speed(\"ear_right\", 30)\n",
    "        feats[\"a_ear_right_vel_2000ms\"]      = part_speed(\"ear_right\", 60)\n",
    "        feats[\"a_ear_right_vel_3000ms\"]      = part_speed(\"ear_right\", 90)\n",
    "        # feats[\"a_ear_left_vel_500ms\"]        = part_speed(\"ear_left\", 15)\n",
    "        # feats[\"a_ear_left_vel_1000ms\"]       = part_speed(\"ear_left\", 30)\n",
    "        # feats[\"a_ear_left_vel_2000ms\"]       = part_speed(\"ear_left\", 60)\n",
    "        # feats[\"a_ear_left_vel_3000ms\"]       = part_speed(\"ear_left\", 90)\n",
    "        \n",
    "        return feats\n",
    "\n",
    "    def _feat_attack_sniff(\n",
    "        self,\n",
    "        ctx: AgentContext,\n",
    "        target_ctx: AgentContext = None,\n",
    "        **kwargs\n",
    "    ) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng phân biệt attack vs sniff cho lab 2-mouse (agent=1, target=2).\n",
    "    \n",
    "        Ý tưởng:\n",
    "          - attack: speed 2 con biến động mạnh, đổi hướng nhiều, body overlap cao.\n",
    "          - sniff : mũi gần cổ/thân, overlap thấp hơn, motion nhẹ/ổn định hơn.\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "    \n",
    "        def zero():\n",
    "            return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "\n",
    "        # helper khoảng cách\n",
    "        def dist(p1, p2):\n",
    "            if p1 is None or p2 is None:\n",
    "                return zero()\n",
    "            d = np.linalg.norm(p1 - p2, axis=1)\n",
    "            return pd.Series(d, index=idx, dtype=\"float32\")\n",
    "\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\"])\n",
    "        parts_t = self._extract_parts_dict(target_ctx, [\"nose\", \"tail_base\"])\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 2) ĐIỂM ĐẠI DIỆN THÂN (BODY CENTER) CHO MỖI CON\n",
    "        #    dùng trung bình neck – hips – tail_base\n",
    "        # ---------------------------------------------------------\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 4) MỨC ĐỘ “BẠO LỰC”: DAO ĐỘNG TỐC ĐỘ & ĐỔI HƯỚNG\n",
    "        # ---------------------------------------------------------\n",
    "        # speed 2 con từ velocity\n",
    "        a_speed = pd.Series(\n",
    "            np.linalg.norm(ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "\n",
    "        ws_05 = self._scale(15)  # ~0.5s\n",
    "        mp_05 = max(ws_05 // 3, 1)\n",
    "    \n",
    "        feats[\"as_a_speed_std_05\"] = (\n",
    "            a_speed.rolling(ws_05, min_periods=mp_05).std().fillna(0.0).astype(\"float32\")\n",
    "        )\n",
    "        feats[\"as_t_speed_std_05\"] = (\n",
    "            t_speed.rolling(ws_05, min_periods=mp_05).std().fillna(0.0).astype(\"float32\")\n",
    "        )\n",
    "        feats[\"as_speed_std_sum_05\"] = (\n",
    "            feats[\"as_a_speed_std_05\"] + feats[\"as_t_speed_std_05\"]\n",
    "        )\n",
    "    \n",
    "        # Đổi hướng (jerk góc) của agent\n",
    "        a_angle = np.arctan2(ctx.vel[:, 1], ctx.vel[:, 0])\n",
    "        a_angle_diff = np.abs(np.diff(a_angle))\n",
    "        a_angle_diff = np.where(\n",
    "            a_angle_diff > np.pi, 2 * np.pi - a_angle_diff, a_angle_diff\n",
    "        )\n",
    "        a_angle_diff = np.concatenate([[0.0], a_angle_diff])\n",
    "        a_angle_diff_s = pd.Series(a_angle_diff, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        feats[\"as_a_turn_jerk_05\"] = (\n",
    "            a_angle_diff_s.rolling(ws_05, min_periods=mp_05)\n",
    "            .sum()\n",
    "            .fillna(0.0)\n",
    "            .astype(\"float32\")\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # 5) XẤP XỈ OVERLAP CƠ THỂ (BODY OVERLAP)\n",
    "        #    dùng bbox từ các bộ phận thân\n",
    "        # ---------------------------------------------------------\n",
    "        def build_bbox(parts: Dict[str, Optional[np.ndarray]]):\n",
    "            arrs = []\n",
    "            for k in [\"nose\", \"body_center\", \"ear_left\", \"ear_right\", \"tail_base\"]:\n",
    "                if parts.get(k) is not None:\n",
    "                    arrs.append(parts[k])\n",
    "            if not arrs:\n",
    "                return None\n",
    "            stack = np.stack(arrs, axis=1)  # [F, K, 2]\n",
    "            xs = stack[:, :, 0]\n",
    "            ys = stack[:, :, 1]\n",
    "            xmin = np.nanmin(xs, axis=1)\n",
    "            xmax = np.nanmax(xs, axis=1)\n",
    "            ymin = np.nanmin(ys, axis=1)\n",
    "            ymax = np.nanmax(ys, axis=1)\n",
    "            return np.stack([xmin, ymin, xmax, ymax], axis=1).astype(\"float32\")\n",
    "    \n",
    "        def iou_box(box1: np.ndarray, box2: np.ndarray):\n",
    "            # box: [F, 4] = (xmin, ymin, xmax, ymax)\n",
    "            x1 = np.maximum(box1[:, 0], box2[:, 0])\n",
    "            y1 = np.maximum(box1[:, 1], box2[:, 1])\n",
    "            x2 = np.minimum(box1[:, 2], box2[:, 2])\n",
    "            y2 = np.minimum(box1[:, 3], box2[:, 3])\n",
    "    \n",
    "            inter_w = np.clip(x2 - x1, 0.0, None)\n",
    "            inter_h = np.clip(y2 - y1, 0.0, None)\n",
    "            inter = inter_w * inter_h\n",
    "    \n",
    "            area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
    "            area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
    "            union = area1 + area2 - inter + 1e-6\n",
    "            iou = inter / union\n",
    "            return iou.astype(\"float32\")\n",
    "\n",
    "        bbox_a = build_bbox(parts_a)\n",
    "        bbox_t = build_bbox(parts_t)\n",
    "        if bbox_a is not None and bbox_t is not None:\n",
    "            iou = iou_box(bbox_a, bbox_t)\n",
    "            iou_s = pd.Series(iou, index=idx, dtype=\"float32\")\n",
    "    \n",
    "            feats[\"as_body_iou\"] = iou_s\n",
    "    \n",
    "            ws_1s = self._scale(30)\n",
    "            mp_1s = max(ws_1s // 3, 1)\n",
    "            feats[\"as_body_iou_mean_1s\"] = (\n",
    "                iou_s.rolling(ws_1s, min_periods=mp_1s).mean().fillna(0.0).astype(\"float32\")\n",
    "            )\n",
    "        else:\n",
    "            feats[\"as_body_iou\"] = zero()\n",
    "            feats[\"as_body_iou_mean_1s\"] = zero()\n",
    "    \n",
    "        # ---------------------------------------------------------\n",
    "        # 6) DỌN NẠN NaN / Inf\n",
    "        # ---------------------------------------------------------\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "\n",
    "    def _feat_climb(self, ctx: AgentContext, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Feature chuyên cho hành vi climb trong arena hình chữ nhật (33 x 19 cm).\n",
    "    \n",
    "        Ý tưởng:\n",
    "          - Chuột đi gần tường: dist_wall giảm nhanh.\n",
    "          - Khi climb: sát tường (dist_wall nhỏ), v_normal ~ 0,\n",
    "            nhưng vẫn có v_tangent (bò ngang trên tường / di chuyển dọc biên).\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        idx = ctx.idx\n",
    "    \n",
    "        def zero() -> pd.Series:\n",
    "            return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 1. Arena size (cm) ---\n",
    "        # Nếu bạn đã set trong FeatureConfig thì dùng:\n",
    "        # W = self.cfg.arena_width_cm or 33.0\n",
    "        # H = self.cfg.arena_height_cm or 19.0\n",
    "        # Ở đây fix luôn cho lab này:\n",
    "        W = 33.0\n",
    "        H = 19.0\n",
    "        parts = self._extract_parts_dict(ctx, [\"nose\"])\n",
    "        head = parts.get(\"nose\")\n",
    "        \n",
    "        if head is not None:\n",
    "            # head đã ở đơn vị cm (vì _extract_part đã to_cm + smooth)\n",
    "            cx = pd.Series(head[:, 0], index=idx)\n",
    "            cy = pd.Series(head[:, 1], index=idx)\n",
    "        else:\n",
    "            # fallback: nếu không có head thì dùng body_center như cũ\n",
    "            cx = ctx.cx\n",
    "            cy = ctx.cy\n",
    "\n",
    "\n",
    "        # # --- 2. Khoảng cách tới 4 bức tường ---\n",
    "        # cx = ctx.cx  # Series\n",
    "        # cy = ctx.cy  # Series\n",
    "    \n",
    "        dist_left   = cx - 0.0\n",
    "        dist_right  = W - cx\n",
    "        dist_bottom = cy - 0.0\n",
    "        dist_top    = H - cy\n",
    "    \n",
    "        d_all = np.stack(\n",
    "            [dist_left.values, dist_right.values, dist_bottom.values, dist_top.values],\n",
    "            axis=1,  # [F, 4]\n",
    "        )\n",
    "    \n",
    "        dist_wall = np.min(d_all, axis=1)          # khoảng cách tới tường gần nhất\n",
    "        wall_idx  = np.argmin(d_all, axis=1)       # 0:left, 1:right, 2:bottom, 3:top\n",
    "    \n",
    "        dist_wall_s = pd.Series(dist_wall, index=idx, dtype=\"float32\")\n",
    "        feats[\"climb_dist_wall\"] = dist_wall_s\n",
    "    \n",
    "        # --- 3. Vận tốc theo NORMAL & TANGENT của tường gần nhất ---\n",
    "        vx = ctx.vel[:, 0]\n",
    "        vy = ctx.vel[:, 1]\n",
    "    \n",
    "        # normal hướng VÀO trong arena từ tường\n",
    "        nx = np.zeros_like(vx, dtype=\"float32\")\n",
    "        ny = np.zeros_like(vy, dtype=\"float32\")\n",
    "\n",
    "        # left  wall (x=0)    → normal = (+1, 0)\n",
    "        # right wall (x=W)    → normal = (-1, 0)\n",
    "        # bottom wall (y=0)   → normal = (0, +1)\n",
    "        # top wall (y=H)      → normal = (0, -1)\n",
    "        nx[wall_idx == 0] =  1.0\n",
    "        nx[wall_idx == 1] = -1.0\n",
    "        ny[wall_idx == 2] =  1.0\n",
    "        ny[wall_idx == 3] = -1.0\n",
    "    \n",
    "        # v_normal = v ⋅ n\n",
    "        v_normal = vx * nx + vy * ny\n",
    "    \n",
    "        # thành phần song song tường: v_tan = v - (v⋅n)n\n",
    "        v_proj_x = v_normal * nx\n",
    "        v_proj_y = v_normal * ny\n",
    "        v_tan_x = vx - v_proj_x\n",
    "        v_tan_y = vy - v_proj_y\n",
    "        v_tangent = np.sqrt(v_tan_x ** 2 + v_tan_y ** 2)\n",
    "    \n",
    "        v_normal_s  = pd.Series(v_normal,  index=idx, dtype=\"float32\")\n",
    "        v_tangent_s = pd.Series(v_tangent, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        feats[\"climb_normal_vel\"]  = v_normal_s\n",
    "        feats[\"climb_tangent_vel\"] = v_tangent_s\n",
    "    \n",
    "        # --- 4. Approach speed: dist_wall giảm mạnh (lao vào tường) ---\n",
    "        ws = self._scale(15)  # ~0.5s (15 frame ở 30fps)\n",
    "        min_p = max(ws // 3, 1)\n",
    "\n",
    "        # diff_dw > 0 khi dist_wall giảm (đi về phía tường)\n",
    "        diff_dw = -dist_wall_s.diff().fillna(0.0)  # dấu trừ để \"giảm\" → dương\n",
    "        approach = diff_dw.rolling(ws, min_periods=min_p).mean()\n",
    "        feats[\"climb_approach_speed_wall\"] = approach.astype(\"float32\")\n",
    "    \n",
    "        # --- 5. Stick score: sát tường + không còn lao vào (v_normal nhỏ) ---\n",
    "        # gần tường\n",
    "        thr_cm = 3.0  # tuỳ chỉnh (3cm sát tường)\n",
    "        near_wall = (dist_wall_s < thr_cm).astype(\"float32\")\n",
    "    \n",
    "        # ít lao vào nữa: |v_normal| nhỏ\n",
    "        stick = near_wall * (1.0 / (1.0 + v_normal_s.abs()))\n",
    "\n",
    "        # Nếu muốn climb thực sự có chút chuyển động dọc tường:\n",
    "        # yêu cầu v_tangent > một ngưỡng nhỏ (ví dụ 0.5 cm/s)\n",
    "        stick = stick * (v_tangent_s > 0.5).astype(\"float32\")\n",
    "    \n",
    "        feats[\"climb_wall_stick_score\"] = stick.astype(\"float32\")\n",
    "    \n",
    "        # --- 6. Clean NaN/Inf ---\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "\n",
    "\n",
    "    def _feat_pairwise(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Đặc trưng tương tác cặp đôi (Pairwise): Khoảng cách, Tốc độ tiếp cận.\n",
    "        \"\"\"\n",
    "        feats = {}\n",
    "        if target_ctx is None: \n",
    "            return feats\n",
    "\n",
    "        def zero(): return pd.Series(0.0, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        def dist_ab(pt_a, pt_b):\n",
    "            if pt_a is None or pt_b is None: return zero()\n",
    "            d = np.linalg.norm(pt_a - pt_b, axis=1)\n",
    "            return pd.Series(d, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        feats[\"rel_dist\"] = pd.Series(dist, index=ctx.idx, dtype=\"float32\")\n",
    "\n",
    "        # Khoảng cách\n",
    "        my_parts = self._extract_parts_dict(ctx, [\"nose\", \"ear_left\", \"ear_right\", \"body_center\", \"tail_base\"])\n",
    "        target_parts = self._extract_parts_dict(target_ctx, [\"nose\", \"body_center\", \"tail_base\", \"ear_left\", \"ear_right\"])\n",
    "\n",
    "        an, tn = my_parts[\"nose\"], target_parts[\"nose\"]\n",
    "        feats[\"dist_nose_nose\"] = dist_ab(an, tn)\n",
    "        feats[\"dist_nose_tail\"] = dist_ab(an, target_parts[\"tail_base\"])\n",
    "        feats[\"dist_nose_el\"]   = dist_ab(an, target_parts[\"ear_left\"])\n",
    "        feats[\"dist_nose_er\"]   = dist_ab(an, target_parts[\"ear_right\"])\n",
    "        feats[\"dist_nose_body\"] = dist_ab(an, target_parts[\"body_center\"])\n",
    "        feats[\"dist_tail_tail\"] = dist_ab(my_parts[\"tail_base\"], target_parts[\"tail_base\"])\n",
    "        \n",
    "\n",
    "        #  Hướng - góc nhìn\n",
    "        def get_body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"nose\")\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if head is not None and tail is not None:\n",
    "                return head - tail\n",
    "            return None\n",
    "\n",
    "        a_vec = get_body_vec(my_parts)\n",
    "        t_vec = get_body_vec(target_parts)\n",
    "\n",
    "        if a_vec is not None and t_vec is not None:\n",
    "            dot = np.sum(a_vec * t_vec, axis=1)\n",
    "            mags = np.linalg.norm(a_vec, axis=1) * np.linalg.norm(t_vec, axis=1)\n",
    "            feats[\"body_cosine\"] = pd.Series(\n",
    "                np.clip(dot / (mags + 1e-6), -1.0, 1.0), index=ctx.idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"body_cosine\"] = zero()\n",
    "\n",
    "        # Vector ánh nhìn = Target_Pos - My_Pos = rel_vec\n",
    "        if a_vec is not None:\n",
    "            dot_gaze = np.sum(a_vec * rel_vec, axis=1)\n",
    "            mag_a = np.linalg.norm(a_vec, axis=1)\n",
    "            feats[\"gaze_cosine\"] = pd.Series(\n",
    "                np.clip(dot_gaze / (mag_a * dist + 1e-6), -1.0, 1.0),\n",
    "                index=ctx.idx, dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            feats[\"gaze_cosine\"] = zero()\n",
    "\n",
    "        # Vector đơn vị hướng về địch (u)\n",
    "        dist_safe = dist.copy()\n",
    "        dist_safe[dist_safe == 0] = 1e-6\n",
    "        u_vec = rel_vec / dist_safe[:, None]\n",
    "\n",
    "        # a_vel và t_vel lấy từ Context\n",
    "        a_vel, t_vel = ctx.vel, target_ctx.vel\n",
    "\n",
    "        # A. Approach Speed (Vận tốc dọc trục nối 2 con)\n",
    "        # Dương: Lao vào nhau | Âm: Chạy ra xa nhau\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1) # Target hướng ngược lại\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)\n",
    "\n",
    "        # B. Lateral Speed (Vận tốc ngang - Vuông góc trục nối)\n",
    "        # Vector chiếu: v_proj = (v . u) * u\n",
    "        a_proj = a_along[:, None] * u_vec\n",
    "        a_lat_vec = a_vel - a_proj\n",
    "        a_lat_speed = np.linalg.norm(a_lat_vec, axis=1)\n",
    "\n",
    "        feats[\"approach_speed_agent\"]  = pd.Series(a_along, index=ctx.idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_target\"] = pd.Series(t_along, index=ctx.idx, dtype=\"float32\")\n",
    "        feats[\"approach_speed_rel\"]    = pd.Series(rel_along, index=ctx.idx, dtype=\"float32\")\n",
    "        feats[\"lateral_speed_agent\"]   = pd.Series(a_lat_speed, index=ctx.idx, dtype=\"float32\")\n",
    "        return feats\n",
    "\n",
    "    def _feat_follow_pattern(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Đặc trưng hành vi FOLLOW:\n",
    "          - Agent ở gần target\n",
    "          - Cùng hướng (body + velocity)\n",
    "          - Tốc độ vừa phải\n",
    "          - Khoảng cách tương đối ổn định trong 0.5–1s\n",
    "        \"\"\"\n",
    "        feats: Dict[str, pd.Series] = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 1. CÁC ĐẠI LƯỢNG CƠ BẢN ---\n",
    "        # Vector Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # Speed agent/target\n",
    "        a_speed = ctx.speed_series.astype(\"float32\")\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "    \n",
    "        # Body vector: nose - tail/body_center\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"nose\", \"tail_base\", \"ear_left\", \"ear_right\"])\n",
    "        parts_t = self._extract_parts_dict(target_ctx, [\"nose\", \"tail_base\", \"ear_right\", \"ear_left\"])\n",
    "    \n",
    "        def body_vec(parts_dict):\n",
    "            head = parts_dict.get(\"nose\")\n",
    "            tail = parts_dict.get(\"tail_base\")\n",
    "            if head is None or tail is None:\n",
    "                return None\n",
    "            return head - tail\n",
    "    \n",
    "        a_body = body_vec(parts_a)\n",
    "        t_body = body_vec(parts_t)\n",
    "    \n",
    "        if a_body is not None and t_body is not None:\n",
    "            dot_bt = np.sum(a_body * t_body, axis=1)\n",
    "            mag_bt = np.linalg.norm(a_body, axis=1) * np.linalg.norm(t_body, axis=1)\n",
    "            cos_body = np.clip(dot_bt / (mag_bt + 1e-6), -1.0, 1.0)\n",
    "            cos_body_s = pd.Series(cos_body, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            cos_body_s = zero()\n",
    "    \n",
    "        # Velocity hướng\n",
    "        a_vel = ctx.vel\n",
    "        t_vel = target_ctx.vel\n",
    "        a_speed_np = np.linalg.norm(a_vel, axis=1)\n",
    "        t_speed_np = np.linalg.norm(t_vel, axis=1)\n",
    "        moving_mask = (a_speed_np > 1e-3) & (t_speed_np > 1e-3)\n",
    "    \n",
    "        # cos giữa hướng velocity 2 con\n",
    "        dot_v = np.sum(a_vel * t_vel, axis=1)\n",
    "        mag_v = a_speed_np * t_speed_np + 1e-6\n",
    "        cos_vel = np.zeros_like(dot_v, dtype=\"float32\")\n",
    "        cos_vel[moving_mask] = np.clip(dot_v[moving_mask] / mag_v[moving_mask], -1.0, 1.0)\n",
    "        cos_vel_s = pd.Series(cos_vel, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- 2. WINDOW NGẮN (FOLLOW LÀ PATTERN DÀI HƠN ATTACK) ---\n",
    "        for w30 in [15, 30, 60]:   # ~0.5s, 1s, 2s\n",
    "            ws = self._scale(w30)\n",
    "            min_p = max(ws // 3, 1)\n",
    "    \n",
    "            # Khoảng cách trung bình & độ dao động\n",
    "            m_dist = rel_dist_s.rolling(ws, min_periods=min_p).mean()\n",
    "            s_dist = rel_dist_s.rolling(ws, min_periods=min_p).std()\n",
    "    \n",
    "            # Cùng hướng (body + velocity)\n",
    "            m_cos_body = cos_body_s.rolling(ws, min_periods=min_p).mean()\n",
    "            m_cos_vel  = cos_vel_s.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            # Tốc độ vừa phải\n",
    "            m_sp_a = a_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            m_sp_t = t_speed.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            feats[f\"follow_dist_mean_{w30}\"] = m_dist\n",
    "            feats[f\"follow_dist_std_{w30}\"]  = s_dist\n",
    "            feats[f\"follow_cos_body_mean_{w30}\"] = m_cos_body\n",
    "            feats[f\"follow_cos_vel_mean_{w30}\"]  = m_cos_vel\n",
    "            feats[f\"follow_speed_agent_mean_{w30}\"] = m_sp_a\n",
    "            feats[f\"follow_speed_target_mean_{w30}\"] = m_sp_t\n",
    "    \n",
    "        # Clean\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = (\n",
    "                v.replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(\"float32\")\n",
    "            )\n",
    "    \n",
    "        return feats\n",
    "    \n",
    "    def _feat_shortburst_social(self, ctx: AgentContext, target_ctx: AgentContext = None, **kwargs) -> Dict[str, pd.Series]:\n",
    "        feats = {}\n",
    "        if target_ctx is None:\n",
    "            return feats\n",
    "    \n",
    "        idx = ctx.idx\n",
    "        def zero(): return pd.Series(0.0, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # --- Lấy lại vài quantity cơ bản từ pairwise/avoidance ---\n",
    "        # vector Agent -> Target\n",
    "        rel_vec = target_ctx.pos - ctx.pos\n",
    "        rel_dist = np.linalg.norm(rel_vec, axis=1)\n",
    "        rel_dist_s = pd.Series(rel_dist, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # unit vector\n",
    "        rel_dist_safe = np.where(rel_dist == 0, 1e-6, rel_dist)\n",
    "        u_vec = rel_vec / rel_dist_safe[:, None]\n",
    "    \n",
    "        # velocity dọc trục nối (approach speed)\n",
    "        a_vel = ctx.vel\n",
    "        t_vel = target_ctx.vel\n",
    "        a_along = np.sum(a_vel * u_vec, axis=1)                # +: lao vào target\n",
    "        t_along = np.sum(t_vel * (-u_vec), axis=1)             # +: target lao vào agent\n",
    "        rel_along = np.sum((a_vel - t_vel) * u_vec, axis=1)    # +: lại gần nhau\n",
    "    \n",
    "        a_along_s = pd.Series(a_along, index=idx, dtype=\"float32\")\n",
    "        t_along_s = pd.Series(t_along, index=idx, dtype=\"float32\")\n",
    "        rel_along_s = pd.Series(rel_along, index=idx, dtype=\"float32\")\n",
    "    \n",
    "        # speed agent / target\n",
    "        a_speed = ctx.speed_series\n",
    "        t_speed = pd.Series(\n",
    "            np.linalg.norm(target_ctx.vel, axis=1),\n",
    "            index=idx,\n",
    "            dtype=\"float32\"\n",
    "        )\n",
    "    \n",
    "        # heading_rel_cos ~ escape / approach\n",
    "        # vector body của agent\n",
    "        # (reuse idea từ _feat_pairwise)\n",
    "        # head ~ nose, tail ~ tail_base/body_center\n",
    "        parts_a = self._extract_parts_dict(ctx, [\"head\", \"tail_base\"])\n",
    "        head_a = parts_a.get(\"nose\")\n",
    "        tail_a = parts_a.get(\"tail_base\")\n",
    "    \n",
    "        if head_a is not None and tail_a is not None:\n",
    "            body_vec_a = head_a - tail_a\n",
    "            dot = np.sum(body_vec_a * rel_vec, axis=1)\n",
    "            mag = np.linalg.norm(body_vec_a, axis=1) * rel_dist_safe\n",
    "            heading_cos = np.clip(dot / (mag + 1e-6), -1.0, 1.0)\n",
    "            heading_cos_s = pd.Series(heading_cos, index=idx, dtype=\"float32\")\n",
    "        else:\n",
    "            heading_cos_s = zero()\n",
    "    \n",
    "        # --- Rolling window 10, 20, 30 frames (ở fps gốc) ---\n",
    "        for w30 in [10, 20, 30]:\n",
    "            ws = self._scale(w30)\n",
    "            min_p = max(1, ws // 3)\n",
    "    \n",
    "            # Attack-like: approach mạnh, khoảng cách giảm nhanh\n",
    "            feats[f\"sb_att_approach_mean_{w30}\"] = a_along_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_att_rel_along_mean_{w30}\"] = rel_along_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_att_dist_delta_{w30}\"] = (rel_dist_s - rel_dist_s.shift(ws)).fillna(0.0)\n",
    "    \n",
    "            # Chase-like: agent & target đều nhanh, dist tương đối nhỏ\n",
    "            feats[f\"sb_chase_speed_agent_mean_{w30}\"] = a_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_chase_speed_target_mean_{w30}\"] = t_speed.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_chase_dist_mean_{w30}\"] = rel_dist_s.rolling(ws, min_periods=min_p).mean()\n",
    "    \n",
    "            # Escape-like: heading ngược, dist tăng nhanh\n",
    "            feats[f\"sb_esc_heading_cos_mean_{w30}\"] = heading_cos_s.rolling(ws, min_periods=min_p).mean()\n",
    "            feats[f\"sb_esc_dist_gain_{w30}\"] = (rel_dist_s.shift(-ws) - rel_dist_s).fillna(0.0)\n",
    "    \n",
    "        # clip & fillna\n",
    "        for k, v in feats.items():\n",
    "            feats[k] = v.replace([np.inf, -np.inf], np.nan).fillna(0.0).astype(\"float32\")\n",
    "    \n",
    "        return feats\n",
    "\n",
    "\n",
    "\n",
    "    # --- Methods tương thích ---\n",
    "    \n",
    "    def build_pose_tensor(self, tracking: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Chuyển dữ liệu tracking (DataFrame) sang Tensor [Frames, Mice, 2] và Dict chi tiết.\n",
    "        \"\"\"\n",
    "        tracking = tracking.sort_values(\"video_frame\")\n",
    "        frames = np.sort(tracking[\"video_frame\"].unique())\n",
    "        \n",
    "        pvid = tracking.pivot(\n",
    "            index=\"video_frame\", \n",
    "            columns=[\"mouse_id\", \"bodypart\"], \n",
    "            values=[\"x\", \"y\"]\n",
    "        )\n",
    "        pvid = pvid.reorder_levels([1, 2, 0], axis=1).sort_index(axis=1).astype(\"float32\")\n",
    "        mouse_ids = list(pvid.columns.get_level_values(0).unique())\n",
    "        pos = np.full((len(frames), len(mouse_ids), 2), np.nan, dtype=np.float32)\n",
    "        per_mouse_df = {}\n",
    "        \n",
    "        for i, mid in enumerate(mouse_ids):\n",
    "            single = pvid[mid]\n",
    "            per_mouse_df[mid] = single\n",
    "            \n",
    "            if \"body_center\" in single.columns.get_level_values(0):\n",
    "                cx = single[\"body_center\"][\"x\"]\n",
    "                cy = single[\"body_center\"][\"y\"]\n",
    "            else:\n",
    "                cx = single.xs(\"x\", level=1, axis=1).mean(axis=1)\n",
    "                cy = single.xs(\"y\", level=1, axis=1).mean(axis=1)\n",
    "            \n",
    "            pos[:, i, 0] = cx.reindex(frames).values\n",
    "            pos[:, i, 1] = cy.reindex(frames).values\n",
    "            \n",
    "        return frames, mouse_ids, pos, per_mouse_df\n",
    "\n",
    "    def extract_agent_target(\n",
    "        self, \n",
    "        frames: np.ndarray, \n",
    "        mouse_ids: List[Any], \n",
    "        pos: np.ndarray, \n",
    "        agent_id: Any, \n",
    "        target_id: Any, \n",
    "        per_mouse_df: Dict = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Trích xuất đặc trưng cho cặp (Agent, Target).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            aid_idx = mouse_ids.index(agent_id)\n",
    "        except ValueError:\n",
    "            return pd.DataFrame() \n",
    "\n",
    "        # 1. Build Agent Context\n",
    "        ctx_agent = self._build_context(\n",
    "            frames, \n",
    "            pos[:, aid_idx, :], \n",
    "            per_mouse_df.get(agent_id) if per_mouse_df else None\n",
    "        )\n",
    "\n",
    "        # 2. Build Target Context\n",
    "        ctx_target = None\n",
    "        if self.cfg.use_pairwise and target_id is not None and target_id in mouse_ids:\n",
    "             tid_idx = mouse_ids.index(target_id)\n",
    "             ctx_target = self._build_context(\n",
    "                 frames, \n",
    "                 pos[:, tid_idx, :], \n",
    "                 per_mouse_df.get(target_id) if per_mouse_df else None\n",
    "             )\n",
    "\n",
    "        # 3. Run all features\n",
    "        all_data = {}\n",
    "        for func_name, func in self.feature_registry.items():\n",
    "            out_dict = func(ctx_agent, target_ctx=ctx_target)\n",
    "            all_data.update(out_dict)\n",
    "\n",
    "        df_out = pd.DataFrame(all_data, index=ctx_agent.idx)\n",
    "        df_out = df_out.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        \n",
    "        return df_out.reindex(sorted(df_out.columns), axis=1)\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import gc\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# === IMPORT MODEL & OPTUNA ===\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "# Cấu hình\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "np.seterr(invalid=\"ignore\", divide=\"ignore\")\n",
    "\n",
    "# Metric\n",
    "import sys\n",
    "sys.path.append(\"/kaggle/usr/lib/mabe-f-beta\")\n",
    "try:\n",
    "    from metric import score\n",
    "except ImportError:\n",
    "    def score(*args, **kwargs): return 0.0\n",
    "\n",
    "# =========================================================\n",
    "# 1. CẤU HÌNH & SEED\n",
    "# =========================================================\n",
    "SEED = 42\n",
    "def seed_everything(seed=42):\n",
    "    np.random.seed(seed)\n",
    "seed_everything(SEED)\n",
    "\n",
    "INPUT_DIR = Path(\"/kaggle/input/MABe-mouse-behavior-detection\")\n",
    "TRAIN_TRACKING_DIR = INPUT_DIR / \"train_tracking\"\n",
    "TRAIN_ANNOTATION_DIR = INPUT_DIR / \"train_annotation\"\n",
    "TEST_TRACKING_DIR = INPUT_DIR / \"test_tracking\"\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "RESULTS_DIR = Path(r\"/kaggle/input/results-ensemble-optuna2\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "SELF_BEHAVIORS = [\"biteobject\", \"climb\", \"dig\", \"exploreobject\", \"freeze\", \"genitalgroom\", \"huddle\", \"rear\", \"rest\", \"run\", \"selfgroom\"]\n",
    "PAIR_BEHAVIORS = [\"allogroom\", \"approach\", \"attack\", \"attemptmount\", \"avoid\", \"chase\", \"chaseattack\", \"defend\", \"disengage\", \"dominance\", \"dominancegroom\", \"dominancemount\", \"ejaculate\", \"escape\", \"flinch\", \"follow\", \"intromit\", \"mount\", \"reciprocalsniff\", \"shepherd\", \"sniff\", \"sniffbody\", \"sniffface\", \"sniffgenital\", \"submit\", \"tussle\"]\n",
    "BAD_VIDEOS = []\n",
    "\n",
    "# =========================================================\n",
    "# 2. DATA LOADING & PREPARATION (NO CACHE)\n",
    "# =========================================================\n",
    "\n",
    "def load_metadata() -> pd.DataFrame:\n",
    "    return pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "\n",
    "def get_video_params(video_id: Any, meta: pd.DataFrame) -> Tuple[float, float]:\n",
    "    row = meta.loc[meta[\"video_id\"] == video_id]\n",
    "    if row.empty: return 30.0, 1.0\n",
    "    row = row.iloc[0]\n",
    "    return float(row[\"frames_per_second\"]), float(row[\"pix_per_cm_approx\"])\n",
    "\n",
    "def load_tracking(lab_id: str, video_id: Any, is_test=False) -> pd.DataFrame:\n",
    "    d = TEST_TRACKING_DIR if is_test else TRAIN_TRACKING_DIR\n",
    "    path = d / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists(): raise FileNotFoundError(path)\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def load_annotation(lab_id: str, video_id: Any) -> pd.DataFrame:\n",
    "    path = TRAIN_ANNOTATION_DIR / str(lab_id) / f\"{video_id}.parquet\"\n",
    "    if not path.exists(): return pd.DataFrame(columns=[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "    return pd.read_parquet(path)[[\"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]]\n",
    "\n",
    "# Hàm lấy feature KHÔNG CACHE để tránh tràn RAM\n",
    "def get_frame_features_no_cache(lab_id, video_id, agent_id, target_id, meta, is_test=False):\n",
    "    if is_test:\n",
    "        row = meta[meta[\"video_id\"] == video_id].iloc[0]\n",
    "        fps, pix = float(row[\"frames_per_second\"]), float(row[\"pix_per_cm_approx\"])\n",
    "        pix = pix if np.isfinite(pix) and pix > 0 else 1.0\n",
    "    else:\n",
    "        fps, pix = get_video_params(video_id, meta)\n",
    "\n",
    "    tracking = load_tracking(lab_id, video_id, is_test)\n",
    "    \n",
    "    # === GỌI CLASS FeatureExtractor (Đã có ở cell trước) ===\n",
    "    fe = FeatureExtractor(fps=fps, pix_per_cm=pix, smooth_sigma=1.0, use_pairwise=True)\n",
    "    \n",
    "    frames, mouse_ids, pos, per_mouse_df = fe.build_pose_tensor(tracking)\n",
    "    \n",
    "    features_df = fe.extract_agent_target(\n",
    "        frames=frames, mouse_ids=mouse_ids, pos=pos,\n",
    "        agent_id=agent_id, target_id=target_id, per_mouse_df=per_mouse_df\n",
    "    )\n",
    "    features_df.index = frames\n",
    "    return frames, features_df\n",
    "\n",
    "def build_frame_dataset_for_lab_behavior(lab_id, behavior, train_meta, mode=\"self\"):\n",
    "    videos = train_meta[train_meta[\"lab_id\"] == lab_id][\"video_id\"].unique().tolist()\n",
    "    index_list, feature_list, label_list = [], [], []\n",
    "\n",
    "    for video_id in videos:\n",
    "        ann = load_annotation(lab_id, video_id)\n",
    "        if ann.empty: continue\n",
    "        \n",
    "        ann_bhv = ann[ann[\"action\"] == behavior]\n",
    "        if ann_bhv.empty: continue\n",
    "\n",
    "        pairs = ann_bhv[[\"agent_id\", \"target_id\"]].drop_duplicates().values.tolist()\n",
    "        for (agent_id, target_id) in pairs:\n",
    "            target_id_use = agent_id if mode == \"self\" else target_id\n",
    "            \n",
    "            # Lấy features (tính trực tiếp)\n",
    "            frames, feat_df = get_frame_features_no_cache(lab_id, video_id, agent_id, target_id_use, train_meta)\n",
    "\n",
    "            ann_pair = ann_bhv[(ann_bhv[\"agent_id\"] == agent_id) & (ann_bhv[\"target_id\"] == target_id)]\n",
    "            if ann_pair.empty and mode == \"self\": ann_pair = ann_bhv[ann_bhv[\"agent_id\"] == agent_id]\n",
    "\n",
    "            pos_frames = set()\n",
    "            for _, r in ann_pair.iterrows(): pos_frames.update(range(int(r[\"start_frame\"]), int(r[\"stop_frame\"])))\n",
    "            \n",
    "            if not pos_frames: continue\n",
    "            label = np.isin(frames, list(pos_frames)).astype(\"int8\")\n",
    "            if label.sum() == 0: continue\n",
    "\n",
    "            # Lưu vào list và reset index ngay để giảm memory overhead\n",
    "            index_list.append(pd.DataFrame({\"video_id\": video_id, \"agent_id\": agent_id, \"target_id\": target_id, \"video_frame\": frames}))\n",
    "            feature_list.append(feat_df.reset_index(drop=True))\n",
    "            label_list.append(label)\n",
    "            \n",
    "            # Dọn dẹp ngay\n",
    "            del frames, feat_df, label\n",
    "\n",
    "    if not index_list: return pd.DataFrame(), pd.DataFrame(), np.zeros(0, dtype=\"int8\")\n",
    "    \n",
    "    return pd.concat(index_list, ignore_index=True), pd.concat(feature_list, ignore_index=True), np.concatenate(label_list).astype(\"int8\")\n",
    "\n",
    "# =========================================================\n",
    "# 3. TRAINING & ENSEMBLE HELPERS\n",
    "# =========================================================\n",
    "\n",
    "def train_catboost_fold(X_tr, y_tr, X_va, y_va, sw=1.0):\n",
    "    p = {\n",
    "        'iterations': 1000, 'learning_rate': 0.05, 'depth': 6, 'scale_pos_weight': sw,\n",
    "        'task_type': 'GPU', 'devices': '0', 'verbose': 0, 'allow_writing_files': False,\n",
    "        'l2_leaf_reg': 5, 'bootstrap_type': 'Bernoulli', 'subsample': 0.8, 'random_seed': SEED\n",
    "    }\n",
    "    m = cb.CatBoostClassifier(**p)\n",
    "    m.fit(cb.Pool(X_tr, y_tr), eval_set=cb.Pool(X_va, y_va), early_stopping_rounds=20, use_best_model=True)\n",
    "    return m\n",
    "\n",
    "def train_lightgbm_fold(X_tr, y_tr, X_va, y_va, sw=1.0):\n",
    "    p = {\n",
    "        'objective': 'binary', 'metric': 'binary_logloss', 'learning_rate': 0.05,\n",
    "        'max_depth': 6, 'num_leaves': 31, 'scale_pos_weight': sw, 'device': 'gpu',\n",
    "        'verbosity': -1, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.8,\n",
    "        'subsample_freq': 1, 'seed': SEED\n",
    "    }\n",
    "    m = lgb.train(p, lgb.Dataset(X_tr, y_tr), 1000, valid_sets=[lgb.Dataset(X_va, y_va)], callbacks=[lgb.early_stopping(20, verbose=False)])\n",
    "    return m\n",
    "\n",
    "def optimize_ensemble_weights(oof_dict, y_true):\n",
    "    models = list(oof_dict.keys())\n",
    "    def obj(trial):\n",
    "        w = [trial.suggest_float(m, 0.0, 1.0) for m in models]\n",
    "        s = sum(w) + 1e-6; w = [x/s for x in w]\n",
    "        p = np.zeros_like(y_true, dtype=float)\n",
    "        for i, m in enumerate(models): p += oof_dict[m] * w[i]\n",
    "        th = trial.suggest_float(\"th\", 0.1, 0.9)\n",
    "        return f1_score(y_true, (p >= th).astype(int), zero_division=0)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "    study.optimize(obj, n_trials=50)\n",
    "    best = study.best_params\n",
    "    th = best.pop(\"th\")\n",
    "    rw = [best[m] for m in models]; s = sum(rw)+1e-6\n",
    "    return {m: w/s for m, w in zip(models, rw)}, th\n",
    "\n",
    "def train_validate_ensemble(lab_id, behavior, indices, features, labels):\n",
    "    res_dir = RESULTS_DIR / lab_id / behavior\n",
    "    res_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if len(labels) == 0 or labels.sum() == 0: return 0.0\n",
    "\n",
    "    X = features.values.astype(\"float32\")\n",
    "    y = labels.astype(\"int8\")\n",
    "    groups = indices[\"video_id\"].values\n",
    "    \n",
    "    oof_preds = {m: np.zeros(len(y), dtype=\"float32\") for m in [\"xgb\", \"cat\", \"lgb\"]}\n",
    "    folds = np.ones(len(y), dtype=\"int8\") * -1\n",
    "\n",
    "    cv = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "    for fold, (tr_idx, va_idx) in enumerate(cv.split(X, y, groups=groups)):\n",
    "        print(f\"   Fold {fold}...\", end=\" \")\n",
    "        fd_dir = res_dir / f\"fold_{fold}\"; fd_dir.mkdir(parents=True, exist_ok=True)\n",
    "        X_tr, y_tr = X[tr_idx], y[tr_idx]; X_va, y_va = X[va_idx], y[va_idx]\n",
    "        pos = y_tr.sum(); neg = len(y_tr) - pos\n",
    "        sw = float(neg/pos) if pos > 0 else 1.0\n",
    "\n",
    "        # 1. XGBoost\n",
    "        dtr = xgb.QuantileDMatrix(X_tr, label=y_tr, feature_names=features.columns.tolist(), max_bin=64)\n",
    "        dva = xgb.DMatrix(X_va, label=y_va, feature_names=features.columns.tolist())\n",
    "        xp = {\n",
    "            \"objective\":\"binary:logistic\", \"eval_metric\":\"logloss\", \"device\":\"cuda\", \n",
    "            \"tree_method\":\"hist\", \"learning_rate\":0.05, \"max_depth\":6, \"scale_pos_weight\":sw,\n",
    "            \"min_child_weight\":5, \"subsample\":0.8, \"colsample_bytree\":0.8, \"max_bin\":64, \"seed\": SEED\n",
    "        }\n",
    "        \n",
    "        # === ĐÃ THÊM 'evals=' VÀO DÒNG DƯỚI ===\n",
    "        mx = xgb.train(\n",
    "            params=xp, \n",
    "            dtrain=dtr, \n",
    "            num_boost_round=1000, \n",
    "            evals=[(dva, \"valid\")],\n",
    "            callbacks=[xgb.callback.EarlyStopping(rounds=20, save_best=True)], \n",
    "            verbose_eval=False\n",
    "        )\n",
    "        mx.save_model(fd_dir / \"model_xgb.json\")\n",
    "        oof_preds[\"xgb\"][va_idx] = mx.predict(dva)\n",
    "\n",
    "        # 2. CatBoost\n",
    "        mc = train_catboost_fold(X_tr, y_tr, X_va, y_va, sw)\n",
    "        mc.save_model(str(fd_dir / \"model_cat.cbm\"))\n",
    "        oof_preds[\"cat\"][va_idx] = mc.predict_proba(X_va)[:,1]\n",
    "\n",
    "        # 3. LightGBM\n",
    "        ml = train_lightgbm_fold(X_tr, y_tr, X_va, y_va, sw)\n",
    "        ml.save_model(fd_dir / \"model_lgb.txt\")\n",
    "        oof_preds[\"lgb\"][va_idx] = ml.predict(X_va)\n",
    "        folds[va_idx] = fold\n",
    "        \n",
    "        print(\"Done.\")\n",
    "        del X_tr, y_tr, X_va, y_va, dtr, dva, mx, mc, ml\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"   Optimizing Weights...\", end=\" \")\n",
    "    weights, th = optimize_ensemble_weights(oof_preds, y)\n",
    "    with open(res_dir / \"ensemble_params.json\", \"w\") as f: json.dump({\"weights\": weights, \"threshold\": th}, f)\n",
    "    \n",
    "    final_pred = sum(oof_preds[m] * weights[m] for m in weights)\n",
    "    final_lbl = (final_pred >= th).astype(\"int8\")\n",
    "    \n",
    "    # Save OOF\n",
    "    df = indices.copy(); df[\"fold\"] = folds; df[\"pred\"] = final_pred; df[\"lbl\"] = final_lbl\n",
    "    df.to_parquet(res_dir / \"oof.parquet\", index=False)\n",
    "    \n",
    "    f1 = f1_score(y, final_lbl, zero_division=0)\n",
    "    print(f\"Best F1: {f1:.4f} (Th={th:.2f}, W={weights})\")\n",
    "    (res_dir / \"f1.txt\").write_text(f\"{f1:.6f}\")\n",
    "    return float(f1)\n",
    "\n",
    "# =========================================================\n",
    "# 4. INFERENCE\n",
    "# =========================================================\n",
    "\n",
    "def load_ensemble_models(lab_id, behavior):\n",
    "    base = RESULTS_DIR / lab_id / behavior\n",
    "    if not base.exists(): return []\n",
    "    models = []\n",
    "    for fd in sorted(base.glob(\"fold_*\")):\n",
    "        if not (fd / \"model_xgb.json\").exists(): continue\n",
    "        \n",
    "        xgb_b = xgb.Booster(); xgb_b.load_model(str(fd / \"model_xgb.json\"))\n",
    "        cat_m = cb.CatBoostClassifier(); \n",
    "        try: cat_m.load_model(str(fd / \"model_cat.cbm\"))\n",
    "        except: cat_m = None\n",
    "        try: lgb_m = lgb.Booster(model_file=str(fd / \"model_lgb.txt\"))\n",
    "        except: lgb_m = None\n",
    "        models.append({\"xgb\": xgb_b, \"cat\": cat_m, \"lgb\": lgb_m})\n",
    "    return models\n",
    "\n",
    "def predict_behaviors_for_pair(lab_id, video_id, aid, tid, behaviors, test_meta):\n",
    "    if lab_id != \"NiftyGoldfinch\": return None\n",
    "    frames, feat_df = get_frame_features_no_cache(lab_id, video_id, aid, tid, test_meta, is_test=True)\n",
    "    if feat_df.empty: return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "    \n",
    "    scores = {}\n",
    "    for bhv in behaviors:\n",
    "        base = RESULTS_DIR / lab_id / bhv\n",
    "        if not (base / \"ensemble_params.json\").exists(): continue\n",
    "        with open(base / \"ensemble_params.json\") as f: p = json.load(f)\n",
    "        ws, th = p[\"weights\"], p[\"threshold\"]\n",
    "        \n",
    "        folds = load_ensemble_models(lab_id, bhv)\n",
    "        if not folds: continue\n",
    "        \n",
    "        cols = folds[0][\"xgb\"].feature_names\n",
    "        X = pd.DataFrame(0.0, index=feat_df.index, columns=cols, dtype=np.float32)\n",
    "        c = list(set(cols) & set(feat_df.columns))\n",
    "        if c: X[c] = feat_df[c]\n",
    "        dtest = xgb.DMatrix(X, feature_names=cols)\n",
    "        \n",
    "        agg = np.zeros(len(feat_df), dtype=np.float32)\n",
    "        for m in folds:\n",
    "            px = m[\"xgb\"].predict(dtest)\n",
    "            pc = m[\"cat\"].predict_proba(X)[:,1] if m[\"cat\"] else np.zeros_like(px)\n",
    "            pl = m[\"lgb\"].predict(X) if m[\"lgb\"] else np.zeros_like(px)\n",
    "            \n",
    "            avg = px*ws.get(\"xgb\", 0.33) + pc*ws.get(\"cat\", 0.33) + pl*ws.get(\"lgb\", 0.33)\n",
    "            agg += avg * (avg >= th).astype(\"int8\")\n",
    "        \n",
    "        if folds: scores[bhv] = agg / len(folds)\n",
    "        \n",
    "        del X, dtest\n",
    "        gc.collect()\n",
    "\n",
    "    if not scores: return pd.DataFrame(columns=[\"video_id\", \"action\", \"start_frame\", \"stop_frame\"])\n",
    "    \n",
    "    bl = list(scores.keys()); mat = np.vstack([scores[b] for b in bl]).T\n",
    "    lbls = np.where(mat.max(1)==0, \"none\", np.array(bl)[mat.argmax(1)])\n",
    "    \n",
    "    segs = []; prev = \"none\"; start = None; pf = None\n",
    "    for f, l in zip(frames, lbls):\n",
    "        if l != prev:\n",
    "            if prev != \"none\": segs.append({\"video_id\": int(video_id), \"action\": prev, \"start_frame\": int(start), \"stop_frame\": int(pf)+1})\n",
    "            prev = l; start = f\n",
    "        pf = f\n",
    "    if prev != \"none\": segs.append({\"video_id\": int(video_id), \"action\": prev, \"start_frame\": int(start), \"stop_frame\": int(pf)+1})\n",
    "    \n",
    "    return pd.DataFrame(segs)\n",
    "\n",
    "# =========================================================\n",
    "# 5. MAIN\n",
    "# =========================================================\n",
    "target_lab = \"NiftyGoldfinch\"\n",
    "print(\"\\n=== START INFERENCE ===\")\n",
    "test_meta = pd.read_csv(INPUT_DIR / \"test.csv\")\n",
    "test_meta = test_meta[test_meta[\"lab_id\"] == target_lab].reset_index(drop=True)\n",
    "\n",
    "trained = sorted([p.name for p in (RESULTS_DIR/target_lab).iterdir() if p.is_dir()])\n",
    "sb, pb = [b for b in trained if b in SELF_BEHAVIORS], [b for b in trained if b in PAIR_BEHAVIORS]\n",
    "\n",
    "all_segs = []\n",
    "def fid(i): return str(i) if str(i).startswith(\"mouse\") else f\"mouse{i}\"\n",
    "\n",
    "for vid in sorted(test_meta[\"video_id\"].unique()):\n",
    "    print(f\"Predicting Video {vid}...\")\n",
    "    tr = load_tracking(target_lab, vid, is_test=True)\n",
    "    mids = sorted(tr[\"mouse_id\"].unique())\n",
    "    \n",
    "    if sb:\n",
    "        for m in mids:\n",
    "            df = predict_behaviors_for_pair(target_lab, vid, m, m, sb, test_meta)\n",
    "            if df is not None and not df.empty:\n",
    "                df[\"agent_id\"] = fid(m); df[\"target_id\"] = \"self\"\n",
    "                all_segs.append(df)\n",
    "    if pb and len(mids) > 1:\n",
    "        for a, t in itertools.permutations(mids, 2):\n",
    "            df = predict_behaviors_for_pair(target_lab, vid, a, t, pb, test_meta)\n",
    "            if df is not None and not df.empty:\n",
    "                df[\"agent_id\"] = fid(a); df[\"target_id\"] = fid(t)\n",
    "                all_segs.append(df)\n",
    "    del tr\n",
    "    gc.collect()\n",
    "\n",
    "cols = [\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\", \"stop_frame\"]\n",
    "\n",
    "if all_segs:\n",
    "    sub3 = pd.concat(all_segs, ignore_index=True)\n",
    "    sub3 = sub3[cols].sort_values([\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\"]).reset_index(drop=True)\n",
    "\n",
    "    sub3.insert(0, \"row_id\", np.arange(len(sub3), dtype=np.int64))\n",
    "else:\n",
    "    # CSV rỗng (0 dòng) nhưng có đủ cột + row_id\n",
    "    sub3 = pd.DataFrame(columns=[\"row_id\"] + cols)\n",
    "\n",
    "sub3.to_csv(WORKING_DIR / \"submission9.csv\", index=False)\n",
    "print(f\"\\nDone! Saved submission to {WORKING_DIR / 'submission9.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39257a68",
   "metadata": {
    "papermill": {
     "duration": 0.018898,
     "end_time": "2025-12-13T17:39:19.189382",
     "exception": false,
     "start_time": "2025-12-13T17:39:19.170484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15615ee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:19.228785Z",
     "iopub.status.busy": "2025-12-13T17:39:19.228514Z",
     "iopub.status.idle": "2025-12-13T17:39:19.344800Z",
     "shell.execute_reply": "2025-12-13T17:39:19.344034Z"
    },
    "papermill": {
     "duration": 0.13818,
     "end_time": "2025-12-13T17:39:19.346173",
     "exception": false,
     "start_time": "2025-12-13T17:39:19.207993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "\n",
    "# 1) Xóa mọi thứ trong /kaggle/working trừ .csv\n",
    "for path in WORKING_DIR.iterdir():\n",
    "    # giữ lại file .csv\n",
    "    if path.is_file() and path.suffix == \".csv\":\n",
    "        continue\n",
    "\n",
    "    if path.is_file():\n",
    "        try:\n",
    "            path.unlink()\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove file {path}: {e}\")\n",
    "    elif path.is_dir():\n",
    "        try:\n",
    "            shutil.rmtree(path, ignore_errors=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove dir {path}: {e}\")\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbaa7c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:19.385662Z",
     "iopub.status.busy": "2025-12-13T17:39:19.385433Z",
     "iopub.status.idle": "2025-12-13T17:39:43.243712Z",
     "shell.execute_reply": "2025-12-13T17:39:43.242812Z"
    },
    "papermill": {
     "duration": 23.880323,
     "end_time": "2025-12-13T17:39:43.245741",
     "exception": false,
     "start_time": "2025-12-13T17:39:19.365418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test metadata...\n",
      "Building behavior table from behaviors_labeled...\n",
      "Number of (lab, video, agent, target) groups: 16\n",
      "Generating self and pair features for all test videos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef596bf7b39949abaf9dc7c7c4b64455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference and building group submissions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9d68504c014781ab6bccac6b182f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No group submissions found, creating empty submission_not DataFrame.\n",
      "Saved submission_not to: /kaggle/working/submission_not.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Improved MABe Social Behavior Detection with XGBoost\n",
    "# Improved inference notebook (fold aggregation + postprocessing)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Input dataset checks\n",
    "# ------------------------------------------------------------\n",
    "COMP_DIR = Path(\"/kaggle/input/MABe-mouse-behavior-detection\")\n",
    "STARTER_DIR = Path(\"/kaggle/input/mabe-starter-train-ja\")\n",
    "MABE_PKG_DIR = Path(\"/kaggle/input/mabe-package\")\n",
    "\n",
    "if not COMP_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Competition dataset 'MABe Challenge - Social Action Recognition in Mice' \"\n",
    "        \"must be attached as an input.\"\n",
    "    )\n",
    "\n",
    "if not STARTER_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Dataset 'mabe-starter-train-ja' is not attached. \"\n",
    "        \"Click 'Add input' and add it before running.\"\n",
    "    )\n",
    "\n",
    "if not MABE_PKG_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Dataset 'mabe-package' is not attached. \"\n",
    "        \"It provides the offline xgboost wheel used by the starter models.\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Install xgboost from offline wheel (no internet)\n",
    "# ------------------------------------------------------------\n",
    "!pip install -q --no-index --find-links=/kaggle/input/mabe-package xgboost==3.1.1\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Copy helper scripts and trained models from starter dataset\n",
    "# ------------------------------------------------------------\n",
    "!cp /kaggle/input/mabe-starter-train-ja/self_features.py .\n",
    "!cp /kaggle/input/mabe-starter-train-ja/pair_features.py .\n",
    "!cp /kaggle/input/mabe-starter-train-ja/robustify.py .\n",
    "!cp -r /kaggle/input/mabe-starter-train-ja/results .\n",
    "\n",
    "# ============================================================\n",
    "# Imports\n",
    "# ============================================================\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# polars is preinstalled on Kaggle GPU/CPU images\n",
    "try:\n",
    "    import polars as pl\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"polars is not available in this environment. \"\n",
    "        \"Use a Kaggle GPU/CPU notebook image where polars is preinstalled.\"\n",
    "    )\n",
    "\n",
    "import xgboost as xgb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Helper scripts from starter notebook\n",
    "%run -i self_features.py\n",
    "%run -i pair_features.py\n",
    "%run -i robustify.py\n",
    "\n",
    "# ============================================================\n",
    "# Paths and constants\n",
    "# ============================================================\n",
    "INPUT_DIR = COMP_DIR\n",
    "TRAIN_TRACKING_DIR = INPUT_DIR / \"train_tracking\"\n",
    "TRAIN_ANNOTATION_DIR = INPUT_DIR / \"train_annotation\"\n",
    "TEST_TRACKING_DIR = INPUT_DIR / \"test_tracking\"\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "WORKING_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SELF_FEATURE_DIR = WORKING_DIR / \"self_features\"\n",
    "PAIR_FEATURE_DIR = WORKING_DIR / \"pair_features\"\n",
    "SELF_FEATURE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PAIR_FEATURE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INDEX_COLS = [\n",
    "    \"video_id\",\n",
    "    \"agent_mouse_id\",\n",
    "    \"target_mouse_id\",\n",
    "    \"video_frame\",\n",
    "]\n",
    "\n",
    "BODY_PARTS = [\n",
    "    \"ear_left\",\n",
    "    \"ear_right\",\n",
    "    \"nose\",\n",
    "    \"neck\",\n",
    "    \"body_center\",\n",
    "    \"lateral_left\",\n",
    "    \"lateral_right\",\n",
    "    \"hip_left\",\n",
    "    \"hip_right\",\n",
    "    \"tail_base\",\n",
    "    \"tail_tip\",\n",
    "]\n",
    "\n",
    "SELF_BEHAVIORS = [\n",
    "    \"biteobject\",\n",
    "    \"climb\",\n",
    "    \"dig\",\n",
    "    \"exploreobject\",\n",
    "    \"freeze\",\n",
    "    \"genitalgroom\",\n",
    "    \"huddle\",\n",
    "    \"rear\",\n",
    "    \"rest\",\n",
    "    \"run\",\n",
    "    \"selfgroom\",\n",
    "]\n",
    "\n",
    "PAIR_BEHAVIORS = [\n",
    "    \"allogroom\",\n",
    "    \"approach\",\n",
    "    \"attack\",\n",
    "    \"attemptmount\",\n",
    "    \"avoid\",\n",
    "    \"chase\",\n",
    "    \"chaseattack\",\n",
    "    \"defend\",\n",
    "    \"disengage\",\n",
    "    \"dominance\",\n",
    "    \"dominancegroom\",\n",
    "    \"dominancemount\",\n",
    "    \"ejaculate\",\n",
    "    \"escape\",\n",
    "    \"flinch\",\n",
    "    \"follow\",\n",
    "    \"intromit\",\n",
    "    \"mount\",\n",
    "    \"reciprocalsniff\",\n",
    "    \"shepherd\",\n",
    "    \"sniff\",\n",
    "    \"sniffbody\",\n",
    "    \"sniffface\",\n",
    "    \"sniffgenital\",\n",
    "    \"submit\",\n",
    "    \"tussle\",\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# Helper functions\n",
    "# ============================================================\n",
    "\n",
    "def parse_behaviors_column(behaviors_str: str):\n",
    "    \"\"\"\n",
    "    behaviors_labeled is stored as a Python like list of tuples.\n",
    "    Use ast.literal_eval for safety instead of eval.\n",
    "\n",
    "    Example:\n",
    "      \"[('mouse1','mouse2','sniff'), ('mouse2','mouse1','sniff')]\"\n",
    "    \"\"\"\n",
    "    if behaviors_str is None:\n",
    "        return []\n",
    "    return ast.literal_eval(behaviors_str)\n",
    "\n",
    "\n",
    "def build_behavior_dataframe(test_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Expand behaviors_labeled into one row per (lab, video, agent, target, behavior).\n",
    "    \"\"\"\n",
    "    behavior_df = (\n",
    "        test_df\n",
    "        .filter(pl.col(\"behaviors_labeled\").is_not_null())\n",
    "        .select([\"lab_id\", \"video_id\", \"behaviors_labeled\"])\n",
    "        .with_columns(\n",
    "            pl.col(\"behaviors_labeled\")\n",
    "            .map_elements(\n",
    "                parse_behaviors_column,\n",
    "                return_dtype=pl.List(pl.Utf8),\n",
    "            )\n",
    "            .alias(\"behaviors_labeled_list\")\n",
    "        )\n",
    "        .explode(\"behaviors_labeled_list\")\n",
    "        .rename({\"behaviors_labeled_list\": \"behaviors_labeled_element\"})\n",
    "        .with_columns(\n",
    "            pl.col(\"behaviors_labeled_element\").str.split(\",\").list.get(0)\n",
    "            .str.replace_all(\"[()' ]\", \"\")\n",
    "            .alias(\"agent\"),\n",
    "            pl.col(\"behaviors_labeled_element\").str.split(\",\").list.get(1)\n",
    "            .str.replace_all(\"[()' ]\", \"\")\n",
    "            .alias(\"target\"),\n",
    "            pl.col(\"behaviors_labeled_element\").str.split(\",\").list.get(2)\n",
    "            .str.replace_all(\"[()' ]\", \"\")\n",
    "            .alias(\"behavior\"),\n",
    "        )\n",
    "        .select([\"lab_id\", \"video_id\", \"agent\", \"target\", \"behavior\"])\n",
    "    )\n",
    "    return behavior_df\n",
    "\n",
    "\n",
    "def extract_mouse_id(mouse_str: str) -> int:\n",
    "    \"\"\"\n",
    "    Convert 'mouse1' -> 1, 'mouse2' -> 2, 'self' -> -1.\n",
    "    \"\"\"\n",
    "    if mouse_str == \"self\":\n",
    "        return -1\n",
    "    m = re.search(r\"mouse(\\d+)\", mouse_str)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    raise ValueError(f\"Unexpected mouse id format: {mouse_str}\")\n",
    "\n",
    "\n",
    "def load_features_for_group(lab_id, video_id, agent, target):\n",
    "    \"\"\"\n",
    "    Load per frame features for a given (lab, video, agent, target) group.\n",
    "    Returns:\n",
    "      index_df   - DataFrame with INDEX_COLS\n",
    "      feature_df - DataFrame with feature columns only\n",
    "    \"\"\"\n",
    "    agent_mouse_id = extract_mouse_id(agent)\n",
    "    target_mouse_id = extract_mouse_id(target)\n",
    "\n",
    "    if target == \"self\":\n",
    "        feature_path = SELF_FEATURE_DIR / f\"{video_id}.parquet\"\n",
    "        scan = pl.scan_parquet(feature_path).filter(\n",
    "            pl.col(\"agent_mouse_id\") == agent_mouse_id\n",
    "        )\n",
    "    else:\n",
    "        feature_path = PAIR_FEATURE_DIR / f\"{video_id}.parquet\"\n",
    "        scan = pl.scan_parquet(feature_path).filter(\n",
    "            (pl.col(\"agent_mouse_id\") == agent_mouse_id)\n",
    "            & (pl.col(\"target_mouse_id\") == target_mouse_id)\n",
    "        )\n",
    "\n",
    "    full_df = scan.collect()\n",
    "    if full_df.height == 0:\n",
    "        return full_df, full_df\n",
    "\n",
    "    index_df = full_df.select(INDEX_COLS)\n",
    "    feature_df = full_df.select(pl.exclude(INDEX_COLS))\n",
    "    return index_df, feature_df\n",
    "\n",
    "\n",
    "def load_models_for_behavior(lab_id: str, behavior: str):\n",
    "    \"\"\"\n",
    "    Load all fold models and thresholds for a given (lab, behavior).\n",
    "    Returns list of (model, threshold).\n",
    "    \"\"\"\n",
    "    behavior_dir = WORKING_DIR / \"results\" / lab_id / behavior\n",
    "    fold_dirs = sorted(behavior_dir.glob(\"fold_*\"))\n",
    "    models = []\n",
    "    for fold_dir in fold_dirs:\n",
    "        model_file = fold_dir / \"model.json\"\n",
    "        thr_file = fold_dir / \"threshold.txt\"\n",
    "        if not model_file.exists() or not thr_file.exists():\n",
    "            continue\n",
    "        with open(thr_file, \"r\") as f:\n",
    "            threshold = float(f.read().strip())\n",
    "        model = xgb.Booster(model_file=str(model_file))\n",
    "        models.append((model, threshold))\n",
    "    return models\n",
    "\n",
    "\n",
    "def predict_for_group(\n",
    "    lab_id: str,\n",
    "    video_id: int,\n",
    "    agent: str,\n",
    "    target: str,\n",
    "    group_behaviors: pl.DataFrame,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run inference for one group of (lab_id, video_id, agent, target).\n",
    "\n",
    "    Improvements:\n",
    "      - Aggregate folds per behavior into a single score column\n",
    "        (mean of thresholded probabilities).\n",
    "      - Pick best behavior per frame using those aggregated scores.\n",
    "    \"\"\"\n",
    "    my_labs = [\"AdaptableSnail\", \"BoisterousParrot\", \"ElegantMink\", \"GroovyShrew\", \"JovialSwallow\", \"PleasantMeerkat\", \"SparklingTapir\", \"TranquilPanther\", \"NiftyGoldfinch\"]\n",
    "    if lab_id not in my_labs:\n",
    "        index_df, feature_df = load_features_for_group(lab_id, video_id, agent, target)\n",
    "    \n",
    "        if feature_df.height == 0:\n",
    "            return None\n",
    "    \n",
    "        # Create XGBoost DMatrix once per group and reuse across behaviors\n",
    "        dtest = xgb.DMatrix(feature_df.to_pandas(), feature_names=feature_df.columns)\n",
    "    \n",
    "        prediction_df = index_df.clone()\n",
    "        used_cols = []\n",
    "    \n",
    "        # Unique behaviors for this group\n",
    "        unique_behaviors = (\n",
    "            group_behaviors.select(\"behavior\").unique()[\"behavior\"].to_list()\n",
    "        )\n",
    "    \n",
    "        for behavior in unique_behaviors:\n",
    "            models = load_models_for_behavior(lab_id, behavior)\n",
    "            if not models:\n",
    "                # No trained model for this (lab, behavior) in the starter models\n",
    "                continue\n",
    "    \n",
    "            # Aggregate over folds: mean of thresholded probabilities\n",
    "            agg_scores = np.zeros(feature_df.height, dtype=np.float32)\n",
    "    \n",
    "            for model, threshold in models:\n",
    "                probs = model.predict(dtest)\n",
    "                labels = (probs >= threshold).astype(np.int8)\n",
    "                agg_scores += probs * labels\n",
    "    \n",
    "            agg_scores /= max(len(models), 1)\n",
    "    \n",
    "            col_name = behavior\n",
    "            prediction_df = prediction_df.with_columns(\n",
    "                pl.Series(name=col_name, values=agg_scores)\n",
    "            )\n",
    "            used_cols.append(col_name)\n",
    "    \n",
    "        if not used_cols:\n",
    "            return None\n",
    "    \n",
    "        # Pick best behavior per frame (over behaviors only)\n",
    "        cols = used_cols\n",
    "    \n",
    "        prediction_labels_df = (\n",
    "            prediction_df\n",
    "            .with_columns(\n",
    "                pl.struct(pl.col(cols))\n",
    "                .map_elements(\n",
    "                    lambda row: (\n",
    "                        \"none\"\n",
    "                        if sum(row.values()) == 0\n",
    "                        else cols[int(np.argmax(list(row.values())))]\n",
    "                    ),\n",
    "                    return_dtype=pl.String,\n",
    "                )\n",
    "                .alias(\"prediction\")\n",
    "            )\n",
    "            .select(INDEX_COLS + [\"prediction\"])\n",
    "        )\n",
    "    \n",
    "        # Convert per frame labels into time segments\n",
    "        agent_mouse_id = extract_mouse_id(agent)\n",
    "        target_mouse_id = extract_mouse_id(target)\n",
    "    \n",
    "        group_submission = (\n",
    "            prediction_labels_df\n",
    "            .filter(pl.col(\"prediction\") != pl.col(\"prediction\").shift(1))\n",
    "            .with_columns(\n",
    "                pl.col(\"video_frame\").shift(-1).alias(\"stop_frame\")\n",
    "            )\n",
    "            .filter(pl.col(\"prediction\") != \"none\")\n",
    "            .select(\n",
    "                pl.col(\"video_id\"),\n",
    "                (pl.lit(\"mouse\") + pl.lit(agent_mouse_id).cast(pl.Utf8)).alias(\"agent_id\"),\n",
    "                pl.when(pl.lit(target_mouse_id) == -1)\n",
    "                .then(pl.lit(\"self\"))\n",
    "                .otherwise(pl.lit(\"mouse\") + pl.lit(target_mouse_id).cast(pl.Utf8))\n",
    "                .alias(\"target_id\"),\n",
    "                pl.col(\"prediction\").alias(\"action\"),\n",
    "                pl.col(\"video_frame\").alias(\"start_frame\"),\n",
    "                pl.col(\"stop_frame\"),\n",
    "            )\n",
    "        )\n",
    "    \n",
    "        return group_submission\n",
    "\n",
    "# ============================================================\n",
    "# 1. Load metadata and build behavior table\n",
    "# ============================================================\n",
    "print(\"Loading test metadata...\")\n",
    "test_df = pl.read_csv(INPUT_DIR / \"test.csv\")\n",
    "\n",
    "print(\"Building behavior table from behaviors_labeled...\")\n",
    "behavior_df = build_behavior_dataframe(test_df)\n",
    "\n",
    "groups = list(\n",
    "    behavior_df.group_by(\"lab_id\", \"video_id\", \"agent\", \"target\", maintain_order=True)\n",
    ")\n",
    "print(f\"Number of (lab, video, agent, target) groups: {len(groups)}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. Pre compute features for all videos\n",
    "# ============================================================\n",
    "print(\"Generating self and pair features for all test videos...\")\n",
    "\n",
    "rows = test_df.rows(named=True)\n",
    "\n",
    "for row in tqdm(rows, total=len(rows)):\n",
    "    lab_id = row[\"lab_id\"]\n",
    "    video_id = row[\"video_id\"]\n",
    "\n",
    "    tracking_path = TEST_TRACKING_DIR / f\"{lab_id}/{video_id}.parquet\"\n",
    "    tracking = pl.read_parquet(tracking_path)\n",
    "\n",
    "    self_feat = make_self_features(metadata=row, tracking=tracking)\n",
    "    pair_feat = make_pair_features(metadata=row, tracking=tracking)\n",
    "\n",
    "    self_feat.write_parquet(SELF_FEATURE_DIR / f\"{video_id}.parquet\")\n",
    "    pair_feat.write_parquet(PAIR_FEATURE_DIR / f\"{video_id}.parquet\")\n",
    "\n",
    "    del self_feat, pair_feat, tracking\n",
    "    gc.collect()\n",
    "\n",
    "# ============================================================\n",
    "# 3. Inference by group and segment construction\n",
    "# ============================================================\n",
    "print(\"Running inference and building group submissions...\")\n",
    "\n",
    "group_submissions = []\n",
    "\n",
    "for (lab_id, video_id, agent, target), group in tqdm(groups, total=len(groups)):\n",
    "    group_submission = predict_for_group(\n",
    "        lab_id=lab_id,\n",
    "        video_id=video_id,\n",
    "        agent=agent,\n",
    "        target=target,\n",
    "        group_behaviors=group,\n",
    "    )\n",
    "\n",
    "    if group_submission is not None and group_submission.height > 0:\n",
    "        group_submissions.append(group_submission)\n",
    "\n",
    "# ============================================================\n",
    "# 4. Robustify and final clean up (always create submission_not)\n",
    "# ============================================================\n",
    "if group_submissions:\n",
    "    # Có dữ liệu → concat + robustify\n",
    "    submission_not = pl.concat(group_submissions, how=\"vertical\").sort(\n",
    "        \"video_id\",\n",
    "        \"agent_id\",\n",
    "        \"target_id\",\n",
    "        \"action\",\n",
    "        \"start_frame\",\n",
    "        \"stop_frame\",\n",
    "    )\n",
    "\n",
    "    print(\"Initial submission_not rows:\", submission_not.height)\n",
    "\n",
    "    print(\"Running robustify on submission_not...\")\n",
    "    submission_not = robustify(submission_not, test_df, train_test=\"test\")\n",
    "\n",
    "    # Keep only valid intervals\n",
    "    submission_not = submission_not.filter(\n",
    "        pl.col(\"start_frame\") < pl.col(\"stop_frame\")\n",
    "    )\n",
    "\n",
    "    # Drop ultra short segments (likely noise)\n",
    "    submission_not = submission_not.with_columns(\n",
    "        (pl.col(\"stop_frame\") - pl.col(\"start_frame\")).alias(\"duration\")\n",
    "    ).filter(pl.col(\"duration\") >= 2).drop(\"duration\")\n",
    "\n",
    "    print(\n",
    "        \"Rows after robustify, validity check and duration filter:\",\n",
    "        submission_not.height,\n",
    "    )\n",
    "else:\n",
    "    # Không có group_submissions → tạo DF rỗng đúng schema, KHÔNG gọi robustify\n",
    "    print(\"No group submissions found, creating empty submission_not DataFrame.\")\n",
    "    submission_not = pl.DataFrame(\n",
    "        schema={\n",
    "            \"video_id\": pl.Int64,\n",
    "            \"agent_id\": pl.Utf8,\n",
    "            \"target_id\": pl.Utf8,\n",
    "            \"action\": pl.Utf8,\n",
    "            \"start_frame\": pl.Int64,\n",
    "            \"stop_frame\": pl.Int64,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Add row_id and save\n",
    "final_submission = submission_not.with_row_index(\"row_id\")\n",
    "final_path = WORKING_DIR / \"submission_not.csv\"\n",
    "final_submission.write_csv(final_path)\n",
    "\n",
    "print(\"Saved submission_not to:\", final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e73984b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:43.289121Z",
     "iopub.status.busy": "2025-12-13T17:39:43.288337Z",
     "iopub.status.idle": "2025-12-13T17:39:43.617582Z",
     "shell.execute_reply": "2025-12-13T17:39:43.616776Z"
    },
    "papermill": {
     "duration": 0.351912,
     "end_time": "2025-12-13T17:39:43.618963",
     "exception": false,
     "start_time": "2025-12-13T17:39:43.267051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "\n",
    "# 1) Xóa mọi thứ trong /kaggle/working trừ .csv\n",
    "for path in WORKING_DIR.iterdir():\n",
    "    # giữ lại file .csv\n",
    "    if path.is_file() and path.suffix == \".csv\":\n",
    "        continue\n",
    "\n",
    "    if path.is_file():\n",
    "        try:\n",
    "            path.unlink()\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove file {path}: {e}\")\n",
    "    elif path.is_dir():\n",
    "        try:\n",
    "            shutil.rmtree(path, ignore_errors=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot remove dir {path}: {e}\")\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "076eb45f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T17:39:43.663978Z",
     "iopub.status.busy": "2025-12-13T17:39:43.663155Z",
     "iopub.status.idle": "2025-12-13T17:39:43.704972Z",
     "shell.execute_reply": "2025-12-13T17:39:43.704215Z"
    },
    "papermill": {
     "duration": 0.063867,
     "end_time": "2025-12-13T17:39:43.706048",
     "exception": false,
     "start_time": "2025-12-13T17:39:43.642181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged submission to: /kaggle/working/submission.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "WORKING_DIR = Path(\"/kaggle/working\")\n",
    "\n",
    "sub_file0 = WORKING_DIR / \"submission_not.csv\"\n",
    "sub_file1 = WORKING_DIR / \"submission1.csv\"\n",
    "sub_file2 = WORKING_DIR / \"submission2.csv\"\n",
    "sub_file3 = WORKING_DIR / \"submission3.csv\"\n",
    "sub_file4 = WORKING_DIR / \"submission4.csv\"\n",
    "sub_file5 = WORKING_DIR / \"submission5.csv\"\n",
    "sub_file6 = WORKING_DIR / \"submission6.csv\"\n",
    "sub_file7 = WORKING_DIR / \"submission7.csv\"\n",
    "sub_file8 = WORKING_DIR / \"submission8.csv\"\n",
    "sub_file9 = WORKING_DIR / \"submission9.csv\"\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for f in [sub_file1, sub_file2, sub_file3, sub_file4, sub_file5, sub_file6, sub_file7, sub_file8, sub_file9, sub_file0]:\n",
    "    if f.exists():\n",
    "        df = pd.read_csv(f)\n",
    "        if \"row_id\" in df.columns:\n",
    "            df = df.drop(columns=[\"row_id\"])\n",
    "        dfs.append(df)\n",
    "\n",
    "if dfs:\n",
    "    merged = pd.concat(dfs, ignore_index=True)\n",
    "    merged = merged.sort_values(\n",
    "        [\"video_id\", \"agent_id\", \"target_id\", \"action\", \"start_frame\"],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    merged.insert(0, \"row_id\", np.arange(len(merged), dtype=np.int64))\n",
    "else:\n",
    "    merged = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"row_id\",\n",
    "            \"video_id\",\n",
    "            \"agent_id\",\n",
    "            \"target_id\",\n",
    "            \"action\",\n",
    "            \"start_frame\",\n",
    "            \"stop_frame\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "out_path = WORKING_DIR / \"submission.csv\"\n",
    "merged.to_csv(out_path, index=False)\n",
    "print(\"Saved merged submission to:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d764f",
   "metadata": {
    "papermill": {
     "duration": 0.02066,
     "end_time": "2025-12-13T17:39:43.747802",
     "exception": false,
     "start_time": "2025-12-13T17:39:43.727142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# final"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13874099,
     "isSourceIdPinned": false,
     "sourceId": 59156,
     "sourceType": "competition"
    },
    {
     "datasetId": 8974334,
     "sourceId": 14093314,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9013568,
     "sourceId": 14143542,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9013639,
     "sourceId": 14143634,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9013726,
     "sourceId": 14143737,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9013783,
     "sourceId": 14143809,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 262477103,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 279806245,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 281545678,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 134.127951,
   "end_time": "2025-12-13T17:39:44.589782",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-13T17:37:30.461831",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0350705d9165421e93c655f454ecb209": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4e50c8d2e58747b2a9a6f591fbcc89ab",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0d3e6eb8aac94890a880d5f03e95aed4",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "0d3e6eb8aac94890a880d5f03e95aed4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0f6672fafd7e4fcc9e01e0f488f536e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_671278310f4d472bab337771ed85df53",
       "placeholder": "​",
       "style": "IPY_MODEL_f61f7d7688c149328306340f42d8f313",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:02&lt;00:00,  2.03s/it]"
      }
     },
     "2ab70950d7f349ec9d1b3a2bc7a9b126": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "34302a4976cf41699364cbb9e60c26cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "35ac5c5d1e5a4c78bd0e1c0c1dddc578": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3e67fe72b9ec44648879ffa29498f7a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7341761df3b04628b98c98e74c08d43b",
       "placeholder": "​",
       "style": "IPY_MODEL_c224727d32464b1b89e4d753347566cb",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "4e50c8d2e58747b2a9a6f591fbcc89ab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "50a825169caa43229d873c178e32501d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6473da06860f43b7ad0ad4b59168812c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "671278310f4d472bab337771ed85df53": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7341761df3b04628b98c98e74c08d43b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9d5cc85679e5460e9537b7d326ea8073": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a17308120c7744ea82507c2dd26ce2d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fa7eddb18e764402af3dbcff35580d27",
       "placeholder": "​",
       "style": "IPY_MODEL_2ab70950d7f349ec9d1b3a2bc7a9b126",
       "tabbable": null,
       "tooltip": null,
       "value": " 16/16 [00:00&lt;00:00, 1896.11it/s]"
      }
     },
     "b67c1f4a0b694cb7b0cba6c00260b210": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_34302a4976cf41699364cbb9e60c26cf",
       "max": 16,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9d5cc85679e5460e9537b7d326ea8073",
       "tabbable": null,
       "tooltip": null,
       "value": 16
      }
     },
     "bb9d68504c014781ab6bccac6b182f40": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e4de22a8c6e043e58f26a4d6c418d62f",
        "IPY_MODEL_b67c1f4a0b694cb7b0cba6c00260b210",
        "IPY_MODEL_a17308120c7744ea82507c2dd26ce2d5"
       ],
       "layout": "IPY_MODEL_50a825169caa43229d873c178e32501d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c224727d32464b1b89e4d753347566cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d26a7fadb4824fb89e41cde443c85486": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e4de22a8c6e043e58f26a4d6c418d62f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d26a7fadb4824fb89e41cde443c85486",
       "placeholder": "​",
       "style": "IPY_MODEL_6473da06860f43b7ad0ad4b59168812c",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "ef596bf7b39949abaf9dc7c7c4b64455": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3e67fe72b9ec44648879ffa29498f7a6",
        "IPY_MODEL_0350705d9165421e93c655f454ecb209",
        "IPY_MODEL_0f6672fafd7e4fcc9e01e0f488f536e2"
       ],
       "layout": "IPY_MODEL_35ac5c5d1e5a4c78bd0e1c0c1dddc578",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f61f7d7688c149328306340f42d8f313": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fa7eddb18e764402af3dbcff35580d27": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
